---
layout: section
category: analytics
analytics_category: overview
title: Data production
weight: 2
---

# Understanding SnowPlow data

The better you understand how your SnowPlow data is stored and structured, the easier it will be for you to query SnowPlow data. There are three elements to understand about SnowPlow data in particular:

1. [How SnowPlow data is produced] [production]
2. [Where SnowPlow data lives] [location]
3. [How SnowPlow data is structured] [structure]

<a name="production"><h2>How SnowPlow data is produced</h2></a>

*Note: this is just an overview geared towards data analysts. For a complete explanation of the SnowPlow technology stack including source code, visit the [wiki] [wiki] and [Github repository] [github-repo]*.

![SnowPlow architecture flowchart] [architecture]

Data is generated by [SnowPlow trackers] [trackers]. In most cases, this will be the SnowPlow [Javascript tracker] [js-tracker], that tracks user behaviour across your website the same way that the Google Analytics or Omniture tracker works.

The Javascript tracker listens for events that happen on your website, be these page views, _add-to-baskets_, plays of video, online transaction or any other event. When an event occurs, the tracker captures the relevant data associated with the event (e.g. the `user-id`, `page_url` etc.) and sends that data to the SnowPlow collector. In most cases, the tracker sends a single line of data for every single event that occurs. (An exception is online transactions, where a single line of data is sent for the overall transaction, and then an additional line for every product that was included in the transaction.)

These data points are received by [SnowPlow collectors] [collectors]. Most businesses running SnowPlow use the [Cloudfront collector] [cloudfront-collector]: this uses Amazon's Cloudfront CDN to serve a tracking pixel. The tracker passes data to the Cloudfront collector by making a `GET` request for the tracking pixel, and appending the data points to pass into SnowPlow to the query string for the `GET` request. The collector logs the request made, including the querystring.

An [ETL process][etl] then processes the raw logs, cleans them up, enriches them and then writes the tidied up data to one or more of the [storage] [storage] modules. The important thing to realise is that for every line of data in, and a line of data out is produced: the output data is as granular as the input data (i.e. at least one line for every event that occurs). The only difference is that the data has been formatted to make it easier to understand and query.

## Understand how SnowPlow data is generated?

[Learn more][location] about how SnowPlow data is stored.

[production]: #production
[location]: snowplow-data-storage.html
[structure]: snowplow-table-structure.html
[github-repo]: http://github.com/snowplow/snowplow
[wiki]: http://github.com/snowplow/snowplow/wiki
[apachehive]: snowplow-data-storage.html#apachehive
[infobright]: snowplow-data-storage.html#infobright
[cloudfront]: http://aws.amazon.com/cloudfront/
[architecture]: /static/img/architecture.png
[trackers]: https://github.com/snowplow/snowplow/tree/master/1-trackers
[js-tracker]: https://github.com/snowplow/snowplow/tree/master/1-trackers/javascript-tracker
[collectors]: https://github.com/snowplow/snowplow/tree/master/2-collectors
[cloudfront-collector]: https://github.com/snowplow/snowplow/tree/master/2-collectors/cloudfront-collector/
[etl]: https://github.com/snowplow/snowplow/tree/master/3-etl
[storage]: https://github.com/snowplow/snowplow/tree/master/4-storage
