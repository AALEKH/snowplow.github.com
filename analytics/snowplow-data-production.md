---
layout: page
group: analytics
sub_group: overview
title: How Snowplow produces data
shortened-link: Data production
weight: 2
---

# Understanding Snowplow data

The better you understand how your Snowplow data is produced, stored and structured, the easier it will be for you to query Snowplow data. There are three elements to understand about Snowplow data in particular:

1. [How Snowplow data is produced] [production]
2. [Where Snowplow data lives] [location]
3. [How Snowplow data is structured] [structure]

<a name="production"><h2>How Snowplow data is produced</h2></a>

*Note: this is just an overview geared towards data analysts. For a complete explanation of the Snowplow technology stack including source code, visit the [wiki] [wiki] and [Github repository] [github-repo]*.

![Snowplow architecture flowchart] [architecture]

Data is generated by [Snowplow trackers] [trackers]. In most cases, this will be the Snowplow [Javascript tracker] [js-tracker], that tracks user behavior across your website the same way that the Google Analytics or Omniture Javascript tracker works.

The Javascript tracker listens for events that happen on your website, be these _page views_, _add to baskets_, _video plays_, _transaction_ or any other event. When an event occurs, the tracker captures the relevant data associated with the event (e.g. the `user_id`, `page_url` etc.) and sends that data to the Snowplow collector. In most cases, the tracker sends a single line of data for every single event that occurs. (An exception is online transactions, where a single line of data is sent for the overall transaction, and then an additional line for every product that was included in the transaction.)

These data points are received by [Snowplow collectors] [collectors]. Most businesses running Snowplow use the [Cloudfront collector] [cloudfront-collector]: this uses Amazon's Cloudfront CDN to serve a tracking pixel. The tracker passes data to the Cloudfront collector by making a `GET` request for the tracking pixel, and appending the data points to pass into Snowplow to the query string for the `GET` request. The collector logs the request made, including the querystring, and stores the log to S3.

An [ETL process][etl] then processes the raw logs, cleans them up, enriches them and then writes the tidied up data to one or more of the [storage] [storage] modules. The important thing to realise is that for every line of data in, and a line of data out is produced: the output data is as granular as the input data (i.e. at least one line for every event that occurs). The only difference is that the data has been formatted to make it easier to understand and query.


## Understand how Snowplow data is generated?

[Learn more][location] about how Snowplow data is stored.

[production]: #production
[location]: snowplow-data-storage.html
[structure]: snowplow-table-structure.html
[github-repo]: http://github.com/snowplow/snowplow
[wiki]: http://github.com/snowplow/snowplow/wiki
[apachehive]: snowplow-data-storage.html#apachehive
[infobright]: snowplow-data-storage.html#infobright
[cloudfront]: http://aws.amazon.com/cloudfront/
[architecture]: /assets/img/architecture.png
[trackers]: https://github.com/snowplow/snowplow/tree/master/1-trackers
[js-tracker]: https://github.com/snowplow/snowplow/tree/master/1-trackers/javascript-tracker
[collectors]: https://github.com/snowplow/snowplow/tree/master/2-collectors
[cloudfront-collector]: https://github.com/snowplow/snowplow/tree/master/2-collectors/cloudfront-collector/
[etl]: https://github.com/snowplow/snowplow/tree/master/3-etl
[storage]: https://github.com/snowplow/snowplow/tree/master/4-storage
