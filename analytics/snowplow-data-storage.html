<!DOCTYPE html>
<html>
<head>
	
	<title>Data storage - SnowPlow Analytics</title>
	

	<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
	<link href="/static/css/styles.css" type="text/css" rel="stylesheet" />
	<link href="/static/css/pygments.css" type="text/css" rel="stylesheet" />

</head>
<body>
	<!-- Google Tag Manager -->
	<noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-DLRG"
	height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
	<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
	new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
	j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
	'//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
	})(window,document,'script','dataLayer','GTM-DLRG');</script>
	<!-- End Google Tag Manager -->

	<div id="container">
		<div id="header" class="span-24">
  <div id="logo">
    <h1><a href="/">SnowPlow</a></h1>
    <p>Your web analytics data in your hands</p>
  </div>
  <div id="menu" class="span-15">
    <ul>
      <li ><a href="/product/index.html">Product</a></li>
      <li ><a href="/services/index.html">Services</a></li>
      <li  class="active" ><a href="/analytics/index.html">Analytics</a></li>
      <li ><a href="/technology/index.html">Technology</a></li>
      <li ><a href="/blog.html">Blog</a></li>
      <li ><a id="mail" href="/contact/index.html">Contact</a></li>
    </ul>
  </div>
</div>
	
		<div id="contents">
<h1 id='understanding_how_your_snowplow_data_is_stored'>Understanding how your SnowPlow data is stored</h1>

<p>SnowPlow data is warehoused using either:</p>

<ol>
<li><a href='#apachehive'>Apache Hive</a></li>

<li><a href='#infobright'>Infobright</a></li>
</ol>

<p>In both cases, the SnowPlow data is stored in a single table with a simple structure (documented <a href='snowplow-table-structure.html'>here</a>).</p>

<p>If the data is warehoused using Hive, it physically lives in Amazon S3. When you fire up Elastic Mapreduce, you define a table for the data and point it at the relevant location in S3, where the physical data lives.</p>

<p>In the case of Infobright, the data lives in a table on your Infobright instance, where it is queried just like any table in a database.</p>
<a name='apachehive' />
<h2 id='apache_hive'>Apache Hive</h2>

<p><a href='http://hive.apache.org/'>Apache Hive</a> is a datawarehousing platform developed by the techies at Facebook and built on top of Hadoop. It lets analysts run SQL-like queries against large volumes of data stored in flat files. (The syntax is especially close to MySQL, in particular.) From a SnowPlow analytics perspective, the important things to understand about Hive are:</p>

<ol>
<li>Hive has been incorporated by the clever folks at Amazon into their <a href='http://aws.amazon.com/elasticmapreduce/'>Elastic Mapreduce</a> service. This makes it easy to setup a Hive cluster and use it to analyse data stored in <a href='http://aws.amazon.com/s3/'>S3</a> directly</li>

<li>Because Hive is built to process large volumes of data stored in flatfiles, it is perfect for querying the the Cloudfront logs. Through a <a href='https://github.com/snowplow/snowplow/tree/master/3-etl/hive/snowplow-log-deserializers'>custom deserializer</a> developed by the SnowPlow team, Hive can read the Cloudfront logs directly and make the data stored in them available as a table so that analysts can query on one of more of the fields stored in them. (The structure of the table is documented <a href='snowplow-table-structure.html'>here</a>.)</li>

<li>Writing queries in Hive is straightforward for anyone with knowledge of SQL and the SnowPlow <a href='snowplow-table-structure.html'>data structure</a></li>

<li>Hive is incredibly scalable. It was built by the folks at Facebook to enable analysts there to comb over <strong>all</strong> Facebook&#8217;s Petabytes of user data. Hive scales linearly: to speed up querying of big data sets, you simply throw additional servers at the query. Amazon <a href='http://aws.amazon.com/elasticmapreduce/'>Elastic Mapreduce</a> makes it easy to setup clusters of as many servers as you&#8217;d like in minutes, and add additional servers to your analytics cluster as you need them</li>
</ol>

<p>Using Hive, it is possible to run analyses against your raw Cloudfront logs directly, as described <a href='#analyse-cloudfront-logs-directly'>below</a>. More often, though, analysts choose to transfer the data into a format that enables faster querying in Hive, as described <a href='#optimised-hive'>here</a>.</p>
<a name='analyse-cloudfront-logs-directly' />
<h3 id='querying_cloudfront_logs_directly_in_hive'>Querying Cloudfront logs directly in Hive</h3>

<p>Querying the Cloudfront log data directly in Hive is straightforward. First, from the terminal, navigate to the directory with your <a href='http://aws.amazon.com/developertools/2264'>Elastic Mapreduce command line tools</a> and launch a Hive interactive session:</p>

<pre><code>./elastic-mapreduce --create --alive --hive-interactive</code></pre>

<p>This command instructs Elastic Mapreduce to fire up an analytics cluster including both Hadoop and Hive. (It will include the default number of servers, which is two. More can be added by specifying additional arguments.) The command line tools will respond with a jobflow ID e.g.:</p>

<pre><code>Created job flow j-EHXD14TNGXI8</code></pre>

<p>Give Amazon a few minutes to get your cluster up and running, then SSH in by typing:</p>

<pre><code>./elastic-mapreduce --ssh --jobflow j-EHXD14TNGXI8</code></pre>

<p>Once the cluster has been setup, you should be SSHed in, and see something like this:</p>

<pre><code>ssh -o ServerAliveInterval=10 -o StrictHostKeyChecking=no -i /home/alex/.emr/pbz-hive-nasqueron.pem hadoop@ec2-176-34-64-73.eu-west-1.compute.amazonaws.com 
Warning: Permanently added &#39;ec2-176-34-64-73.eu-west-1.compute.amazonaws.com,176.34.64.73&#39; (RSA) to the list of known hosts.
Linux (none) 2.6.35.11-83.9.amzn1.i686 #1 SMP Sat Feb 19 23:41:56 UTC 2011 i686
--------------------------------------------------------------------------------

Welcome to Amazon Elastic MapReduce running Hadoop and Debian/Squeeze.
 
Hadoop is installed in /home/hadoop. Log files are in /mnt/var/log/hadoop. Check
/mnt/var/log/hadoop/steps for diagnosing step failures.

The Hadoop UI can be accessed via the following commands: 

  JobTracker    lynx http://localhost:9100/
  NameNode      lynx http://localhost:9101/
 
--------------------------------------------------------------------------------
hadoop@ip-10-234-109-57:~$ </code></pre>

<p>You are in the cluster. Launch Hive by typing:</p>

<p>We now need to create a table in Hive based on the Cloudfront logs. To do that, we first need to let Hive know where the SnowPlow deserializer is, so that Hive can read the raw log files. To do that, execute the following command:</p>

<p>(Substitute the S3 bucket you use to store the deserializer JAR.) Now you can create the table:</p>

<p>You wll need to replace the &#8216;LOGS-BUCKET-NAME&#8217; with the name of the S3 bucket where your Cloudfront logs are stored.</p>

<p>Hive now knows where your Cloudfront logs are stored, and it knows from the deserializer how to translate those individual logs into a tidy table. We can inspect the tidy table created prior to querying it:</p>

<p>Hive will respond by listing all the different fields:</p>

<p>You as an analyst can now query the &#8220;views_events&#8221; table as you would any table in SQL. For example, to count the number of unique visitors by day, execute the following query:</p>
<a name='optimised-hive' />
<h3 id='querying_the_data_in_a_format_optimsied_for_hive'>Querying the data in a format optimsied for Hive</h3>

<p>Whilst it is possible to query the Cloudfront logs directly, this is inefficient for a number of reasons:</p>

<ol>
<li>When Hive uses custom deserializers to read custom data formats, it slows down dramatically</li>

<li>The Cloudfront logs are not partitioned. That means that every time you run a query, Hive has to chunk through the complete data sets, which may run into Terabytes of data for a big website.</li>
</ol>

<p>As a result, SnowPlow offers a daily ETL process that takes identifies the new records in the Cloudfront logs and writes this data into another bucket on S3 into a partitioned format that Hive can read directly without the custom deserializer.</p>

<p>Querying this table is very similar to querying the raw Cloudfront logs. The biggest difference notable to the analyst will be that results are returned much faster.</p>

<p>To perform the queries, we need to start off by defining our table, and telling Hive where the data lives:</p>

<p>Some things to note when comparing the above CREATE TABLE statement with the one for Cloudfront earlier:</p>

<ol>
<li>We have to specify every field in the table. When we created a table for the raw Cloudfront logs, this wasn&#8217;t necessary, as it was performed implicitly by the deserializer used.</li>

<li>We have not had to specify a deserializer or table format: the default Hive settings are used</li>

<li>The data is partitioned by date. This means that if we only want to query data in a particular time period, we do <strong>not</strong> need to process the complete data set, only the data for the relevant time period.</li>
</ol>

<p>Once the table is setup, querying it is exactly like querying the raw logs table, however. For example, to calculate the number of uniques by day, enter:</p>
<a name='infobright' />
<h2 id='infobright'>Infobright</h2>

<p>Whilst Hive has a number of features that make it well suited to performing analytics (especially horizontal scaling and direct integration to Amazon S3 via EMR), there are two disadvantages to using Hive:</p>

<ol>
<li>Analysis is not instant. Even with a large cluster, chunking through large volumes of data in Hive typically takes minutes. Whilst that is fast, it is not fast enough for <em>train of thought analytics</em></li>

<li>Amazon charges for <a href='http://aws.amazon.com/elasticmapreduce/'>emr</a> based on the number of servers used and the number of hours they are run. This can make analysts reluctant to run big jobs.</li>

<li>Because it is <strong>not</strong> a standard RDBMS database, it is not trivial to integrate Hive with standard analytics tools including <a href='http://www.r-project.org/'>R</a> or BI tools like <a href='http://www.tableausoftware.com/'>Tableau</a> or <a href='http://www.microstrategy.co.uk/'>Microstrategy</a></li>
</ol>

<p>Enter <a href='http://www.infobright.org/'>Infobright</a>. Infobright is an open source analytics columnar database that is optimised around enabling analysts to run ad hoc queries against large data sets very quickly. Most of the query functionality available in MySQL (on which Infobright is based) is available in Infobright, making writing queries easy. Infobright scales to Terabytes of data, which is enough for all but the largest websites / networks, who will have to stick with Hive. Further, because it is based on MySQL, it integrates easily with any BI tool built to work with MySQL, which is pretty much all of them.</p>

<p>Connecting to your data stored in Infobright is straightforward. If you&#8217;re connecting via the command-line, you&#8217;ll need to SSH onto the server running your Infobright instance, and then connect to Infobright:</p>

<p>Substitute your username for &#8216;USERNAME&#8217; and enter your password when prompted. Once in Infobright, you can connect to the SnowPlow database:</p>

<p>In the SnowPlow database is the events table. You can query it as normal, so, for example, to find out the number of unique visitors by day:</p>

<p>Note that the query is identical to that executed in Hive. In general, queries in the two datawarehousing platforms are very similar.</p>

<h2 id='understand_how_snowplow_data_is_warehoused_in_hive__infobright'>Understand how SnowPlow data is warehoused in Hive / Infobright?</h2>

<p><a href='snowplow-table-structure.html'>Learn more</a> about how data is structured in Hive / Infobright</p>
</div>

 <div id="sidebar">
	<h1>Analysts Cookbook</h1>
	<p>
		<a href="/analytics/index.html" class="active">Overview</a>
		
		<ul>
			
				
					
				
					
				
					
				
					
						
							
							<li><a href="/analytics/index.html">Introduction</a></li>
									
									
					
				
					
									
					
				
					
									
					
				
					
									
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
									
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
			
				
					
				
					
				
					
				
					
									
					
				
					
						
							
							<li><a href="/analytics/snowplow-data-production.html">Data production</a></li>
									
									
					
				
					
									
					
				
					
									
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
									
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
			
				
					
				
					
				
					
				
					
									
					
				
					
									
					
				
					
						
							
							<li class="active"><a href="/analytics/snowplow-data-storage.html" class="active">Data storage</a></li>
									
									
					
				
					
									
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
									
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
			
				
					
				
					
				
					
				
					
									
					
				
					
									
					
				
					
									
					
				
					
									
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
						
							
							<li><a href="/analytics/snowplow-table-structure.html">Data structure</a></li>
									
									
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
			
				
					
				
					
				
					
				
					
									
					
				
					
									
					
				
					
									
					
				
					
						
							
							<li><a href="/analytics/basic-recipes.html">Basic recipes</a></li>
									
									
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
									
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
			
				
					
				
					
				
					
				
					
									
					
				
					
									
					
				
					
									
					
				
					
									
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
									
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
			
				
					
				
					
				
					
				
					
									
					
				
					
									
					
				
					
									
					
				
					
									
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
									
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
			
				
					
				
					
				
					
				
					
									
					
				
					
									
					
				
					
									
					
				
					
									
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
									
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
			
				
					
				
					
				
					
				
					
									
					
				
					
									
					
				
					
									
					
				
					
									
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
									
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
			
				
					
				
					
				
					
				
					
									
					
				
					
									
					
				
					
									
					
				
					
									
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
									
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
					
				
			
		</ul>
		
	</p>
	<p>
		<a href="/analytics/customer-analytics/overview.html" >Customer analytics</a>
		
	</p>
	<p>
		<a href="/analytics/platform-analytics/overview.html" >Platform analytics</a>
		
	</p>
	<p>
		<a href="/analytics/tools-and-techniques/overview.html" >Tools and techniques</a>
		
	</p>
</div>

		<div id="footer">
	<p>Copyright © SnowPlow Analytics Limited 2012.  All rights reserved</p>
</div>
	</div>
	<!-- Following Javascript function used by Disqus to count the number of comments for each blog post and display in the main index -->
	  <script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'snowplow'; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function () {
            var s = document.createElement('script'); s.async = true;
            s.type = 'text/javascript';
            s.src = 'http://' + disqus_shortname + '.disqus.com/count.js';
            (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
        }());
        </script>
</body>
</html>