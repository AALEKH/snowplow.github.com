<!DOCTYPE html>
<html>
<head>
	
	<title>Writing Hive UDFs - a tutorial - Snowplow Analytics</title>
	

	<link rel="icon" type="image/x-icon" href="/favicon.ico" />

	<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
	<link href="/static/css/styles.css" type="text/css" rel="stylesheet" />
	<link href="/static/css/pygments.css" type="text/css" rel="stylesheet" />
	
	<!--For the homepage slider-->
	<link rel="stylesheet" href="/static/css/nivo-slider.css" type="text/css" media="screen" />
	<link rel="stylesheet" href="/static/css/nivo-slider-theme-default.css" type="text/css" media="screen" />
	<script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.0/jquery.min.js"></script>
	<script src="/static/js/jquery-nivo-slider-pack.js" type="text/javascript" ></script>
	<!--MathJax http://www.mathjax.org/-->
	<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_HTMLorMML.js"></script>
	<script type="text/javascript">
		MathJax.Hub.Config({
	      tex2jax: {
	        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
	      }
	    });
	    MathJax.Hub.Queue(function() {
	        var all = MathJax.Hub.getAllJax(), i;
	        for(i=0; i < all.length; i += 1) {
	            all[i].SourceElement().parentNode.className += ' has-jax';
	        }
    	});
	</script>
	<!-- end mathjax -->
	<!-- typekit -->
	<script type="text/javascript" src="//use.typekit.net/noo1diw.js"></script>
	<script type="text/javascript">try{Typekit.load();}catch(e){}</script>
	<!-- end typekit -->
</head>
<body>
	<!-- Google Tag Manager -->
	<noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-DLRG"
	height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
	<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
	new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
	j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
	'//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
	})(window,document,'script','dataLayer','GTM-DLRG');</script>
	<!-- End Google Tag Manager -->

	<div id="container">
		<div id="header" class="span-24">
  <div id="logo">
    <h1><a href="/"><img src="/static/img/snowplow-logo-website.png" title="Snowplow Analytics" /></a></h1>
  </div>
  <div id="menu" class="span-15">
    <ul>
      <li ><a href="/product/index.html">Product</a></li>
      <li ><a href="/services/index.html">Services</a></li>
      <li ><a href="/analytics/index.html">Analytics</a></li>
      <li ><a href="/technology/index.html">Technology</a></li>
      <li  class="active" ><a href="/blog.html">Blog</a></li>
      <li ><a id="mail" href="/about/index.html">About</a></li>
    </ul>
  </div>
</div>
	
		<div id="contents">
		<div class="post">
			08 Feb 2013
			<h1>Writing Hive UDFs - a tutorial</h1>
			 <span class="author">Author: <a href="/alex.html" rel="author">Alex Dean </a></span>
			<p><em>Snowplow&#8217;s own <a href='https://github.com/alexanderdean'>Alexander Dean</a> was recently asked to write an article for the <a href='http://sdjournal.org/apache-hadoop-ecosystem/?a_aid=bartoszmiedeksza&amp;a_bid=45f0d439'>Software Developer&#8217;s Journal edition on Hadoop</a> The kind folks at the Software Developer&#8217;s Journal have allowed us to reprint his article in full below.</em></p>

<p><em>Alex started writing Hive UDFs as part of the process to write the <a href='https://github.com/snowplow/snowplow/tree/master/3-etl/hive-etl/snowplow-log-deserializers'>Snowplow log deserializer</a> - the custom SerDe used to parse Snowplow logs generated by the Cloudfront and Clojure collectors so they can be processed in the Snowplow ETL step.</em></p>

<h2 id='article_synopsis'>Article Synopsis</h2>

<p>In this article you will learn how to write a user-defined function (&#8220;UDF&#8221;) to work with the Apache Hive platform. We will start gently with an introduction to Hive, then move on to developing the UDF and writing tests for it. We will write our UDF in Java, but use Scala&#8217;s SBT as our build tool and write our tests in Scala with Specs2.</p>

<p>In order to get the most out of this article, you should be comfortable programming in Java. You do not need to have any experience with Apache Hive, HiveQL (the Hive query language) or indeed Hive UDFs - I will introduce all of these concepts from first principles. Experience with Scala is advantageous, but not necessary.</p>
<!--more-->
<h2 id='introduction'>Introduction</h2>

<p>Before we start: my name is Alex Dean, and I am the co-founder of Snowplow (http://snowplowanalytics.com/), an open-source web analytics platform built on top of Apache Hadoop and Apache Hive. My experience writing Java code to extend the Hive platform comes from Snowplow, where we built a core piece of our launch platform using a Hive deserializer (https://github.com/snowplow/snowplow/tree/master/3-etl/hive-etl/snowplow-log-deserializers).</p>

<p>So, what is Apache Hive, and what would you want a Hive UDF for? Hive is a data warehouse system built on top of Hadoop for ad-hoc queries and processing of large datasets. Now an Apache Software Foundation project, Hive was originally developed at Facebook, where analysts and data scientists wanted a SQL-like abstraction over traditional Hadoop MapReduce. As such, the key distinguishing feature of Hive is the SQL-like query language HiveQL. An example HiveQL query might look like this:</p>

<p>Listing 1: An example HiveQL query</p>
<div class='highlight'><pre><code class='mysql'><span class='k'>SELECT</span> 
<span class='n'>dt</span><span class='p'>,</span>
<span class='nf'>COUNT</span><span class='p'>(</span><span class='k'>DISTINCT</span> <span class='p'>(</span><span class='n'>user_id</span><span class='p'>))</span>
<span class='k'>FROM</span> <span class='n'>events</span>
<span class='k'>GROUP</span> <span class='k'>BY</span> <span class='n'>dt</span> <span class='p'>;</span>
</code></pre></div>
<p>This is actually a standard Snowplow query to calculate the number of unique visitors to a website by day. So what happens when an analyst runs this query in Hive? Simply this:</p>

<ol>
<li>Hive converts the query into the simplest possible set of MapReduce jobs</li>

<li>The MapReduce job or jobs is run on the Hadoop platform</li>

<li>The generated result set is then returned to the user&#8217;s console</li>
</ol>

<p>Certainly this is a powerful abstraction over MapReduce jobs, which can be tedious and difficult to write by hand. And HiveQL has a lot of power - there is very little in the ANSI SQL standard which is not available in HiveQL. Nonetheless, sometimes the Hive user will need more power, and for these occasions Hive has three main extension points:</p>

<ol>
<li>User-defined functions (&#8220;UDFs&#8221;), which provide a way of extending the functionality of Hive with a function (written in Java) that can be evaluated in HiveQL statements</li>

<li>Custom serializers and/or deserializers (&#8220;serdes&#8221;), which provide a way of either deserializing a custom file format stored on HDFS to a POJO (plain old Java object), or serializing a POJO to a custom file format (or both)</li>

<li>Custom mappers/reducers, which allow you to add custom map or reduce steps into your Hive query. These map/reduce steps can be written in any programming language - not just Java</li>
</ol>

<p>We will not consider serdes or custom mappers/reducers further in this article - we hope to write further articles on each of these in the future.</p>

<p>Now that we understand why you might write a UDF for Hive, let&#8217;s crack on and start writing one!</p>

<h2 id='setting_up_our_project'>Setting up our project</h2>

<p>We will be writing a relatively simple UDF - one which generates a converts a string in Hive to upper-case. Note that a version of this function is actually built into Hive as the UPPER function - for a full list of built-in UDFs in Hive, please see: https://cwiki.apache.org/Hive/languagemanual-udf.html</p>

<p>As mentioned previously, we will write our UDF in Java - but we will wrap our Java core in a Scala project (with Scala tests), because at Snowplow we much prefer writing Scala to Java. We will use SBT, the Scala build tool, to configure our project - this is an alternative to Maven or similar; SBT handles mixed Java and Scala projects perfectly well.</p>

<p>First, let&#8217;s create a directory for our project, and add a file, project.sbt into the project root, which contains:</p>

<p>Listing 2: Our project.sbt build file</p>
<div class='highlight'><pre><code class='scala'><span class='n'>name</span> <span class='o'>:=</span> <span class='s'>&quot;hive-example-udf&quot;</span>
<span class='n'>version</span> <span class='o'>:=</span> <span class='s'>&quot;0.0.1&quot;</span>
<span class='n'>organization</span> <span class='o'>:=</span> <span class='s'>&quot;com.snowplowanalytics&quot;</span>
<span class='n'>scalaVersion</span> <span class='o'>:=</span> <span class='s'>&quot;2.9.2&quot;</span>
<span class='n'>scalacOptions</span> <span class='o'>++=</span> <span class='nc'>Seq</span><span class='o'>(</span><span class='s'>&quot;-unchecked&quot;</span><span class='o'>,</span> <span class='s'>&quot;-deprecation&quot;</span><span class='o'>)</span>
<span class='n'>resolvers</span> <span class='o'>+=</span> <span class='s'>&quot;CDH4&quot;</span> <span class='n'>at</span> <span class='s'>&quot;https://repository.cloudera.com/artifactory/cloudera-repos/&quot;</span>
<span class='n'>libraryDependencies</span> <span class='o'>+=</span> <span class='s'>&quot;org.apache.hadoop&quot;</span> <span class='o'>%</span>  <span class='s'>&quot;hadoop-core&quot;</span>        <span class='o'>%</span> <span class='s'>&quot;0.20.2&quot;</span>      <span class='o'>%</span> <span class='s'>&quot;provided&quot;</span>
<span class='n'>libraryDependencies</span> <span class='o'>+=</span> <span class='s'>&quot;org.apache.hive&quot;</span>   <span class='o'>%</span>  <span class='s'>&quot;hive-exec&quot;</span>          <span class='o'>%</span> <span class='s'>&quot;0.8.1&quot;</span>       <span class='o'>%</span> <span class='s'>&quot;provided&quot;</span>
<span class='n'>libraryDependencies</span> <span class='o'>+=</span> <span class='s'>&quot;org.specs2&quot;</span>        <span class='o'>%%</span> <span class='s'>&quot;specs2&quot;</span>             <span class='o'>%</span> <span class='s'>&quot;1.12.1&quot;</span>      <span class='o'>%</span> <span class='s'>&quot;test&quot;</span>
</code></pre></div>
<p>This is a simple project configuration which names and versions our project, and also adds Hadoop, Hive and Specs2 (our testing library) as dependencies. If you do not have SBT already installed, you can find instructions here http://www.scala-sbt.org/release/docs/Getting-Started/Setup.html</p>

<h2 id='writing_our_udf'>Writing our UDF</h2>

<p>Done? Onto the code. First let&#8217;s create a folder for it:</p>
<div class='highlight'><pre><code class='bash'><span class='nv'>$ </span>mkdir -p src/main/java/com/snowplowanalytics/hive/udf
</code></pre></div>
<p>Now let&#8217;s add a file into our udf folder called ToUpper.java, containing:</p>

<p>Listing 3: Our ToUpper.java UDF definition</p>
<div class='highlight'><pre><code class='java'><span class='kn'>package</span> <span class='n'>com</span><span class='o'>.</span><span class='na'>snowplowanalytics</span><span class='o'>.</span><span class='na'>hive</span><span class='o'>.</span><span class='na'>udf</span><span class='o'>;</span>

<span class='kn'>import</span> <span class='nn'>org.apache.hadoop.hive.ql.exec.UDF</span><span class='o'>;</span>
<span class='kn'>import</span> <span class='nn'>org.apache.hadoop.hive.ql.exec.Description</span><span class='o'>;</span>
<span class='kn'>import</span> <span class='nn'>org.apache.hadoop.io.Text</span><span class='o'>;</span>

<span class='nd'>@Description</span><span class='o'>(</span>
	<span class='n'>name</span> <span class='o'>=</span> <span class='s'>&quot;toupper&quot;</span><span class='o'>,</span>
	<span class='n'>value</span> <span class='o'>=</span> <span class='s'>&quot;_FUNC_(str) - Converts a string to uppercase&quot;</span><span class='o'>,</span>
	<span class='n'>extended</span> <span class='o'>=</span> <span class='s'>&quot;Example:\n&quot;</span> <span class='o'>+</span>
	<span class='s'>&quot;  &gt; SELECT toupper(author_name) FROM authors a;\n&quot;</span> <span class='o'>+</span>
	<span class='s'>&quot;  STEPHEN KING&quot;</span>
	<span class='o'>)</span>
<span class='kd'>public</span> <span class='kd'>class</span> <span class='nc'>ToUpper</span> <span class='kd'>extends</span> <span class='n'>UDF</span> <span class='o'>{</span>

    <span class='kd'>public</span> <span class='n'>Text</span> <span class='nf'>evaluate</span><span class='o'>(</span><span class='n'>Text</span> <span class='n'>s</span><span class='o'>)</span> <span class='o'>{</span>
		<span class='n'>Text</span> <span class='n'>to_value</span> <span class='o'>=</span> <span class='k'>new</span> <span class='n'>Text</span><span class='o'>(</span><span class='s'>&quot;&quot;</span><span class='o'>);</span>
		<span class='k'>if</span> <span class='o'>(</span><span class='n'>s</span> <span class='o'>!=</span> <span class='kc'>null</span><span class='o'>)</span> <span class='o'>{</span>
		    <span class='k'>try</span> <span class='o'>{</span> 
				<span class='n'>to_value</span><span class='o'>.</span><span class='na'>set</span><span class='o'>(</span><span class='n'>s</span><span class='o'>.</span><span class='na'>toString</span><span class='o'>().</span><span class='na'>toUpperCase</span><span class='o'>());</span>
		    <span class='o'>}</span> <span class='k'>catch</span> <span class='o'>(</span><span class='n'>Exception</span> <span class='n'>e</span><span class='o'>)</span> <span class='o'>{</span> <span class='c1'>// Should never happen</span>
				<span class='n'>to_value</span> <span class='o'>=</span> <span class='k'>new</span> <span class='n'>Text</span><span class='o'>(</span><span class='n'>s</span><span class='o'>);</span>
		    <span class='o'>}</span>
		<span class='o'>}</span>
		<span class='k'>return</span> <span class='n'>to_value</span><span class='o'>;</span>
    <span class='o'>}</span>
<span class='o'>}</span>
</code></pre></div>
<p>This file defines our UDF, ToUpper. The package definition and imports should be self-explanatory; the <code>@Description</code> annotation is a useful Hive-specific annotation to provide usage information for our UDF in the Hive console.</p>

<p>All user-defined functions extend the Hive UDF class; a UDF sub-class must then implement one or more methods named &#8220;evaluate&#8221; which will be called by Hive. We implement an evaluate method which takes one Hadoop Text (which stores text using UTF8) and returns the same Hadoop Text, but now in upper-case. The only complexity is some exception handling, which we include for safety&#8217;s sake.</p>

<p>Now let&#8217;s check that this compiles. In the root folder, run SBT like so:</p>

<p>Listing 4: Compiling in SBT</p>
<div class='highlight'><pre><code class='bash'><span class='nv'>$ </span>sbt
&gt; compile
<span class='o'>[</span>success<span class='o'>]</span> Total <span class='nb'>time</span>: 0 s, completed 28-Jan-2013 16:41:53
</code></pre></div>
<h2 id='testing_our_udf'>Testing our UDF</h2>

<p>Okay great, now time to write a test to make sure this is doing what we expect! First we create a folder for it:</p>
<div class='highlight'><pre><code class='bash'><span class='nv'>$ </span>mkdir -p src/test/scala/com/snowplowanalytics/hive/udf
</code></pre></div>
<p>Now let&#8217;s add a file into our udf test folder called ToUpperTest.scala, containing:</p>

<p>Listing 5: Our ToUpperTest.scala Specs2 test</p>
<div class='highlight'><pre><code class='java'><span class='kn'>package</span> <span class='n'>com</span><span class='o'>.</span><span class='na'>snowplowanalytics</span><span class='o'>.</span><span class='na'>hive</span><span class='o'>.</span><span class='na'>udf</span>

<span class='kn'>import</span> <span class='nn'>org.apache.hadoop.io.Text</span>

<span class='kn'>import</span> <span class='nn'>org.specs2._</span>

<span class='kd'>class</span> <span class='nc'>ToUpperSpec</span> <span class='kd'>extends</span> <span class='n'>mutable</span><span class='o'>.</span><span class='na'>Specification</span> <span class='o'>{</span>
  <span class='n'>val</span> <span class='n'>toUpper</span> <span class='o'>=</span> <span class='k'>new</span> <span class='n'>ToUpper</span>

  <span class='s'>&quot;ToUpper#evaluate&quot;</span> <span class='n'>should</span> <span class='o'>{</span>
    <span class='s'>&quot;return an empty string if passed a null value&quot;</span> <span class='n'>in</span> <span class='o'>{</span>
      <span class='n'>toUpper</span><span class='o'>.</span><span class='na'>evaluate</span><span class='o'>(</span><span class='kc'>null</span><span class='o'>).</span><span class='na'>toString</span> <span class='n'>mustEqual</span> <span class='s'>&quot;&quot;</span>
    <span class='o'>}</span>

    <span class='s'>&quot;return a capitalised string if passed a mixed-case string&quot;</span> <span class='n'>in</span> <span class='o'>{</span>
      <span class='n'>toUpper</span><span class='o'>.</span><span class='na'>evaluate</span><span class='o'>(</span><span class='k'>new</span> <span class='n'>Text</span><span class='o'>(</span><span class='s'>&quot;Stephen King&quot;</span><span class='o'>)).</span><span class='na'>toString</span> <span class='n'>mustEqual</span> <span class='s'>&quot;STEPHEN KING&quot;</span>
    <span class='o'>}</span>
  <span class='o'>}</span>
<span class='o'>}</span>
</code></pre></div>
<p>This is a Specs2 unit test (http://etorreborre.github.com/specs2/), written in Scala, which checks that ToUpper is performing correctly: we test that an empty string is handled correctly, and then we test that a mixed-case string (&#8220;Stephen King&#8221;) is successfully converted to &#8220;STEPHEN KING&#8221;.</p>

<p>So let&#8217;s run this next from SBT:</p>

<p>Listing 6: Testing in SBT</p>
<div class='highlight'><pre><code class='text'>&gt; test
[info] Compiling 1 Scala source to /home/alex/Development/Snowplow/hive-example-udf/target/scala-2.9.2/test-classes...
[info] ToUpperSpec
[info] 
[info] ToUpper#evaluate should
[info] + return an empty string if passed a null value
[info] + return a capitalised string if passed a mixed-case string
[info]  
[info]  
[info] Total for specification ToUpperSpec
[info] Finished in 742 ms
[info] 2 examples, 0 failure, 0 error
[info] 
[info] Passed: : Total 2, Failed 0, Errors 0, Passed 2, Skipped 0
[success] Total time: 8 s, completed 28-Jan-2013 17:11:45
</code></pre></div>
<h2 id='building_and_using_our_udf'>Building and using our UDF</h2>

<p>Our tests passed! Now we&#8217;re ready to use our function &#8220;in anger&#8221; from Hive. First, still from SBT, let&#8217;s build our jarfile:</p>

<p>Listing 7: Packaging our jar</p>
<div class='highlight'><pre><code class='text'>&gt; package
[info] Packaging /home/alex/Development/Snowplow/hive-example-udf/target/scala-2.9.2/hive-example-udf_2.9.2-0.0.1.jar ...
[info] Done packaging.
[success] Total time: 1 s, completed 28-Jan-2013 17:21:02
</code></pre></div>
<p>Now take the jarfile (hive-example-udf_2.9.2-0.0.1.jar) and upload it to our Hive cluster - on Amazon&#8217;s Elastic MapReduce, for example, you could upload it to S3.</p>

<p>From your Hive console, you can now add our new UDF like so:</p>
<div class='highlight'><pre><code class='mysql'><span class='o'>&gt;</span> <span class='k'>add</span> <span class='n'>jar</span> <span class='o'>/</span><span class='n'>path</span><span class='o'>/</span><span class='k'>to</span><span class='o'>/</span><span class='n'>HiveSwarm</span><span class='p'>.</span><span class='n'>jar</span><span class='p'>;</span>
<span class='o'>&gt;</span> <span class='k'>create</span> <span class='n'>temporary</span> <span class='n'>function</span> <span class='n'>to_upper</span> <span class='k'>as</span> <span class='s1'>&#39;com.snowplowanalytics.hive.udf.ToUpper&#39;</span><span class='p'>;</span>
</code></pre></div>
<p>And then finally you can use our new UDF in your HiveQL queries, something like this:</p>
<div class='highlight'><pre><code class='mysql'><span class='o'>&gt;</span> <span class='k'>SELECT</span> <span class='nf'>toupper</span><span class='p'>(</span><span class='n'>author_name</span><span class='p'>)</span> <span class='k'>FROM</span> <span class='n'>authors</span> <span class='n'>a</span><span class='p'>;</span>
  <span class='n'>STEPHEN</span> <span class='n'>KING</span>
</code></pre></div>
<p>That completes our article. If you would like to download the example code above as a working project, you can find it on GitHub here: https://github.com/snowplow/hive-example-udf</p>

<p>I hope to return with further articles about Hive and Hadoop in the future - potentially one on writing a custom serde - an area where we have a lot of experience at Snowplow Analytics.</p>

<h2 id='looking_for_help_performing_analytics_or_developing_data_pipelines_using_hive_and_other_hadooppowered_tools'>Looking for help performing analytics or developing data pipelines using Hive and other Hadoop-powered tools</h2>

<p><a href='/services/pipelines.html'>Learn more</a> about services offered by the <a href='/services/pipelines.html'>Snowplow Professional Services team</a>.</p>
			<div class="author_summary">
				<h2>About the author</h2>
				<div class="author_image"><img src="https://lh6.googleusercontent.com/-4Ydq6ygNbgQ/AAAAAAAAAAI/AAAAAAAAAF4/SX2Fn3veqp4/s120-c/photo.jpg" /></div> <div class="author_spiel">
  <a href="/alex.html">Alex</a> is co-founder and technical lead at Snowplow Analytics. You can find in him on <a href="https://plus.google.com/u/0/113518635920914092796" rel="author">Google+</a>, <a href="https://twitter.com/alexatkeplar">Twitter</a> and <a href="http://uk.linkedin.com/in/alexdean">LinkedIn</a>.
</div>

			</div> 
			<div id="comments">
	<h2>Questions? Comments? Join the debate!</h2>
	 <div id="disqus_thread"></div>
        <script type="text/javascript">
            /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
            var disqus_shortname = 'snowplow'; // required: replace example with your forum shortname
            /* var disqus_identifier =  ; // unique ID so that disqus fetches the correct comments for each post
            var disqus_url =  ;
            var disqus_title =  ; */

            /* * * DON'T EDIT BELOW THIS LINE * * */
            (function() {
                var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
                dsq.src = 'http://' + disqus_shortname + '.disqus.com/embed.js';
                (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
            })();
        </script>
        <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
        <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</div>
		</div>
		<p>Return to the <a href="/blog.html">main blog page</a></p>
		

</div>

<div id="sidebar">
	<h1>Recent posts</h1>
	<ul>
		
			<li><a href="/blog/2013/07/09/2013-dotnet-support-added-to-referer-parser">.NET (C#) support added to referer-parser</a></li>
		
			<li><a href="/blog/2013/07/08/help-us-build-out-the-snowplow-total-cost-of-ownership-model">Help us build out the Snowplow Total Cost of Ownership Model</a></li>
		
			<li><a href="/blog/2013/07/07/snowplow-0.8.7-released">Snowplow 0.8.7 released with JavaScript Tracker improvements</a></li>
		
			<li><a href="/blog/2013/07/03/snowplow-tracker-for-lua-event-analytics-released">Snowplow Tracker for Lua event analytics released</a></li>
		
			<li><a href="/blog/2013/07/02/reduce-your-cloudfront-bills-with-cache-control">Reduce your Cloudfront costs with cache control</a></li>
		
	</ul>

	
		<h1>Other</h1>
		<ul>
		
			
				<li><a href="/blog/2013/07/08/help-us-build-out-the-snowplow-total-cost-of-ownership-model">Help us build out the Snowplow Total Cost of Ownership Model</a></li>
			
				<li><a href="/blog/2013/07/02/reduce-your-cloudfront-bills-with-cache-control">Reduce your Cloudfront costs with cache control</a></li>
			
				<li><a href="/blog/2013/06/28/is-web-analytics-easy-or-hard-distinguishing-different-types-of-complexity">Is web analytics easy or hard? Distinguishing different types of complexity, and approaches for dealing with them</a></li>
			
				<li><a href="/blog/2013/06/05/tracking-olark-chat-events-with-snowplow">Tracking Olark chat events with Snowplow</a></li>
			
				<li><a href="/blog/2013/02/20/transferring-data-from-s3-to-redshift-at-the-command-line">Bulk loading data from Amazon S3 into Redshift at the command line</a></li>
			
		
		</ul>		
	
		<h1>Releases</h1>
		<ul>
		
			
				<li><a href="/blog/2013/07/09/2013-dotnet-support-added-to-referer-parser">.NET (C#) support added to referer-parser</a></li>
			
				<li><a href="/blog/2013/07/07/snowplow-0.8.7-released">Snowplow 0.8.7 released with JavaScript Tracker improvements</a></li>
			
				<li><a href="/blog/2013/07/03/snowplow-tracker-for-lua-event-analytics-released">Snowplow Tracker for Lua event analytics released</a></li>
			
				<li><a href="/blog/2013/06/03/snowplow-0.8.6-released-with-performance-improvements">Snowplow 0.8.6 released with performance improvements</a></li>
			
				<li><a href="/blog/2013/05/24/snowplow-0.8.5-released-with-etl-bug-fixes">Snowplow 0.8.5 released with ETL bug fixes</a></li>
			
		
		</ul>		
	
		<h1>Analytics</h1>
		<ul>
		
			
				<li><a href="/blog/2013/06/26/getting-started-with-r-for-data-analysis-and-visualization">Getting started using R for data analysis</a></li>
			
				<li><a href="/blog/2013/05/22/measuring-how-much-individual-items-in-your-catalog-contribute-to-inbound-marketing">Measuring how much traffic individual items in your catalog drive to your website</a></li>
			
				<li><a href="/blog/2013/05/20/performing-market-basket-analysis-with-r-arules-and-snowplow">Performing market basket analysis on web analytics data with R</a></li>
			
				<li><a href="/blog/2013/05/10/where-does-your-traffic-really-come-from">Where does your traffic *really* come from?</a></li>
			
				<li><a href="/blog/2013/04/23/performing-funnel-analysis-with-snowplow">Funnel analysis with Snowplow (Platform analytics part 1)</a></li>
			
		
		</ul>		
	
		<h1>Inside the Plow</h1>
		<ul>
		
			
				<li><a href="/blog/2013/05/30/dealing-with-hadoops-small-files-problem">Dealing with Hadoop's small files problem</a></li>
			
				<li><a href="/blog/2013/04/10/snowplow-event-validation">Towards high-fidelity web analytics - introducing Snowplow's innovative new event validation capabilities</a></li>
			
				<li><a href="/blog/2013/03/20/rob-slifka-elasticity">Inside the Plow - Rob Slifka's Elasticity</a></li>
			
				<li><a href="/blog/2013/02/08/writing-hive-udfs-and-serdes">Writing Hive UDFs - a tutorial</a></li>
			
				<li><a href="/blog/2013/02/04/help-us-build-out-the-snowplow-event-model">Help us build out the Snowplow Event Model</a></li>
			
		
		</ul>		
	

	<h1>Useful links</h1>
	<ul>
		<li><a href="/blog/atom.xml">Atom feed</a></li>
	</ul>
	<!--<strong>Tags</strong> -->
</div>

		<div id="footer">
	<p>Copyright © Snowplow Analytics Limited 2012 - 2013.  All rights reserved</p>
</div>
	</div>
		<!-- Following Javascript function used by Disqus to count the number of comments for each blog post and display in the main index -->
	  	<script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'snowplow'; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function () {
            var s = document.createElement('script'); s.async = true;
            s.type = 'text/javascript';
            s.src = 'http://' + disqus_shortname + '.disqus.com/count.js';
            (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
        }());
        </script>
        <!-- begin olark code -->
		<script data-cfasync="false" type='text/javascript'>/*<![CDATA[*/window.olark||(function(c){var f=window,d=document,l=f.location.protocol=="https:"?"https:":"http:",z=c.name,r="load";var nt=function(){
		f[z]=function(){
		(a.s=a.s||[]).push(arguments)};var a=f[z]._={
		},q=c.methods.length;while(q--){(function(n){f[z][n]=function(){
		f[z]("call",n,arguments)}})(c.methods[q])}a.l=c.loader;a.i=nt;a.p={
		0:+new Date};a.P=function(u){
		a.p[u]=new Date-a.p[0]};function s(){
		a.P(r);f[z](r)}f.addEventListener?f.addEventListener(r,s,false):f.attachEvent("on"+r,s);var ld=function(){function p(hd){
		hd="head";return["<",hd,"></",hd,"><",i,' onl' + 'oad="var d=',g,";d.getElementsByTagName('head')[0].",j,"(d.",h,"('script')).",k,"='",l,"//",a.l,"'",'"',"></",i,">"].join("")}var i="body",m=d[i];if(!m){
		return setTimeout(ld,100)}a.P(1);var j="appendChild",h="createElement",k="src",n=d[h]("div"),v=n[j](d[h](z)),b=d[h]("iframe"),g="document",e="domain",o;n.style.display="none";m.insertBefore(n,m.firstChild).id=z;b.frameBorder="0";b.id=z+"-loader";if(/MSIE[ ]+6/.test(navigator.userAgent)){
		b.src="javascript:false"}b.allowTransparency="true";v[j](b);try{
		b.contentWindow[g].open()}catch(w){
		c[e]=d[e];o="javascript:var d="+g+".open();d.domain='"+d.domain+"';";b[k]=o+"void(0);"}try{
		var t=b.contentWindow[g];t.write(p());t.close()}catch(x){
		b[k]=o+'d.write("'+p().replace(/"/g,String.fromCharCode(92)+'"')+'");d.close();'}a.P(2)};ld()};nt()})({
		loader: "static.olark.com/jsclient/loader0.js",name:"olark",methods:["configure","extend","declare","identify"]});
		/* custom configuration goes here (www.olark.com/documentation) */
		olark.identify('9752-503-10-5227');/*]]>*/</script><noscript><a href="https://www.olark.com/site/9752-503-10-5227/contact" title="Contact us" target="_blank">Questions? Feedback?</a> powered by <a href="http://www.olark.com?welcome" title="Olark live chat software">Olark live chat software</a></noscript>
		<!-- end olark code -->
		<!-- Track Olark chats in GTM (so can pass data onto Snowplow) -->
		<script type="text/javascript">
		olark('api.chat.onMessageToOperator', function(event) {
		    dataLayer.push({'event': 'olarkMessageToOperator'});
		});
		olark('api.chat.onMessageToVisitor', function(event) {
		    dataLayer.push({'event': 'olarkMessageToVisitor'});
		});
		</script>
		<!-- end track olark code -->


</body>
</html>