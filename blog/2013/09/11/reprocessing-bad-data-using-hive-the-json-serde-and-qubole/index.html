<!DOCTYPE html>
<html>
<head>
	
	<title>Reprocessing bad rows of Snowplow data using Hive, the JSON Serde and Qubole - Snowplow Analytics</title>
	

	<link rel="icon" type="image/x-icon" href="/favicon.ico" />

	<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
	<link href="/static/css/styles.css" type="text/css" rel="stylesheet" />
	<link href="/static/css/pygments.css" type="text/css" rel="stylesheet" />
	
	<!--For the homepage slider-->
	<link rel="stylesheet" href="/static/css/nivo-slider.css" type="text/css" media="screen" />
	<link rel="stylesheet" href="/static/css/nivo-slider-theme-default.css" type="text/css" media="screen" />
	<script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.0/jquery.min.js"></script>
	<script src="/static/js/jquery-nivo-slider-pack.js" type="text/javascript" ></script>
	<!--MathJax http://www.mathjax.org/-->
	<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_HTMLorMML.js"></script>
	<script type="text/javascript">
		MathJax.Hub.Config({
	      tex2jax: {
	        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
	      }
	    });
	    MathJax.Hub.Queue(function() {
	        var all = MathJax.Hub.getAllJax(), i;
	        for(i=0; i < all.length; i += 1) {
	            all[i].SourceElement().parentNode.className += ' has-jax';
	        }
    	});
	</script>
	<!-- end mathjax -->
	<!-- typekit -->
	<script type="text/javascript" src="//use.typekit.net/noo1diw.js"></script>
	<script type="text/javascript">try{Typekit.load();}catch(e){}</script>
	<!-- end typekit -->
</head>
<body>
	<!-- Google Tag Manager -->
	<noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-DLRG"
	height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
	<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
	new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
	j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
	'//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
	})(window,document,'script','dataLayer','GTM-DLRG');</script>
	<!-- End Google Tag Manager -->

	<div id="container">
		<div id="header" class="span-24">
  <div id="logo">
    <h1><a href="/"><img src="/static/img/snowplow-logo-website.png" title="Snowplow Analytics" /></a></h1>
  </div>
  <div id="menu" class="span-15">
    <ul>
      <li ><a href="/product/index.html">Product</a></li>
      <li ><a href="/services/index.html">Services</a></li>
      <li ><a href="/analytics/index.html">Analytics</a></li>
      <li ><a href="/technology/index.html">Technology</a></li>
      <li  class="active" ><a href="/blog.html">Blog</a></li>
      <li ><a id="mail" href="/about/index.html">About</a></li>
    </ul>
  </div>
</div>
	
		<div id="contents">
		<div class="post">
			11 Sep 2013
			<h1>Reprocessing bad rows of Snowplow data using Hive, the JSON Serde and Qubole</h1>
			 <span class="author">Author: <a href="/yali.html" rel="author">Yali Sassoon </a></span>
			<p>One of the distinguishing features of the Snowplow data pipeline is the handling of &#8220;bad&#8221; data. Every row of incoming, raw data is validated. When a row fails validation, it is logged in a &#8220;bad rows&#8221; bucket on S3 alongside the error message that was generated by the failed validation. That means you can keep track of the number of rows that fail validation, and have the opportunity to update and then reprocess those bad rows. (This makes Snowplow different from traditional web analytics platforms, that simply ignore bad rows of data, and provide no insight into the volume of incoming data that ends up being ignored.)</p>

<p>This functionality was crucial in spotting that, in mid-August, Amazon made an <a href='/blog/2013/09/05/snowplow-0.8.9-released-to-handle-cloudfront-log-file-format-change/'>undocumented update the CloudFront collector file format</a>. This resulted in a sudden spike in the number of &#8220;bad rows&#8221; generated by Snowplow, as the <code>cs-uri-query</code> field format changed from the format the Enrichment process expected. (For details of the change, see <a href='/blog/2013/09/05/snowplow-0.8.9-released-to-handle-cloudfront-log-file-format-change/'>this blog post</a>, and the links in it.) Amazon has since rolled back the update, and we have since updated Snowplow to be able to process rows in both formats. However, Snowplow users will have three weeks of data with lines of data missing, that ideally need to be reprocessed using the updated Snowplow version.</p>
<img src='/static/img/blog/2013/09/black_sheep.jpg' title='black sheet - can you spot bad data?' width='300' />
<p>In this blog post, we will walk through:</p>

<ol>
<li>How to use <a href='http://hive.apache.org/'>Apache Hive</a>, <a href='http://www.qubole.com/'>Qubole</a> and <a href='https://github.com/rcongiu'>Robert Congui&#8217;s</a> <a href='https://github.com/rcongiu/Hive-JSON-Serde'>JSON serde</a> to monitor the number of bad rows generated over time</li>

<li>How to use the same tools to reprocess the bad rows of data, so that they are added to your Snowplow data in Redshift / PostgreSQL</li>
</ol>

<p>The steps necessary to reprocess the data will be very similar to those required regardless of the reason that the reprocessing is necessary: as a result, this blog post should be useful for anyone interested in using the bad rows functionality to debug and improve the robustness of their event data collection. It should also be useful for anyone interested in using <a href='http://hive.apache.org/'>Hive</a> and the <a href='https://github.com/rcongiu/Hive-JSON-Serde'>JSON serde</a> to process JSON data in S3. (Bad row data is stored by Snowplow in JSON format.) We will use <a href='http://www.qubole.com/'>Qubole</a>, our preferred platform for running Hive jobs on data in S3, which we previously introduced in <a href='/blog/2013/09/03/using-qubole-to-analyze-snowplow-web-data/'>this blog post</a>.</p>

<ol>
<li><a href='/blog/2013/09/11/reprocessing-bad-data-using-hive-the-json-serde-and-qubole/#how-snowplow-handles-bad-rows'>Understanding how Snowplow handles bad rows</a></li>

<li><a href='/blog/2013/09/11/reprocessing-bad-data-using-hive-the-json-serde-and-qubole/#processing-bad-rows-data-using-json-serde-hive-qubole'>Processing the bad rows data using the JSON serde, Hive and Qubole</a></li>

<li><a href='/blog/2013/09/11/reprocessing-bad-data-using-hive-the-json-serde-and-qubole/#plot-bad-rows-over-time'>Plotting the number of bad rows over time</a></li>

<li><a href='/blog/2013/09/11/reprocessing-bad-data-using-hive-the-json-serde-and-qubole/#processing-bad-rows'>Reprocessing bad rows</a></li>
</ol>
<!--more--><a name='how-snowplow-handles-bad-rows'><h2>1. Understanding how Snowplow handles bad rows</h2></a>
<p>The Snowplow enrichment process takes input lines of data, in the form of collector logs. It validates the format of the data in each of those lines. If the format is as expected, it performs the relevant enrichments on that data (e.g. referer parsing, geo-IP lookups), and writes the enriched data to the Out Bucket on S3, from where it can be loaded into Redshift / PostgreSQL. If the input line of data fails the validation, it gets written to the Bad Rows Bucket on S3.</p>

<p><img alt='flow-chart' src='/static/img/blog/2013/09/snowplow-data-processing-bad-bucket-flow-chart-cropped.png' /></p>

<p>The locations are specified in the <a href='https://github.com/snowplow/snowplow/blob/master/3-enrich/emr-etl-runner/config/config.yml.sample'>EmrEtlRunner config file</a>, an example of which can be found <a href='https://github.com/snowplow/snowplow/blob/master/3-enrich/emr-etl-runner/config/config.yml.sample'>here</a>.</p>
<div class='highlight'><pre><code class='yaml'><span class='l-Scalar-Plain'>:s3</span><span class='p-Indicator'>:</span>
  <span class='l-Scalar-Plain'>:region</span><span class='p-Indicator'>:</span> <span class='l-Scalar-Plain'>ADD HERE</span>
  <span class='l-Scalar-Plain'>:buckets</span><span class='p-Indicator'>:</span>
    <span class='l-Scalar-Plain'>:assets</span><span class='p-Indicator'>:</span> <span class='l-Scalar-Plain'>s3://snowplow-hosted-assets</span> <span class='c1'># DO NOT CHANGE unless you are hosting the jarfiles etc yourself in your own bucket</span>
    <span class='l-Scalar-Plain'>:log</span><span class='p-Indicator'>:</span> <span class='l-Scalar-Plain'>ADD HERE</span>
    <span class='l-Scalar-Plain'>:in</span><span class='p-Indicator'>:</span> <span class='l-Scalar-Plain'>ADD HERE</span>
    <span class='l-Scalar-Plain'>:processing</span><span class='p-Indicator'>:</span> <span class='l-Scalar-Plain'>ADD HERE</span>
    <span class='l-Scalar-Plain'>:out</span><span class='p-Indicator'>:</span> <span class='l-Scalar-Plain'>ADD HERE WITH SUB-FOLDER</span> <span class='c1'># e.g. s3://my-out-bucket/events</span>
    <span class='l-Scalar-Plain'>:out_bad_rows</span><span class='p-Indicator'>:</span> <span class='l-Scalar-Plain'>ADD HERE</span> <span class='c1'># e.g. s3://my-out-bucket/bad-rows</span>
    <span class='l-Scalar-Plain'>:out_errors</span><span class='p-Indicator'>:</span> <span class='l-Scalar-Plain'>ADD HERE</span> <span class='c1'># Leave blank unless :continue_on_unexpected_error: set to true below</span>
    <span class='l-Scalar-Plain'>:archive</span><span class='p-Indicator'>:</span> <span class='l-Scalar-Plain'>ADD HERE</span>
</code></pre></div>
<p>Each bad row is a JSON containing just two fields:</p>

<ol>
<li>A field called <code>line</code> (of type String), which is the <em>raw</em> line of data from the collector log</li>

<li>A field called <code>errors</code> (an Array of Strings), which includes an error message for <em>every</em> validation test the line failed</li>
</ol>

<p>An example row generated for the Snowplow website, caused by Amazon&#8217;s CloudFront log file format update, is shown below (formatted to make it easier to read):</p>
<div class='highlight'><pre><code class='json'><span class='p'>{</span>
    <span class='nt'>&quot;line&quot;</span><span class='p'>:</span> <span class='s2'>&quot;2013-08-19\t04:06:09\tHKG50\t826\t175.159.22.201\tGET\td3v6ndkyapxc2w.cloudfront.net\t/i\t200\thttp://snowplowanalytics.com/analytics/catalog-analytics/market-basket-analysis-identifying-products-that-sell-well-together.html\tMozilla/5.0%20(Macintosh;%20Intel%20Mac%20OS%20X%2010_6_8)%20AppleWebKit/534.57.2%20(KHTML,%20like%20Gecko)%20Version/5.1.7%20Safari/534.57.2\te=pv&amp;page=Market%20basket%20analysis%20-%20identifying%20products%20and%20content%20that%20go%20well%20together%20-%20Snowplow%20Analytics&amp;dtm=1376885168897&amp;tid=479753&amp;vp=1361x678&amp;ds=1346x6578&amp;vid=1&amp;duid=24210ca58692c76e&amp;p=web&amp;tv=js-0.12.0&amp;fp=421731260&amp;aid=snowplowweb&amp;lang=en-us&amp;cs=UTF-8&amp;tz=Asia%2FShanghai&amp;refr=http%3A%2F%2Fwww.google.com%2Furl%3Fsa%3Dt%26rct%3Dj%26q%3Dmarket%2520basket%2520analysis%2520apriori%2520algorithm%26source%3Dweb%26cd%3D9%26sqi%3D2%26ved%3D0CGgQFjAI%26url%3Dhttp%253A%252F%252Fsnowplowanalytics.com%252Fanalytics%252Fcatalog-analytics%252Fmarket-basket-analysis-identifying-products-that-sell-well-together.html%26ei%3DnZkRUp_UF4qdiAem-YHwAg%26usg%3DAFQjCNE8XEB-2ItaXcOC5i2T-jLvpv77uQ%26sig2%3DFPZRScoJkUEg5G2qa8BoBA%26bvm%3Dbv.50768961%2Cd.aGc%26cad%3Drjt&amp;f_pdf=1&amp;f_qt=1&amp;f_realp=0&amp;f_wma=0&amp;f_dir=0&amp;f_fla=1&amp;f_java=1&amp;f_gears=0&amp;f_ag=0&amp;res=1440x900&amp;cd=24&amp;cookie=1&amp;url=http%3A%2F%2Fsnowplowanalytics.com%2Fanalytics%2Fcatalog-analytics%2Fmarket-basket-analysis-identifying-products-that-sell-well-together.html\t-\tRefreshHit\tmEPXmPmaMHvqTD6ung3_IlOgVuNOLnliGz9mVYn29oyOPMDadhuQpQ==&quot;</span><span class='p'>,</span>
    <span class='nt'>&quot;errors&quot;</span><span class='p'>:</span> <span class='p'>[</span>
        <span class='s2'>&quot;Provided URI string [http://www.google.com/url?sa=t&amp;rct=j&amp;q=market basket analysis apriori algorithm&amp;source=web&amp;cd=9&amp;sqi=2&amp;ved=0CGgQFjAI&amp;url=http://snowplowanalytics.com/analytics/catalog-analytics/market-basket-analysis-identifying-products-that-sell-well-together.html&amp;ei=nZkRUp_UF4qdiAem-YHwAg&amp;usg=AFQjCNE8XEB-2ItaXcOC5i2T-jLvpv77uQ&amp;sig2=FPZRScoJkUEg5G2qa8BoBA&amp;bvm=bv.50768961,d.aGc&amp;cad=rjt] violates RFC 2396: [Illegal character in query at index 45: http://www.google.com/url?sa=t&amp;rct=j&amp;q=market basket analysis apriori algorithm&amp;source=web&amp;cd=9&amp;sqi=2&amp;ved=0CGgQFjAI&amp;url=http://snowplowanalytics.com/analytics/catalog-analytics/market-basket-analysis-identifying-products-that-sell-well-together.html&amp;ei=nZkRUp_UF4qdiAem-YHwAg&amp;usg=AFQjCNE8XEB-2ItaXcOC5i2T-jLvpv77uQ&amp;sig2=FPZRScoJkUEg5G2qa8BoBA&amp;bvm=bv.50768961,d.aGc&amp;cad=rjt]&quot;</span>
    <span class='p'>]</span>
<span class='p'>}</span>
</code></pre></div><a name='processing-bad-rows-data-using-json-serde-hive-qubole'><h2>2. Processing the bad rows data using the JSON serde, Hive and Qubole</h2> </a>
<p>There are a couple of ways to process JSON data in Hive. For this tutorial, we&#8217;re going to use Roberto Congiu&#8217;s <a href='https://github.com/rcongiu/Hive-JSON-Serde'>Hive-JSON-Serde</a>. This is our preferred method of working with JSONs in Hive, where your complete data set is stored as a series of JSONs. (When you have a single JSON-formatted field in a regular Hive table, we recommend using the <code>get_json_object</code> UDF to parse the JSON data.)</p>

<p>The Hive-JSON-serde is available <a href='https://github.com/rcongiu/Hive-JSON-Serde'>on Github</a> and can be built using Maven. If you prefer not to compile it for yourself, we have made a hosted version of the compiled JAR available <a href='snowplow-hosted-assets.s3.amazonaws.com/third-party/rcongiu/json-serde-1.1.6-jar-with-dependencies.jar'>here</a>.</p>

<p>Now that we have placed the JSON serde in an S3 location that is accessible to us when we run Hive, we are in a position to fire up Qubole and start analyzing our bad rows data. Log into Qubole via the web UI to get started and open up the <strong>Composer</strong> window. (If you have not tried Qubole yet, we recommend you <a href='https://github.com/snowplow/snowplow/wiki/Setting-up-Qubole-to-analyze-Snowplow-data-using-Apache-Hive'>read our guide to getting started with Qubole</a>.)</p>

<p>Now enter the following in the Qubole Composer:</p>

<pre><code>ADD JAR s3://snowplow-hosted-assets/third-party/rcongiu/json-serde-1.1.6-jar-with-dependencies.jar;</code></pre>

<p>After a short period Qubole should alert you that the JAR has been successfully uploaded:</p>

<p><img alt='qubole-pic-1' src='/static/img/blog/2013/09/qubole-add-jar.png' /></p>

<p>Now we need to define a table so that Hive can query our bad row data in S3. Execute the following query in the Qubole Composer, making sure that you update the <code>LOCATION</code> setting to point to the location in S3 where your bad rows are stored. (This can be worked out from your EmrEtlRunner&#8217;s <code>config.yml</code> file, as explained <a href='#how-snowplow-handles-bad-rows'>above</a>).</p>
<div class='highlight'><pre><code class='mysql'><span class='k'>CREATE</span> <span class='n'>EXTERNAL</span> <span class='k'>TABLE</span> <span class='ss'>`bad_rows`</span> <span class='p'>(</span>
	<span class='n'>line</span> <span class='n'>string</span><span class='p'>,</span>
	<span class='n'>errors</span> <span class='n'>array</span><span class='o'>&lt;</span><span class='n'>string</span><span class='o'>&gt;</span>
<span class='p'>)</span> 
<span class='n'>PARTITIONED</span> <span class='k'>BY</span> <span class='p'>(</span><span class='n'>run</span> <span class='n'>string</span><span class='p'>)</span>
<span class='n'>ROW</span> <span class='n'>FORMAT</span> <span class='n'>SERDE</span> <span class='s1'>&#39;org.openx.data.jsonserde.JsonSerDe&#39;</span>
<span class='n'>STORED</span> <span class='k'>AS</span> <span class='n'>TEXTFILE</span>
<span class='n'>LOCATION</span> <span class='s1'>&#39;s3n://snowplow-data/snplow/bad-rows/&#39;</span><span class='p'>;</span>
</code></pre></div>
<p><img alt='qubole-pic-2' src='/static/img/blog/2013/09/qubole-create-table.png' /></p>

<p>Our table is partitioned by <code>run</code> - each time the Snowplow enrichment process is run (in our case daily), any bad rows are saved in their own separate subfolder labelled <code>run=2013-xx-xx...</code>. Let&#8217;s recover those partitions, by executing the following:</p>
<div class='highlight'><pre><code class='mysql'><span class='k'>ALTER</span> <span class='k'>TABLE</span> <span class='ss'>`bad_rows`</span> <span class='n'>RECOVER</span> <span class='n'>PARTITIONS</span><span class='p'>;</span>
</code></pre></div><a name='plot-bad-rows-over-time'><h2>3. Plotting the number of bad rows over time</h2></a>
<p>We run the Snowplow ETL once a day. As a result, each &#8220;run&#8221; represents one days worth of data. By counting the number of bad rows per run, we effectively calculate the number of bad rows of data generated per day. We can do that by executing the following query:</p>
<div class='highlight'><pre><code class='mysql'><span class='k'>SELECT</span> 
<span class='n'>run</span><span class='p'>,</span>
<span class='nf'>count</span><span class='p'>(</span><span class='o'>*</span><span class='p'>)</span>
<span class='k'>FROM</span> <span class='ss'>`bad_rows`</span>
<span class='k'>GROUP</span> <span class='k'>BY</span> <span class='n'>run</span><span class='p'>;</span>
</code></pre></div>
<p><img alt='qubole-pic-3' src='/static/img/blog/2013/09/qubole-execute-count-query.png' /></p>

<p>Execute that in Qubole, and then download your results. (By clicking the <strong>Download</strong> link in the UI. If you open them in Excel, you should see something as follows:</p>
<table><thead><tr><th><strong>Run ID</strong></th><th><strong>Number of bad rows</strong></th></tr></thead><tbody><tr><td style='text-align: left;'>2013-08-17-03-00-02</td><td style='text-align: left;'>6</td>
</tr><tr><td style='text-align: left;'>2013-08-18-03-00-03</td><td style='text-align: left;'>2</td>
</tr><tr><td style='text-align: left;'>2013-08-19-03-00-03</td><td style='text-align: left;'>3</td>
</tr><tr><td style='text-align: left;'>&#8230;</td><td style='text-align: left;'>&#8230;</td>
</tr></tbody></table>
<p>(We have added the headers to the table above - these will not be downloaded)</p>

<p>We can plot the data directly in Excel:</p>

<p><img alt='excel-graph' src='/static/img/blog/2013/09/excel-graph-of-bad-rows-per-day.JPG' /></p>

<p>Notice:</p>

<ul>
<li>We have <em>no</em> bad rows before August 17th, when Amazon updated their Cloudfront log format</li>

<li>We then have bad rows every day since. (In our case, this varies between 2-25. This is on the Snowplow site, which attracts c.200 uniques per day.)</li>
</ul>
<a name='processing-bad-rows'><h2>4. Reprocessing bad rows</h2></a>
<p>Using plots like the one above to spot emerging problems with your Snowplow data pipeline is one thing. When you&#8217;ve identified the cause of the problem, and fixed it (as we have), you then need to reprocess those bad lines of data.</p>

<p>Fortunately, this is pretty straightforward. We need to extract the bad lines out of the JSONs, and write them back into a new location in S3 in their raw form. We can then set the <code>IN</code> bucket on the EmrEtlRunner to point to this new location, and run the updated Enrichment process on the data.</p>

<p>To extract the raw lines of data out of the JSONs, we first create another external table in Hive, this time in the location where we will save the data to be reprocessed:</p>
<div class='highlight'><pre><code class='mysql'><span class='k'>CREATE</span> <span class='n'>EXTERNAL</span> <span class='k'>TABLE</span> <span class='ss'>`data_to_reprocess`</span> <span class='p'>(</span>
	<span class='n'>line</span> <span class='n'>string</span>  
<span class='p'>)</span>
<span class='n'>ROW</span> <span class='n'>FORMAT</span> <span class='n'>DELIMITED</span>
<span class='k'>LINES</span> <span class='k'>TERMINATED</span> <span class='k'>BY</span> <span class='s1'>&#39;\n&#39;</span>
<span class='n'>STORED</span> <span class='k'>AS</span> <span class='n'>TEXTFILE</span>
<span class='n'>LOCATION</span> <span class='s1'>&#39;s3n://qubole-analysis/data-to-reprocess/snplow/2013-09-11/&#39;</span><span class='p'>;</span>
</code></pre></div>
<p>Note:</p>

<ul>
<li>We&#8217;ve created our table in the special bucket that we&#8217;ve given Qubole unrestricted write access to</li>

<li>We&#8217;ve created a specific folder in that bucket for the new data, so it will be easy to find later</li>
</ul>

<p>Now that we&#8217;ve created our table, we need to insert into it the bad rows to reprocess:</p>
<div class='highlight'><pre><code class='mysql'><span class='k'>INSERT</span> <span class='k'>INTO</span> <span class='k'>TABLE</span> <span class='ss'>`data_to_reprocess`</span>
<span class='k'>SELECT</span> <span class='n'>line</span>
<span class='k'>FROM</span> <span class='ss'>`bad_rows`</span><span class='p'>;</span>
</code></pre></div>
<p>Note how we are <strong>only</strong> writing the actual raw line of data into the new table (and ignoring everything else in the <code>bad_rows</code> table, including both the <code>run</code> and the actual error message itself).</p>

<p>Bingo! When the query is complete, the data to reprocess is available in the new bucket we&#8217;ve created:</p>

<p><img alt='s3-pic' src='/static/img/blog/2013/09/file_with_lines_of_data_to_reprocess.png' /></p>

<p>We now need to run the Snowplow Enrichment process on this new data set. We do that using EmrEtlRunner. Navigate to the server you run EmrEtlRunner from, and navigate to the directory it is installed in.</p>

<p>Now, create a copy of your <a href='https://github.com/snowplow/snowplow/blob/master/3-enrich/emr-etl-runner/config/config.yml.sample'>EmrEtlRunner config.yml</a> with a suitable name e.g. <code>config-process-bad-rows-2013-09-11.yml</code> and update the In Bucket to point to the location of the the data to be reprocessed is (i.e. the location of the Hive <code>data_to_reprocess</code> table). Don&#8217;t forget as well to update (if you haven&#8217;t already done so) the ETL to the latest version, which can handle the change in Amazon&#8217;s CloudFront log file format:</p>
<div class='highlight'><pre><code class='yaml'><span class='l-Scalar-Plain'>:snowplow</span><span class='p-Indicator'>:</span>
  <span class='l-Scalar-Plain'>:hadoop_etl_version</span><span class='p-Indicator'>:</span> <span class='l-Scalar-Plain'>0.3.4</span> <span class='c1'># Version of the Hadoop ETL</span>
</code></pre></div>
<p>Now execute the following command at the command line:</p>
<div class='highlight'><pre><code class='bash'><span class='nv'>$ </span>bundle <span class='nb'>exec </span>bin/snowplow-emr-etl-runner --config config/config-process-bad-rows-2013-09-11.yml
</code></pre></div>
<p>Make sure you update the path to point at the name of the config file you created in the previous step. This should kick off the Enrichment process in EMR. Once it has been completed, you can run the StorageLoader to load the newly processed data into Redshift / PostgreSQL as normal:</p>
<div class='highlight'><pre><code class='bash'><span class='nv'>$ </span><span class='nb'>cd</span> ../../4-storage/storage-loader
<span class='nv'>$ </span>bundle <span class='nb'>exec </span>bin/snowplow-storage-loader --config config/config.yml
</code></pre></div>
<p>Done! The data that was previously excluded has now been added to your Snowplow database!</p>
			<div class="author_summary">
				<h2>About the author</h2>
				<div class="author_image"><img src="https://lh4.googleusercontent.com/--uMP0uMpzEs/AAAAAAAAAAI/AAAAAAAABH0/lo82KAkjEIU/s120-c/photo.jpg" /></div> <div class="author_spiel">
  <a href="/yali.html">Yali</a> is co-founder and analytics lead at Snowplow Analytics. You can find in him on <a href="https://plus.google.com/u/0/106510540736941709264" rel="author">Google+</a>, <a href="https://twitter.com/yalisassoon">Twitter</a> and <a href="http://uk.linkedin.com/in/yalisassoon">LinkedIn</a>.
</div>

			</div> 
			<div id="comments">
	<h2>Questions? Comments? Join the debate!</h2>
	 <div id="disqus_thread"></div>
        <script type="text/javascript">
            /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
            var disqus_shortname = 'snowplow'; // required: replace example with your forum shortname
            /* var disqus_identifier =  ; // unique ID so that disqus fetches the correct comments for each post
            var disqus_url =  ;
            var disqus_title =  ; */

            /* * * DON'T EDIT BELOW THIS LINE * * */
            (function() {
                var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
                dsq.src = 'http://' + disqus_shortname + '.disqus.com/embed.js';
                (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
            })();
        </script>
        <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
        <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</div>
		</div>
		<p>Return to the <a href="/blog.html">main blog page</a></p>
		

</div>

<div id="sidebar">
	<h1>Recent posts</h1>
	<ul>
		
			<li><a href="/blog/2013/10/21/scripting-hadoop-part-1-adventures-with-scala-rhino-and-javascript">Scripting Hadoop, Part One - Adventures with Scala, Rhino and JavaScript</a></li>
		
			<li><a href="/blog/2013/10/18/snowplow-0.8.10-released-with-analytics-recipes-and-cubes">Snowplow 0.8.10 released with analytics cubes and recipes 'baked in'</a></li>
		
			<li><a href="/blog/2013/10/07/announcing-our-winter-open-source-internship-program">Announcing our winter open source internship program</a></li>
		
			<li><a href="/blog/2013/10/01/snowplow-passes-500-stars">Snowplow passes 500 stars on GitHub</a></li>
		
			<li><a href="/blog/2013/09/30/book-review-instant-hive-essentials-how-to">Book review - Apache Hive Essentials How-to</a></li>
		
	</ul>

	
		<h1>Other</h1>
		<ul>
		
			
				<li><a href="/blog/2013/10/01/snowplow-passes-500-stars">Snowplow passes 500 stars on GitHub</a></li>
			
				<li><a href="/blog/2013/09/30/book-review-instant-hive-essentials-how-to">Book review - Apache Hive Essentials How-to</a></li>
			
				<li><a href="/blog/2013/09/11/reprocessing-bad-data-using-hive-the-json-serde-and-qubole">Reprocessing bad rows of Snowplow data using Hive, the JSON Serde and Qubole</a></li>
			
				<li><a href="/blog/2013/07/19/snowplow-presentation-to-hadoop-user-group-london-aws-event">Snowplow presentation at the Hadoop User Group London AWS event</a></li>
			
				<li><a href="/blog/2013/07/10/help-us-build-out-the-snowplow-total-cost-of-ownership-model">Help us build out the Snowplow Total Cost of Ownership Model</a></li>
			
		
		</ul>		
	
		<h1>Releases</h1>
		<ul>
		
			
				<li><a href="/blog/2013/10/18/snowplow-0.8.10-released-with-analytics-recipes-and-cubes">Snowplow 0.8.10 released with analytics cubes and recipes 'baked in'</a></li>
			
				<li><a href="/blog/2013/09/05/snowplow-0.8.9-released-to-handle-cloudfront-log-file-format-change">Snowplow 0.8.9 released to handle CloudFront log file format change</a></li>
			
				<li><a href="/blog/2013/08/05/snowplow-0.8.8-released-with-postgres-and-hive-support">Snowplow 0.8.8 released with Postgres and Hive support</a></li>
			
				<li><a href="/blog/2013/07/09/dotnet-support-added-to-referer-parser">.NET (C#) support added to referer-parser</a></li>
			
				<li><a href="/blog/2013/07/07/snowplow-0.8.7-released">Snowplow 0.8.7 released with JavaScript Tracker improvements</a></li>
			
		
		</ul>		
	
		<h1>Analytics</h1>
		<ul>
		
			
				<li><a href="/blog/2013/09/03/using-qubole-to-analyze-snowplow-web-data">Using Qubole to crunch your Snowplow web data using Apache Hive</a></li>
			
				<li><a href="/blog/2013/06/26/getting-started-with-r-for-data-analysis-and-visualization">Getting started using R for data analysis</a></li>
			
				<li><a href="/blog/2013/05/22/measuring-how-much-individual-items-in-your-catalog-contribute-to-inbound-marketing">Measuring how much traffic individual items in your catalog drive to your website</a></li>
			
				<li><a href="/blog/2013/05/20/performing-market-basket-analysis-with-r-arules-and-snowplow">Performing market basket analysis on web analytics data with R</a></li>
			
				<li><a href="/blog/2013/05/10/where-does-your-traffic-really-come-from">Where does your traffic *really* come from?</a></li>
			
		
		</ul>		
	
		<h1>Inside the Plow</h1>
		<ul>
		
			
				<li><a href="/blog/2013/09/27/how-much-does-snowplow-cost-to-run">How much does Snowplow cost to run, vs the competition?</a></li>
			
				<li><a href="/blog/2013/08/12/towards-universal-event-analytics-building-an-event-grammar">Towards universal event analytics - building an event grammar</a></li>
			
				<li><a href="/blog/2013/07/09/understanding-how-different-parts-of-the-Snowplow-data-pipeline-drive-AWS-costs">Unpicking the Snowplow data pipeline and how it drives AWS costs</a></li>
			
				<li><a href="/blog/2013/05/30/dealing-with-hadoops-small-files-problem">Dealing with Hadoop's small files problem</a></li>
			
				<li><a href="/blog/2013/04/10/snowplow-event-validation">Towards high-fidelity web analytics - introducing Snowplow's innovative new event validation capabilities</a></li>
			
		
		</ul>		
	
		<h1>Recruitment</h1>
		<ul>
		
			
				<li><a href="/blog/2013/10/07/announcing-our-winter-open-source-internship-program">Announcing our winter open source internship program</a></li>
			
		
		</ul>		
	
		<h1>Research</h1>
		<ul>
		
			
				<li><a href="/blog/2013/10/21/scripting-hadoop-part-1-adventures-with-scala-rhino-and-javascript">Scripting Hadoop, Part One - Adventures with Scala, Rhino and JavaScript</a></li>
			
		
		</ul>		
	

	<h1>Useful links</h1>
	<ul>
		<li><a href="/blog/atom.xml">Atom feed</a></li>
	</ul>
	<!--<strong>Tags</strong> -->
</div>

		<div id="footer">
	<p>Copyright © Snowplow Analytics Limited 2012 - 2013.  All rights reserved</p>
</div>
	</div>
		<!-- Following Javascript function used by Disqus to count the number of comments for each blog post and display in the main index -->
	  	<script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'snowplow'; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function () {
            var s = document.createElement('script'); s.async = true;
            s.type = 'text/javascript';
            s.src = 'http://' + disqus_shortname + '.disqus.com/count.js';
            (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
        }());
        </script>
        <!-- begin olark code -->
		<script data-cfasync="false" type='text/javascript'>/*<![CDATA[*/window.olark||(function(c){var f=window,d=document,l=f.location.protocol=="https:"?"https:":"http:",z=c.name,r="load";var nt=function(){
		f[z]=function(){
		(a.s=a.s||[]).push(arguments)};var a=f[z]._={
		},q=c.methods.length;while(q--){(function(n){f[z][n]=function(){
		f[z]("call",n,arguments)}})(c.methods[q])}a.l=c.loader;a.i=nt;a.p={
		0:+new Date};a.P=function(u){
		a.p[u]=new Date-a.p[0]};function s(){
		a.P(r);f[z](r)}f.addEventListener?f.addEventListener(r,s,false):f.attachEvent("on"+r,s);var ld=function(){function p(hd){
		hd="head";return["<",hd,"></",hd,"><",i,' onl' + 'oad="var d=',g,";d.getElementsByTagName('head')[0].",j,"(d.",h,"('script')).",k,"='",l,"//",a.l,"'",'"',"></",i,">"].join("")}var i="body",m=d[i];if(!m){
		return setTimeout(ld,100)}a.P(1);var j="appendChild",h="createElement",k="src",n=d[h]("div"),v=n[j](d[h](z)),b=d[h]("iframe"),g="document",e="domain",o;n.style.display="none";m.insertBefore(n,m.firstChild).id=z;b.frameBorder="0";b.id=z+"-loader";if(/MSIE[ ]+6/.test(navigator.userAgent)){
		b.src="javascript:false"}b.allowTransparency="true";v[j](b);try{
		b.contentWindow[g].open()}catch(w){
		c[e]=d[e];o="javascript:var d="+g+".open();d.domain='"+d.domain+"';";b[k]=o+"void(0);"}try{
		var t=b.contentWindow[g];t.write(p());t.close()}catch(x){
		b[k]=o+'d.write("'+p().replace(/"/g,String.fromCharCode(92)+'"')+'");d.close();'}a.P(2)};ld()};nt()})({
		loader: "static.olark.com/jsclient/loader0.js",name:"olark",methods:["configure","extend","declare","identify"]});
		/* custom configuration goes here (www.olark.com/documentation) */
		olark.identify('9752-503-10-5227');/*]]>*/</script><noscript><a href="https://www.olark.com/site/9752-503-10-5227/contact" title="Contact us" target="_blank">Questions? Feedback?</a> powered by <a href="http://www.olark.com?welcome" title="Olark live chat software">Olark live chat software</a></noscript>
		<!-- end olark code -->
		<!-- Track Olark chats in GTM (so can pass data onto Snowplow) -->
		<script type="text/javascript">
		olark('api.chat.onMessageToOperator', function(event) {
		    dataLayer.push({'event': 'olarkMessageToOperator'});
		});
		olark('api.chat.onMessageToVisitor', function(event) {
		    dataLayer.push({'event': 'olarkMessageToVisitor'});
		});
		</script>
		<!-- end track olark code -->


</body>
</html>