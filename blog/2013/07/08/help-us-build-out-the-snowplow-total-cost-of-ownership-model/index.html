<!DOCTYPE html>
<html>
<head>
	
	<title>Help us build out the Snowplow Total Cost of Ownership Model - Snowplow Analytics</title>
	

	<link rel="icon" type="image/x-icon" href="/favicon.ico" />

	<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
	<link href="/static/css/styles.css" type="text/css" rel="stylesheet" />
	<link href="/static/css/pygments.css" type="text/css" rel="stylesheet" />
	
	<!--For the homepage slider-->
	<link rel="stylesheet" href="/static/css/nivo-slider.css" type="text/css" media="screen" />
	<link rel="stylesheet" href="/static/css/nivo-slider-theme-default.css" type="text/css" media="screen" />
	<script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.0/jquery.min.js"></script>
	<script src="/static/js/jquery-nivo-slider-pack.js" type="text/javascript" ></script>
	<!--MathJax http://www.mathjax.org/-->
	<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_HTMLorMML.js"></script>
	<script type="text/javascript">
		MathJax.Hub.Config({
	      tex2jax: {
	        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
	      }
	    });
	    MathJax.Hub.Queue(function() {
	        var all = MathJax.Hub.getAllJax(), i;
	        for(i=0; i < all.length; i += 1) {
	            all[i].SourceElement().parentNode.className += ' has-jax';
	        }
    	});
	</script>
	<!-- end mathjax -->
	<!-- typekit -->
	<script type="text/javascript" src="//use.typekit.net/noo1diw.js"></script>
	<script type="text/javascript">try{Typekit.load();}catch(e){}</script>
	<!-- end typekit -->
</head>
<body>
	<!-- Google Tag Manager -->
	<noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-DLRG"
	height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
	<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
	new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
	j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
	'//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
	})(window,document,'script','dataLayer','GTM-DLRG');</script>
	<!-- End Google Tag Manager -->

	<div id="container">
		<div id="header" class="span-24">
  <div id="logo">
    <h1><a href="/"><img src="/static/img/snowplow-logo-website.png" title="Snowplow Analytics" /></a></h1>
  </div>
  <div id="menu" class="span-15">
    <ul>
      <li ><a href="/product/index.html">Product</a></li>
      <li ><a href="/services/index.html">Services</a></li>
      <li ><a href="/analytics/index.html">Analytics</a></li>
      <li ><a href="/technology/index.html">Technology</a></li>
      <li  class="active" ><a href="/blog.html">Blog</a></li>
      <li ><a id="mail" href="/about/index.html">About</a></li>
    </ul>
  </div>
</div>
	
		<div id="contents">
		<div class="post">
			08 Jul 2013
			<h1>Help us build out the Snowplow Total Cost of Ownership Model</h1>
			 <span class="author">Author: <a href="/yali.html" rel="author">Yali Sassoon </a></span>
			<p>Back in March, <a href='https://groups.google.com/forum/#!searchin/snowplow-user/cloudfront$20cost/snowplow-user/b_HPkt3nwzo/Ms-J54e8bUYJ'>Robert Kingston suggested</a> that we develop a Total Cost of Ownership model for Snowplow: something that would enable a user or prospective user to accurately estimate their Amazon Web Services monthly charges going forwards, and see how those costs vary with different traffic levels. We thought this was an excellent idea.</p>

<p>Since Rob&#8217;s suggestion, we&#8217;ve made a number of important changes to the Snowplow platform that have changed the way Snowplow costs scale with the number of events served:</p>

<ol>
<li>We replaced the Hive-based ETL with the <a href='/blog/2013/04/03/snowplow-0.8.0-released-with-all-new-scalding-based-data-enrichment/'>Scalding-based enrichment process</a></li>

<li>We dealt with the <a href='/blog/2013/05/30/dealing-with-hadoops-small-files-problem/'>small files problem</a>, dramatically reducing EMR costs</li>

<li>We enabled support for <a href='/blog/2013/06/03/snowplow-0.8.6-released-with-performance-improvements/#task-instances'>task and spot instances</a></li>
</ol>

<p>As a result of the pending updates, we held off building the model. But now that they have been delivered, we have started putting together the model. In this post we&#8217;ll cover:</p>

<ol>
<li><a href='/blog/2013/07/08/help-us-build-out-the-snowplow-total-cost-of-ownership-model#details'>How the model will work: what drives AWS costs for Snowplow users</a></li>

<li><a href='/blog/2013/07/08/help-us-build-out-the-snowplow-total-cost-of-ownership-model#help'>How you can help</a></li>
</ol>

<p>We think walking through the <a href='#details'>mechanics of the model</a> should be useful both to help Snowplow users understand the drivers of their AWS fees, and also an interesting exercise in mathematical modelling: we will open source the model that we build, and use it to power a calculator on our website, so that anyone can use it to estimate their Snowplow AWS costs.</p>

<p><img alt='your-country-needs-you-image' src='/static/img/blog/2013/07/your-country-needs-you.jpg' /></p>

<p>We hope Snowplow users will help us build the model by sharing with us some sample data points (related to the number size of files processed by the Snowplow data pipeline), so that we can build a bottom-up model that is as accurate at low volumes as it is at estimating the costs associated with processing billions of lines of rows per day. And we intend that the model can easily be updated as Amazon reduce their AWS pricing, as is becoming increasingly frequent.</p>

<p>As an extra incentive, we&#8217;re offering every Snowplow user who shares some of the data requested below a <strong>free Snowplow T-shirt</strong> :-).</p>
<!--more--><h2><a name='details'>1. How the model will work: what drives AWS costs for Snowplow users</a></h2>
<p>It is worth distinguishing the different AWS services, and examining how each scales with volume of events per day, and over time. If we take a <em>typical</em> Snowplow user (i.e. one running the <a href='https://github.com/snowplow/snowplow/tree/master/2-collectors/cloudfront-collector'>Cloudfront collector</a> rather than the <a href='https://github.com/snowplow/snowplow/tree/master/2-collectors/clojure-collector'>Clojure collector</a>), and storing their data on Redshift for analysis, rather than analyzing their data in S3 using EMR) then we need to account for:</p>

<ol>
<li><a href='#cloudfront'>Cloudfront costs</a>,</li>

<li><a href='#s3'>S3 costs</a>,</li>

<li><a href='#emr'>EMR costs</a> and</li>

<li><a href='#redshift'>Redshift costs</a></li>
</ol>
<h3><a name='cloudfront'>1.1 Cloudfront costs</a></h3>
<p>Snowplow uses Cloudfront to serve both <code>sp.js</code>, the Snowplow Javascript, and the Snowplow pixel <code>i</code>. Broadly, Cloudfront costs scale linearly with the volume of data served out of Cloudfront, with a couple of provisos:</p>

<ul>
<li>The cost per MB served goes down, as volumes rise (because Amazon tier their pricing)</li>

<li>The exact cost varies by AWS account region, and the region the browser that loads the content served by Cloudfront is in (so the exact cost depends on the distribution of your visitors geographically)</li>
</ul>

<p>Modelling the costs is therefore reasonably straightforward. The <code>i</code> pixel is served with every event tracked: so we can calculate the cost of serving <code>i</code> based on the size of <code>i</code> (it is 37 bytes), the number of events, and the geographic split of visitors by Amazon region. The costs scale almost linearly with the number of events tracked per day.</p>

<p>Modelling the cost of serving <code>sp.js</code> is a little trickier. As discussed in our blog post on <a href='/blog/2013/07/02/reduce-your-cloudfront-bills-with-cache-control/'>browser caching</a>, it is possible to set <code>sp.js</code> so that it is only served once per unique visitor, rather than once per event. Because <code>sp.js</code> is 37KB (so a lot larger than <code>i</code>), this has a significant impact on your Cloudfront costs. From a modelling perspective, then, we should estimate costs, based on the number of unique visitors per month, and their geographic distribution by Amazon regions. The costs scale almost linearly with the number of unique visitors to the site / network.</p>
<h3><a name='s3'>1.2 S3 costs</a></h3>
<p>Snowplow uses S3 to store event data. Amazon charges for S3 based on:</p>

<ul>
<li>The volume of data stored in S3</li>

<li>The number of requests for that data (in the form of <code>GET</code> / <code>PUT</code> / <code>COPY</code> requests)</li>
</ul>

<p>Modelling how the volume of data grows with increasing numbers of events, and over time (as the amount of data stored in S3 grows, because Snowplow users generally never throw data away) is straightforward: we calculate how large a &#8216;typical&#8217; row of Snowplow data is in the raw collector logs, and then in the enriched Snowplow event files. We then assume one row of each type for every event that has been tracked, sum to find the total required storage space, and then multiply by Amazon S3&#8217;s cost per GB.</p>

<p>What is more tricky is modelling the number of requests to S3 for the files. To understand why, we need to examine the Snowplow data pipeline, and in particular, the part of the pipeline that takes the raw data generated by the Snowplow collector, cleans that data, enriches it, and uploads the enriched data into Amazon Redshift for analysis.</p>

<p>The first part of the data pipeline is orchestrated by <a href='https://github.com/snowplow/snowplow/wiki/setting-up-EmrEtlRunner'>EmrEtlRunner</a>. This performs the bulk of data processing work:</p>

<p><img alt='emr-etl-runner' src='/static/img/blog/2013/07/emr-etl-runner-steps.png' /></p>

<p>This encapsulates the bulk of the data processing:</p>

<ul>
<li>Raw collector log files that need to be processed are identified in the in-bucket, and moved to the processing bucket</li>

<li>EmrEtlRunner then triggers the Enrichment process to run. This spins up an EMR cluster, loads the data in the processing bucket into HDFS, loads Scalding Enrichment process (as a JAR) and uses that JAR to process the raw logs uploaded into HDFS. The output of that processing is written back, into HDFS</li>

<li>The output of that Scalding Enrichment is then copied from HDFS into the out-bucket in S3. The EMR cluster is then shut down.</li>

<li>Once the job has completed, the raw logs are moved from the processing bucket to the archive bucket.</li>
</ul>

<p>For modelling AWS costs (and S3 costs in particular), we need to note that two <code>COPY</code> requests are executed for each collector log file written to S3: one to move that data from the in-bucket to the processing-bucket, and then another one to move the same file from the processing-bucket to the archive bucket.</p>

<p>In addition, one <code>GET</code> request is executed per raw collector log file, when the data is read from S3 for the purposes of writing into HDFS.</p>

<p>The second part of the data pipeline is orchestrated by the <a href='https://github.com/snowplow/snowplow/wiki/1-Installing-the-StorageLoader'>StorageLoader</a>:</p>

<p><img alt='storage-loader' src='/static/img/blog/2013/07/storage-loader-steps.png' /></p>

<p>This is a much simpler stage of the data processing pipeline:</p>

<ul>
<li>Data from enriched event files generated by the Scalding process on EMR is read and written to Amazon Redshift</li>

<li>The enriched event files are then moved from the in-bucket (which <em>was</em> the archive bucket for EmrEtlRunner) to the archive bucket (for the StorageLoader)</li>
</ul>

<p>Again, for modelling purposes, we note that a single <code>GET</code> request is executed for each enriched event file (when the file is read for the purposes of copying the data into Redshift). and then a <code>COPY</code> request is executed for that file to move it from the in to the out bucket.</p>

<p>This means that we can accurately forecast costs based on the number of raw collector log files generated and the number of enriched event files generated. Unfortunately, modelling how this number scales with visitors / page views and events is <em>not</em> straightforward because it is not clear how the number of collector log files and Snowplow event files scales with numbers of events tracked. This is one of the things <a href='#help'>we hope the community can help us with</a>, by providing us with data points to help us unpick the relevant relationships.</p>

<p>To articulate our challenge: we need to understand</p>

<ol>
<li>How the number of raw collector log files (for the Cloudfront collector) scales with number of events</li>

<li>How the number of enriched Snowplow event files scale with the number of events</li>
</ol>

<p>We have working hypotheses for what determines both numbers. We need to validate these with the Snowplow community, and quantify the relationships mathematically, based on data points shared by members of our community:</p>

<p>We believe that Amazon Cloudfront generates one log file per server per edge location every hour. That means that Snowplow users who do not track large volumes of traffic, will generate a surprisingly large number of log files, each with very low volumes of data. (E.g. 1-5 rows.) As traffic levels climb, the number of log files will increase (more requests to more edge locations is bound to hit more Cloudfront servers), but that this tails off reasonably rapidly once there are enough visitors that most servers are hit at most edge locations every hour. (We guess that Amazon has <em>lots</em> of servers at each edge location, so this tailing off might only happen at very large volumes.) We&#8217;d therefore expect a graph like the one below, if we plotted numbers of log files vs events:</p>

<p><img alt='line-graph' src='/static/img/blog/2013/07/line-graph.png' /></p>

<p>In the case of forecasting the number of Snowplow Event files: this should be more straightforward. We believe that the Scalding Enrichment Process generates one output file for every input file that it receives. The Scalding Enrichment process does <em>not</em> operate on the raw collector logs: to speed up data processing (and reduce costs), these are aggregated using S3DistCopy prior to being fed into the Enrichment Process. This <em>should</em> aggregate the input log files so they are as close to 128MB each as possible. If one output file is produced for each input file, then they will have the same number of rows: it is likely that there is reasonably constant relationship between the size of the two, that mean they are of similar, but not identical, size. (Because they have roughly the same data, but in different formats. with different levels of row-level enrichments.) If that is right, then the number of event files should scale almost linearly with the number of events recorded: almost because given the large maximum file size (roughly 128MBs) the graph would look more like a series of step functions, where a new step change occurs each time an additional 128MBs of events are recorded:</p>

<p><img alt='step-function' src='/static/img/blog/2013/07/step-function.png' /></p>

<p>As we <a href='#help'>explain below</a>, we hope to plot the above graphs with data points volunteered by Snowplow users, to see if we are correct, and then use to drive the model.</p>
<h3><a name='emr'>1.3 EMR costs</a></h3>
<p>Snowplow uses EMR to run the Enrichment process on the raw logs created by the collector. Because the process is powered by Hadoop, it should scale linearly: double the number of lines of data to be processed, the time to process should double. Double the number of machines in the cluster, the processing time should be halved.</p>

<p>Amazon charges for EMR based on the number of boxes in the cluster, the size of those boxes and the time the cluster is live for. Amazon rounds up the nearest hour: as a result, we would expect the cost of the daily job to be a step function.</p>

<p><img alt='emr-costs' src='/static/img/blog/2013/07/emr-costs.png' /></p>

<p>We need to work out how many lines of data we can expect each Amazon box to process in one hour. Getting a handle on these figures will also mean we can advise Snowplow users what size of cluster to spin up based on the number of events processed per run.</p>
<h3><a name='redshift'>1.4 Redshift costs</a></h3>
<p>The majority of Snowplow users store their events table in Amazon Redshift, and then plug analytics tools into Redshift to crunch that data. As a result, they run a Redshift cluster constantly.</p>

<p>Amazon charges per Redshift node, where each node provides a 2TB of storage (for standard XL nodes) or 16TB of storage (for 8XL nodes). As a result, we can model Redshift costs simply as a function of the volume of data stored (itself just a function of the number of events tracked per day, and the number of days Snowplow has been running). As for EMR, we expect a step function (with a big increase in cost each time an additional 2TB node is required):</p>

<p><img alt='redshift-costs' src='/static/img/blog/2013/07/redshift-costs.png' /></p>
<h2><a name='help'>How <i>you</i> can help</a></h2>
<p>As should hopefully be clear from reading the above, we need Snowplow users to share with us the following data, to help us accurately model the above relationships:</p>

<ol>
<li><a href='#events-per-day'>The number of events tracked per day</a></li>

<li><a href='#runs-per-day'>The number of times the enrichment process is run per day</a></li>

<li><a href='#log-files-per-day'>The number of Cloudfront log files generated per day, adn the volume of data</a></li>

<li><a href='#emr-details'>The amount of time taken to enrich the data in EMR (and the size of cluster used to perform the enrichment)</a></li>

<li><a href='#output-back-to-s3'>The number of files outputted back to S3, and the size of those files</a></li>

<li><a href='#redshift-data-points'>The total number of lines of data in Redshift, and the amount of Redshift capacity used</a></li>
</ol>

<p>We will then share this data back, in an anonymized form, with the community, as part of the model.</p>

<p>We recognise that that is a fair few data points! To thank Snowplow users for their trouble in providing them (as well as building a model for you). we will <em>also</em> send each person that provides data a <strong>free Snowplow T-shirt</strong> in their size.</p>

<p>To get those data points, please work through the following steps:</p>
<h3><a name='events-per-day'>2.1 Calculating the number of events tracked per day</a></h3>
<p>Simply execute the following SQL statement in Redshift</p>
<div class='highlight'><pre><code class='sql'><span class='k'>SELECT</span>
<span class='n'>to_char</span><span class='p'>(</span><span class='n'>collector_tstamp</span><span class='p'>,</span> <span class='s1'>&#39;YYYY-MM-DD&#39;</span><span class='p'>)</span> <span class='k'>AS</span> <span class='ss'>&quot;Day&quot;</span><span class='p'>,</span>
<span class='k'>count</span><span class='p'>(</span><span class='o'>*</span><span class='p'>)</span> <span class='k'>AS</span> <span class='ss'>&quot;Number of events&quot;</span>
<span class='k'>FROM</span> <span class='n'>events</span>
<span class='k'>WHERE</span> <span class='n'>collector_tstamp</span> <span class='o'>&gt;</span> <span class='err'>{$</span><span class='k'>START</span><span class='o'>-</span><span class='nb'>DATE</span><span class='err'>}</span>
<span class='k'>AND</span> <span class='n'>collector_tstamp</span><span class='o'>&lt;</span> <span class='err'>{$</span><span class='k'>START</span><span class='o'>-</span><span class='nb'>DATE</span><span class='err'>}</span>
<span class='k'>GROUP</span> <span class='k'>BY</span> <span class='ss'>&quot;Day&quot;</span>
</code></pre></div><h3><a name='runs-per-day'>2.2 Calculating the number of times the enrichment process is run per day</a></h3>
<p>Most Snowplow users run the enrichment process once per day.</p>

<p>You can confirm how many times you run Snowplow by logging into the AWS S3 console and navigating to the bucket where you archive your Snowplow event files. (This is specified in the <a href='https://github.com/snowplow/snowplow/wiki/1-installing-the-storageloader#wiki-configuration'>StorageLoader config file</a>.) Within the bucket you&#8217;ll see a single folder generated for each enrichment &#8216;run&#8217;, labelled with the timestamp of the run. You&#8217;ll be able to tell directly how many times the enrichment process is run - in the below case - it is once per day:</p>

<p><img alt='aws-s3-screenshot' src='/static/img/blog/2013/07/number-of-runs-per-day.png' /></p>
<h3><a name='log-files-per-day'>2.3 Measuring The number of Cloudfront log files generated per day, adn the volume of data</a></h3>
<p>This is most easily done using an S3 front end, as the AWS S3 console is a bit limited. We use <a href='http://www.cloudberrylab.com/'>Cloudberry</a>. On Cloudberry, you can read the number of files generated per day, and their size, directly, by simply right clicking on the folder with the day&#8217;s worth of log file archives and selecting properties:</p>

<p><img alt='number-of-collector-logs-and-size' src='/static/img/blog/2013/07/number-of-collector-logs-and-size.JPG' /></p>

<p>In the above case we see there were 370 files generated on 2013-07-08, which occupied a total of 366.5KB.</p>
<h3><a name='emr-details'>2.4 The amount of time taken to enrich the data in EMR (and the size of cluster used to perform the enrichment)</a></h3>
<p>You can use the EMR command line tools to generate a JSON with details of each EMR job. In the below example, we pull a JSON for a specific job:</p>
<div class='highlight'><pre><code class='bash'><span class='nv'>$ </span>./elastic-mapreduce --describe --jobflow j-Y9QNJI44PA0X
<span class='o'>{</span>
  <span class='s2'>&quot;JobFlows&quot;</span>: <span class='o'>[</span>
    <span class='o'>{</span>
      <span class='s2'>&quot;Instances&quot;</span>: <span class='o'>{</span>
        <span class='s2'>&quot;TerminationProtected&quot;</span>: <span class='nb'>false</span>,
        <span class='s2'>&quot;MasterInstanceId&quot;</span>: <span class='s2'>&quot;i-944414d9&quot;</span>,
        <span class='s2'>&quot;HadoopVersion&quot;</span>: <span class='s2'>&quot;1.0.3&quot;</span>,
        <span class='s2'>&quot;NormalizedInstanceHours&quot;</span>: 2,
        <span class='s2'>&quot;MasterPublicDnsName&quot;</span>: <span class='s2'>&quot;ec2-54-228-105-10.eu-west-1.compute.amazonaws.com&quot;</span>,
        <span class='s2'>&quot;SlaveInstanceType&quot;</span>: <span class='s2'>&quot;m1.small&quot;</span>,
        <span class='s2'>&quot;MasterInstanceType&quot;</span>: <span class='s2'>&quot;m1.small&quot;</span>,
        <span class='s2'>&quot;InstanceGroups&quot;</span>: <span class='o'>[</span>
          <span class='o'>{</span>
            <span class='s2'>&quot;ReadyDateTime&quot;</span>: 1372215923.0,
            <span class='s2'>&quot;InstanceGroupId&quot;</span>: <span class='s2'>&quot;ig-2TGA68QGUOCUV&quot;</span>,
            <span class='s2'>&quot;State&quot;</span>: <span class='s2'>&quot;ENDED&quot;</span>,
            <span class='s2'>&quot;LastStateChangeReason&quot;</span>: <span class='s2'>&quot;Job flow terminated&quot;</span>,
            <span class='s2'>&quot;InstanceType&quot;</span>: <span class='s2'>&quot;m1.small&quot;</span>,
            <span class='s2'>&quot;InstanceRequestCount&quot;</span>: 1,
            <span class='s2'>&quot;InstanceRunningCount&quot;</span>: 0,
            <span class='s2'>&quot;StartDateTime&quot;</span>: 1372215848.0,
            <span class='s2'>&quot;Name&quot;</span>: null,
            <span class='s2'>&quot;BidPrice&quot;</span>: null,
            <span class='s2'>&quot;Market&quot;</span>: <span class='s2'>&quot;ON_DEMAND&quot;</span>,
            <span class='s2'>&quot;CreationDateTime&quot;</span>: 1372215689.0,
            <span class='s2'>&quot;InstanceRole&quot;</span>: <span class='s2'>&quot;MASTER&quot;</span>,
            <span class='s2'>&quot;EndDateTime&quot;</span>: 1372216249.0
          <span class='o'>}</span>,
          <span class='o'>{</span>
            <span class='s2'>&quot;ReadyDateTime&quot;</span>: 1372215929.0,
            <span class='s2'>&quot;InstanceGroupId&quot;</span>: <span class='s2'>&quot;ig-2M2UW6B8LFWOG&quot;</span>,
            <span class='s2'>&quot;State&quot;</span>: <span class='s2'>&quot;ENDED&quot;</span>,
            <span class='s2'>&quot;LastStateChangeReason&quot;</span>: <span class='s2'>&quot;Job flow terminated&quot;</span>,
            <span class='s2'>&quot;InstanceType&quot;</span>: <span class='s2'>&quot;m1.small&quot;</span>,
            <span class='s2'>&quot;InstanceRequestCount&quot;</span>: 1,
            <span class='s2'>&quot;InstanceRunningCount&quot;</span>: 0,
            <span class='s2'>&quot;StartDateTime&quot;</span>: 1372215929.0,
            <span class='s2'>&quot;Name&quot;</span>: null,
            <span class='s2'>&quot;BidPrice&quot;</span>: null,
            <span class='s2'>&quot;Market&quot;</span>: <span class='s2'>&quot;ON_DEMAND&quot;</span>,
            <span class='s2'>&quot;CreationDateTime&quot;</span>: 1372215689.0,
            <span class='s2'>&quot;InstanceRole&quot;</span>: <span class='s2'>&quot;CORE&quot;</span>,
            <span class='s2'>&quot;EndDateTime&quot;</span>: 1372216249.0
          <span class='o'>}</span>
        <span class='o'>]</span>,
        <span class='s2'>&quot;InstanceCount&quot;</span>: 2,
        <span class='s2'>&quot;KeepJobFlowAliveWhenNoSteps&quot;</span>: <span class='nb'>false</span>,
        <span class='s2'>&quot;Placement&quot;</span>: <span class='o'>{</span>
          <span class='s2'>&quot;AvailabilityZone&quot;</span>: <span class='s2'>&quot;eu-west-1a&quot;</span>
        <span class='o'>}</span>,
        <span class='s2'>&quot;Ec2SubnetId&quot;</span>: null,
        <span class='s2'>&quot;Ec2KeyName&quot;</span>: <span class='s2'>&quot;etl-nasqueron&quot;</span>
      <span class='o'>}</span>,
      <span class='s2'>&quot;JobFlowId&quot;</span>: <span class='s2'>&quot;j-Y9QNJI44PA0X&quot;</span>,
      <span class='s2'>&quot;BootstrapActions&quot;</span>: <span class='o'>[]</span>,
      <span class='s2'>&quot;JobFlowRole&quot;</span>: null,
      <span class='s2'>&quot;AmiVersion&quot;</span>: <span class='s2'>&quot;2.3.6&quot;</span>,
      <span class='s2'>&quot;LogUri&quot;</span>: <span class='s2'>&quot;s3n:\/\/snowplow-emr-logs\/pbz\/&quot;</span>,
      <span class='s2'>&quot;Steps&quot;</span>: <span class='o'>[</span>
        <span class='o'>{</span>
          <span class='s2'>&quot;ExecutionStatusDetail&quot;</span>: <span class='o'>{</span>
            <span class='s2'>&quot;State&quot;</span>: <span class='s2'>&quot;COMPLETED&quot;</span>,
            <span class='s2'>&quot;LastStateChangeReason&quot;</span>: null,
            <span class='s2'>&quot;StartDateTime&quot;</span>: 1372215928.0,
            <span class='s2'>&quot;CreationDateTime&quot;</span>: 1372215689.0,
            <span class='s2'>&quot;EndDateTime&quot;</span>: 1372216010.0
          <span class='o'>}</span>,
          <span class='s2'>&quot;StepConfig&quot;</span>: <span class='o'>{</span>
            <span class='s2'>&quot;HadoopJarStep&quot;</span>: <span class='o'>{</span>
              <span class='s2'>&quot;MainClass&quot;</span>: null,
              <span class='s2'>&quot;Args&quot;</span>: <span class='o'>[</span>
                <span class='s2'>&quot;--src&quot;</span>,
                <span class='s2'>&quot;s3n:\/\/snowplow-emr-processing\/pbz\/&quot;</span>,
                <span class='s2'>&quot;--dest&quot;</span>,
                <span class='s2'>&quot;hdfs:\/\/\/local\/snowplow-logs&quot;</span>,
                <span class='s2'>&quot;--groupBy&quot;</span>,
                <span class='s2'>&quot;.*\\.([0-9]+-[0-9]+-[0-9]+)-[0-9]+\\..*&quot;</span>,
                <span class='s2'>&quot;--targetSize&quot;</span>,
                <span class='s2'>&quot;128&quot;</span>,
                <span class='s2'>&quot;--outputCodec&quot;</span>,
                <span class='s2'>&quot;lzo&quot;</span>,
                <span class='s2'>&quot;--s3Endpoint&quot;</span>,
                <span class='s2'>&quot;s3-eu-west-1.amazonaws.com&quot;</span>
              <span class='o'>]</span>,
              <span class='s2'>&quot;Properties&quot;</span>: <span class='o'>[]</span>,
              <span class='s2'>&quot;Jar&quot;</span>: <span class='s2'>&quot;\/home\/hadoop\/lib\/emr-s3distcp-1.0.jar&quot;</span>
            <span class='o'>}</span>,
            <span class='s2'>&quot;Name&quot;</span>: <span class='s2'>&quot;Elasticity Custom Jar Step&quot;</span>,
            <span class='s2'>&quot;ActionOnFailure&quot;</span>: <span class='s2'>&quot;TERMINATE_JOB_FLOW&quot;</span>
          <span class='o'>}</span>
        <span class='o'>}</span>,
        <span class='o'>{</span>
          <span class='s2'>&quot;ExecutionStatusDetail&quot;</span>: <span class='o'>{</span>
            <span class='s2'>&quot;State&quot;</span>: <span class='s2'>&quot;COMPLETED&quot;</span>,
            <span class='s2'>&quot;LastStateChangeReason&quot;</span>: null,
            <span class='s2'>&quot;StartDateTime&quot;</span>: 1372216010.0,
            <span class='s2'>&quot;CreationDateTime&quot;</span>: 1372215689.0,
            <span class='s2'>&quot;EndDateTime&quot;</span>: 1372216196.0
          <span class='o'>}</span>,
          <span class='s2'>&quot;StepConfig&quot;</span>: <span class='o'>{</span>
            <span class='s2'>&quot;HadoopJarStep&quot;</span>: <span class='o'>{</span>
              <span class='s2'>&quot;MainClass&quot;</span>: null,
              <span class='s2'>&quot;Args&quot;</span>: <span class='o'>[</span>
                <span class='s2'>&quot;com.snowplowanalytics.snowplow.enrich.hadoop.EtlJob&quot;</span>,
                <span class='s2'>&quot;--hdfs&quot;</span>,
                <span class='s2'>&quot;--input_folder&quot;</span>,
                <span class='s2'>&quot;hdfs:\/\/\/local\/snowplow-logs&quot;</span>,
                <span class='s2'>&quot;--input_format&quot;</span>,
                <span class='s2'>&quot;cloudfront&quot;</span>,
                <span class='s2'>&quot;--maxmind_file&quot;</span>,
                <span class='s2'>&quot;http:\/\/snowplow-hosted-assets.s3.amazonaws.com\/third-party\/maxmind\/GeoLiteCity.dat&quot;</span>,
                <span class='s2'>&quot;--output_folder&quot;</span>,
                <span class='s2'>&quot;s3n:\/\/snowplow-events-pbz\/events\/2013-06-26-04-00-03\/&quot;</span>,
                <span class='s2'>&quot;--bad_rows_folder&quot;</span>,
                <span class='s2'>&quot;2013-06-26-04-00-03\/&quot;</span>
              <span class='o'>]</span>,
              <span class='s2'>&quot;Properties&quot;</span>: <span class='o'>[]</span>,
              <span class='s2'>&quot;Jar&quot;</span>: <span class='s2'>&quot;s3:\/\/snowplow-hosted-assets\/3-enrich\/hadoop-etl\/snowplow-hadoop-etl-0.3.2.jar&quot;</span>
            <span class='o'>}</span>,
            <span class='s2'>&quot;Name&quot;</span>: <span class='s2'>&quot;Elasticity Custom Jar Step&quot;</span>,
            <span class='s2'>&quot;ActionOnFailure&quot;</span>: <span class='s2'>&quot;TERMINATE_JOB_FLOW&quot;</span>
          <span class='o'>}</span>
        <span class='o'>}</span>
      <span class='o'>]</span>,
      <span class='s2'>&quot;Name&quot;</span>: <span class='s2'>&quot;Snowplow Enrichment for pbz&quot;</span>,
      <span class='s2'>&quot;ExecutionStatusDetail&quot;</span>: <span class='o'>{</span>
        <span class='s2'>&quot;ReadyDateTime&quot;</span>: 1372215929.0,
        <span class='s2'>&quot;State&quot;</span>: <span class='s2'>&quot;COMPLETED&quot;</span>,
        <span class='s2'>&quot;LastStateChangeReason&quot;</span>: <span class='s2'>&quot;Steps completed&quot;</span>,
        <span class='s2'>&quot;StartDateTime&quot;</span>: 1372215929.0,
        <span class='s2'>&quot;CreationDateTime&quot;</span>: 1372215689.0,
        <span class='s2'>&quot;EndDateTime&quot;</span>: 1372216249.0
      <span class='o'>}</span>,
      <span class='s2'>&quot;SupportedProducts&quot;</span>: <span class='o'>[]</span>,
      <span class='s2'>&quot;VisibleToAllUsers&quot;</span>: <span class='nb'>false</span>
    <span class='o'>}</span>
  <span class='o'>]</span>
<span class='o'>}</span>
╭─alex@nasqueron  ~/Apps/emr-cli  
╰─<span class='nv'>$ </span>
</code></pre></div>
<p>Rather than parse the JSON yourself, we&#8217;re very happy for community members to simply save the JSON and email it to us, with the other data points. We can then extract the relevant data points from the JSON directly. (We&#8217;ll use R and the RJSON package, and blog about how we do it.) You can either generate a JSON for a specific job (you will need to enter the job ID:</p>
<div class='highlight'><pre><code class='bash'><span class='nv'>$ </span>./elastic-mapreduce --describe --jobflow <span class='o'>{</span><span class='nv'>$jobflow</span>-id<span class='o'>}</span> &gt; emr-job-data.json
</code></pre></div>
<p>Or you can fetch the data for every job run in the last two days:</p>
<div class='highlight'><pre><code class='bash'><span class='nv'>$ </span>./elastic-mapreduce --describe &gt; emr-job-data.json
</code></pre></div>
<p>Or all the data for every job in the last fortnight:</p>
<div class='highlight'><pre><code class='bash'><span class='nv'>$ </span>./elastic-mapreduce --describe all &gt; emr-job-data.json
</code></pre></div><h3><a name='output-back-to-s3'>2.5 Measuring the number of files written back to S3, and their size</a></h3>
<p>We can use Cloudberry again. Simply identify a folder in the archive bucket specified in the <a href='https://github.com/snowplow/snowplow/wiki/1-installing-the-storageloader#wiki-configuration'>StorageLoader config</a>, right click on it and select properties:</p>

<p><img alt='number-of-snowplow-event-files-and-size' src='/static/img/blog/2013/07/number-of-snowplow-event-files-and-size.JPG' /></p>

<p>In the above example, 3 files were generated for a single run, with a total size of 981.4KB.</p>
<h3><a name='redshift-data-points'>2.6 The total number of lines of data in Redshift, and the amount of Redshift capacity used</a></h3>
<p>Measuring the amount of space occupied by your events in Redshift is very easy.</p>

<p>First, measure the number of events by executing the following query:</p>
<div class='highlight'><pre><code class='sql'><span class='k'>select</span> <span class='k'>count</span><span class='p'>(</span><span class='o'>*</span><span class='p'>)</span> <span class='k'>from</span> <span class='n'>events</span><span class='p'>;</span>
</code></pre></div>
<p>Then to find out how much disk space that occupies in your Redshift cluster execute the following query:</p>
<div class='highlight'><pre><code class='sql'><span class='k'>select</span> <span class='k'>owner</span> <span class='k'>as</span> <span class='n'>node</span><span class='p'>,</span> <span class='n'>diskno</span><span class='p'>,</span> <span class='n'>used</span><span class='p'>,</span> <span class='n'>capacity</span> 
<span class='k'>from</span> <span class='n'>stv_partitions</span> 
<span class='k'>order</span> <span class='k'>by</span> <span class='mi'>1</span><span class='p'>,</span> <span class='mi'>2</span><span class='p'>,</span> <span class='mi'>3</span><span class='p'>,</span> <span class='mi'>4</span><span class='p'>;</span>
</code></pre></div>
<p>The amount of used capacity (in MB) is given in the &#8220;used&#8221; column: it is 1941MB in the below example. The total capacity is given at 1906184 i.e. 1.8TB: that is because we are running a single (2TB) node.</p>

<p><img alt='redshift-example' src='/static/img/blog/2013/07/redshift-disk-space.JPG' /></p>

<p>For our purposes, we only need one of the lines of data to calculate the relationship between disk space on Redshift and number of events stored on Redshift, and use that to model Redshift costs.</p>

<h2 id='help_us_build_an_accurate_robust_model_that_we_all_can_use_to_forecast_snowplow_aws_costs'>Help us build an accurate, robust model, that we all can use to forecast Snowplow AWS costs</h2>

<p>We realise that you, our users, are busy people who have plenty to do aside from spending 20-30 minutes fetching data points related to your Snowplow installation, and sending them to us. We really hope, however, that many of you do, because:</p>

<ol>
<li>A Total Cost of Ownership Model will be really useful for all of us!</li>

<li>We&#8217;ll send you a Snowplow T-shirt, by way of thanks.</li>
</ol>

<p>So please help us help you, and keep plowing!</p>
			<div class="author_summary">
				<h2>About the author</h2>
				<div class="author_image"><img src="https://lh4.googleusercontent.com/--uMP0uMpzEs/AAAAAAAAAAI/AAAAAAAABH0/lo82KAkjEIU/s120-c/photo.jpg" /></div> <div class="author_spiel">
  <a href="/yali.html">Yali</a> is co-founder and analytics lead at Snowplow Analytics. You can find in him on <a href="https://plus.google.com/u/0/106510540736941709264" rel="author">Google+</a>, <a href="https://twitter.com/yalisassoon">Twitter</a> and <a href="http://uk.linkedin.com/in/yalisassoon">LinkedIn</a>.
</div>

			</div> 
			<div id="comments">
	<h2>Questions? Comments? Join the debate!</h2>
	 <div id="disqus_thread"></div>
        <script type="text/javascript">
            /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
            var disqus_shortname = 'snowplow'; // required: replace example with your forum shortname
            /* var disqus_identifier =  ; // unique ID so that disqus fetches the correct comments for each post
            var disqus_url =  ;
            var disqus_title =  ; */

            /* * * DON'T EDIT BELOW THIS LINE * * */
            (function() {
                var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
                dsq.src = 'http://' + disqus_shortname + '.disqus.com/embed.js';
                (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
            })();
        </script>
        <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
        <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</div>
		</div>
		<p>Return to the <a href="/blog.html">main blog page</a></p>
		

</div>

<div id="sidebar">
	<h1>Recent posts</h1>
	<ul>
		
			<li><a href="/blog/2013/07/09/dotnet-support-added-to-referer-parser">.NET (C#) support added to referer-parser</a></li>
		
			<li><a href="/blog/2013/07/08/help-us-build-out-the-snowplow-total-cost-of-ownership-model">Help us build out the Snowplow Total Cost of Ownership Model</a></li>
		
			<li><a href="/blog/2013/07/07/snowplow-0.8.7-released">Snowplow 0.8.7 released with JavaScript Tracker improvements</a></li>
		
			<li><a href="/blog/2013/07/03/snowplow-tracker-for-lua-event-analytics-released">Snowplow Tracker for Lua event analytics released</a></li>
		
			<li><a href="/blog/2013/07/02/reduce-your-cloudfront-bills-with-cache-control">Reduce your Cloudfront costs with cache control</a></li>
		
	</ul>

	
		<h1>Other</h1>
		<ul>
		
			
				<li><a href="/blog/2013/07/08/help-us-build-out-the-snowplow-total-cost-of-ownership-model">Help us build out the Snowplow Total Cost of Ownership Model</a></li>
			
				<li><a href="/blog/2013/07/02/reduce-your-cloudfront-bills-with-cache-control">Reduce your Cloudfront costs with cache control</a></li>
			
				<li><a href="/blog/2013/06/28/is-web-analytics-easy-or-hard-distinguishing-different-types-of-complexity">Is web analytics easy or hard? Distinguishing different types of complexity, and approaches for dealing with them</a></li>
			
				<li><a href="/blog/2013/06/05/tracking-olark-chat-events-with-snowplow">Tracking Olark chat events with Snowplow</a></li>
			
				<li><a href="/blog/2013/02/20/transferring-data-from-s3-to-redshift-at-the-command-line">Bulk loading data from Amazon S3 into Redshift at the command line</a></li>
			
		
		</ul>		
	
		<h1>Releases</h1>
		<ul>
		
			
				<li><a href="/blog/2013/07/09/dotnet-support-added-to-referer-parser">.NET (C#) support added to referer-parser</a></li>
			
				<li><a href="/blog/2013/07/07/snowplow-0.8.7-released">Snowplow 0.8.7 released with JavaScript Tracker improvements</a></li>
			
				<li><a href="/blog/2013/07/03/snowplow-tracker-for-lua-event-analytics-released">Snowplow Tracker for Lua event analytics released</a></li>
			
				<li><a href="/blog/2013/06/03/snowplow-0.8.6-released-with-performance-improvements">Snowplow 0.8.6 released with performance improvements</a></li>
			
				<li><a href="/blog/2013/05/24/snowplow-0.8.5-released-with-etl-bug-fixes">Snowplow 0.8.5 released with ETL bug fixes</a></li>
			
		
		</ul>		
	
		<h1>Analytics</h1>
		<ul>
		
			
				<li><a href="/blog/2013/06/26/getting-started-with-r-for-data-analysis-and-visualization">Getting started using R for data analysis</a></li>
			
				<li><a href="/blog/2013/05/22/measuring-how-much-individual-items-in-your-catalog-contribute-to-inbound-marketing">Measuring how much traffic individual items in your catalog drive to your website</a></li>
			
				<li><a href="/blog/2013/05/20/performing-market-basket-analysis-with-r-arules-and-snowplow">Performing market basket analysis on web analytics data with R</a></li>
			
				<li><a href="/blog/2013/05/10/where-does-your-traffic-really-come-from">Where does your traffic *really* come from?</a></li>
			
				<li><a href="/blog/2013/04/23/performing-funnel-analysis-with-snowplow">Funnel analysis with Snowplow (Platform analytics part 1)</a></li>
			
		
		</ul>		
	
		<h1>Inside the Plow</h1>
		<ul>
		
			
				<li><a href="/blog/2013/05/30/dealing-with-hadoops-small-files-problem">Dealing with Hadoop's small files problem</a></li>
			
				<li><a href="/blog/2013/04/10/snowplow-event-validation">Towards high-fidelity web analytics - introducing Snowplow's innovative new event validation capabilities</a></li>
			
				<li><a href="/blog/2013/03/20/rob-slifka-elasticity">Inside the Plow - Rob Slifka's Elasticity</a></li>
			
				<li><a href="/blog/2013/02/08/writing-hive-udfs-and-serdes">Writing Hive UDFs - a tutorial</a></li>
			
				<li><a href="/blog/2013/02/04/help-us-build-out-the-snowplow-event-model">Help us build out the Snowplow Event Model</a></li>
			
		
		</ul>		
	

	<h1>Useful links</h1>
	<ul>
		<li><a href="/blog/atom.xml">Atom feed</a></li>
	</ul>
	<!--<strong>Tags</strong> -->
</div>

		<div id="footer">
	<p>Copyright © Snowplow Analytics Limited 2012 - 2013.  All rights reserved</p>
</div>
	</div>
		<!-- Following Javascript function used by Disqus to count the number of comments for each blog post and display in the main index -->
	  	<script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'snowplow'; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function () {
            var s = document.createElement('script'); s.async = true;
            s.type = 'text/javascript';
            s.src = 'http://' + disqus_shortname + '.disqus.com/count.js';
            (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
        }());
        </script>
        <!-- begin olark code -->
		<script data-cfasync="false" type='text/javascript'>/*<![CDATA[*/window.olark||(function(c){var f=window,d=document,l=f.location.protocol=="https:"?"https:":"http:",z=c.name,r="load";var nt=function(){
		f[z]=function(){
		(a.s=a.s||[]).push(arguments)};var a=f[z]._={
		},q=c.methods.length;while(q--){(function(n){f[z][n]=function(){
		f[z]("call",n,arguments)}})(c.methods[q])}a.l=c.loader;a.i=nt;a.p={
		0:+new Date};a.P=function(u){
		a.p[u]=new Date-a.p[0]};function s(){
		a.P(r);f[z](r)}f.addEventListener?f.addEventListener(r,s,false):f.attachEvent("on"+r,s);var ld=function(){function p(hd){
		hd="head";return["<",hd,"></",hd,"><",i,' onl' + 'oad="var d=',g,";d.getElementsByTagName('head')[0].",j,"(d.",h,"('script')).",k,"='",l,"//",a.l,"'",'"',"></",i,">"].join("")}var i="body",m=d[i];if(!m){
		return setTimeout(ld,100)}a.P(1);var j="appendChild",h="createElement",k="src",n=d[h]("div"),v=n[j](d[h](z)),b=d[h]("iframe"),g="document",e="domain",o;n.style.display="none";m.insertBefore(n,m.firstChild).id=z;b.frameBorder="0";b.id=z+"-loader";if(/MSIE[ ]+6/.test(navigator.userAgent)){
		b.src="javascript:false"}b.allowTransparency="true";v[j](b);try{
		b.contentWindow[g].open()}catch(w){
		c[e]=d[e];o="javascript:var d="+g+".open();d.domain='"+d.domain+"';";b[k]=o+"void(0);"}try{
		var t=b.contentWindow[g];t.write(p());t.close()}catch(x){
		b[k]=o+'d.write("'+p().replace(/"/g,String.fromCharCode(92)+'"')+'");d.close();'}a.P(2)};ld()};nt()})({
		loader: "static.olark.com/jsclient/loader0.js",name:"olark",methods:["configure","extend","declare","identify"]});
		/* custom configuration goes here (www.olark.com/documentation) */
		olark.identify('9752-503-10-5227');/*]]>*/</script><noscript><a href="https://www.olark.com/site/9752-503-10-5227/contact" title="Contact us" target="_blank">Questions? Feedback?</a> powered by <a href="http://www.olark.com?welcome" title="Olark live chat software">Olark live chat software</a></noscript>
		<!-- end olark code -->
		<!-- Track Olark chats in GTM (so can pass data onto Snowplow) -->
		<script type="text/javascript">
		olark('api.chat.onMessageToOperator', function(event) {
		    dataLayer.push({'event': 'olarkMessageToOperator'});
		});
		olark('api.chat.onMessageToVisitor', function(event) {
		    dataLayer.push({'event': 'olarkMessageToVisitor'});
		});
		</script>
		<!-- end track olark code -->


</body>
</html>