<!DOCTYPE html>
<html>
<head>
	
	<title>Help us build out the Snowplow Total Cost of Ownership Model - Snowplow Analytics</title>
	

	<link rel="icon" type="image/x-icon" href="/favicon.ico" />

	<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
	<meta name="description" content="" />
	<link href="/static/css/styles.css" type="text/css" rel="stylesheet" />
	<link href="/static/css/pygments.css" type="text/css" rel="stylesheet" />
	
	<!--For the homepage slider-->
	<link rel="stylesheet" href="/static/css/nivo-slider.css" type="text/css" media="screen" />
	<link rel="stylesheet" href="/static/css/nivo-slider-theme-default.css" type="text/css" media="screen" />
	<script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.0/jquery.min.js"></script>
	<script src="/static/js/jquery-nivo-slider-pack.js" type="text/javascript" ></script>
	<!--MathJax http://www.mathjax.org/-->
	<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_HTMLorMML.js"></script>
	<script type="text/javascript">
		MathJax.Hub.Config({
	      tex2jax: {
	        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
	      }
	    });
	    MathJax.Hub.Queue(function() {
	        var all = MathJax.Hub.getAllJax(), i;
	        for(i=0; i < all.length; i += 1) {
	            all[i].SourceElement().parentNode.className += ' has-jax';
	        }
    	});
	</script>
	<!-- end mathjax -->
	<!-- typekit -->
	<script type="text/javascript" src="//use.typekit.net/noo1diw.js"></script>
	<script type="text/javascript">try{Typekit.load();}catch(e){}</script>
	<!-- end typekit -->
</head>
<body>
	<!-- Google Tag Manager -->
	<noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-DLRG"
	height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
	<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
	new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
	j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
	'//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
	})(window,document,'script','dataLayer','GTM-DLRG');</script>
	<!-- End Google Tag Manager -->
	<div id="universe">
		<div id="container">
			<div id="header" class="span-24">
  <div id="logo">
    <h1><a href="/"><img src="/static/img/snowplow-logo-website.png" title="Snowplow Analytics" /></a></h1>
  </div>
  <div id="menu" class="span-15">
    <ul>
      <li ><a href="/product/index.html">Product</a></li>
      <li ><a href="/services/index.html">Services</a></li>
      <li ><a href="/analytics/index.html">Analytics</a></li>
      <li ><a href="/technology/index.html">Technology</a></li>
      <li  class="active" ><a href="/blog.html">Blog</a></li>
      <li ><a id="mail" href="/about/index.html">About</a></li>
    </ul>
  </div>
</div>
	
			<div id="contents">
		<div class="post">
			10 Jul 2013
			<h1>Help us build out the Snowplow Total Cost of Ownership Model</h1>
			 <span class="author">Author: <a href="/yali.html" rel="author">Yali Sassoon </a></span>
			<p>In a <a href='/blog/2013/07/09/understanding-how-different-parts-of-the-Snowplow-data-pipeline-drive-AWS-costs/'>previous blog post</a>, we described how we were in the process of building a <a href='/blog/2013/07/09/understanding-how-different-parts-of-the-Snowplow-data-pipeline-drive-AWS-costs/'>Total Cost of Ownership model</a> for Snowplow: something that would enable a Snowplow user, or prospective user, to accurately forecast their AWS bill going forwards based on their traffic levels.</p>

<p><img alt='your-country-needs-you' src='/static/img/blog/2013/07/your-country-needs-you.jpg' /></p>

<p>To build that model, though, we need <strong>your help</strong>. In order to ensure that our model is accurate and robust, we need to make sure that the relationships we believe exist between the number of events tracked, and the number and size of files generated, as detailed in the <a href='/blog/2013/07/09/understanding-how-different-parts-of-the-Snowplow-data-pipeline-drive-AWS-costs/'>last post</a>, are correct, and that we have modelled them accurately. To that end, we are asking Snowplow users to help us by providing the following data:</p>

<ol>
<li><a href='/blog/2013/07/10/help-us-build-out-the-snowplow-total-cost-of-ownership-model/#events-per-day'>The number of events tracked per day</a></li>

<li><a href='/blog/2013/07/10/help-us-build-out-the-snowplow-total-cost-of-ownership-model/#runs-per-day'>The number of times the enrichment process is run per day</a></li>

<li><a href='/blog/2013/07/10/help-us-build-out-the-snowplow-total-cost-of-ownership-model/#log-files-per-day'>The number of Cloudfront log files generated per day, and the volume of data</a></li>

<li><a href='/blog/2013/07/10/help-us-build-out-the-snowplow-total-cost-of-ownership-model/#emr-details'>The amount of time taken to enrich the data in EMR (and the size of cluster used to perform the enrichment)</a></li>

<li><a href='/blog/2013/07/10/help-us-build-out-the-snowplow-total-cost-of-ownership-model/#output-back-to-s3'>The number of files outputted back to S3, and the size of those files</a></li>

<li><a href='/blog/2013/07/10/help-us-build-out-the-snowplow-total-cost-of-ownership-model/#redshift-data-points'>The total number of lines of data in Redshift, and the amount of Redshift capacity used</a></li>
</ol>

<p>We will then share this data back, in an anonymized form, with the community, as part of the model.</p>

<p>We recognise that that is a fair few data points! To thank Snowplow users for their trouble in providing them (as well as building a model for you), we will <em>also</em> send each person that provides data a <strong>free Snowplow T-shirt</strong> in their size.</p>

<p>In the rest of this post, we provide simple instructions for pulling the relevant data from Amazon.</p>
<!--more--><h3><a name='events-per-day'>1. Calculating the number of events tracked per day</a></h3>
<p>Simply execute the following SQL statement in Redshift</p>
<div class='highlight'><pre><code class='sql'><span class='k'>SELECT</span>
<span class='n'>to_char</span><span class='p'>(</span><span class='n'>collector_tstamp</span><span class='p'>,</span> <span class='s1'>&#39;YYYY-MM-DD&#39;</span><span class='p'>)</span> <span class='k'>AS</span> <span class='ss'>&quot;Day&quot;</span><span class='p'>,</span>
<span class='k'>count</span><span class='p'>(</span><span class='o'>*</span><span class='p'>)</span> <span class='k'>AS</span> <span class='ss'>&quot;Number of events&quot;</span>
<span class='k'>FROM</span> <span class='n'>events</span>
<span class='k'>WHERE</span> <span class='n'>collector_tstamp</span> <span class='o'>&gt;</span> <span class='err'>{$</span><span class='k'>START</span><span class='o'>-</span><span class='nb'>DATE</span><span class='err'>}</span>
<span class='k'>AND</span> <span class='n'>collector_tstamp</span><span class='o'>&lt;</span> <span class='err'>{$</span><span class='k'>START</span><span class='o'>-</span><span class='nb'>DATE</span><span class='err'>}</span>
<span class='k'>GROUP</span> <span class='k'>BY</span> <span class='ss'>&quot;Day&quot;</span>
</code></pre></div><h3><a name='runs-per-day'>2. Calculating the number of times the enrichment process is run per day</a></h3>
<p>Most Snowplow users run the enrichment process once per day.</p>

<p>You can confirm how many times you run Snowplow by logging into the AWS S3 console and navigating to the bucket where you archive your Snowplow event files. (This is specified in the <a href='https://github.com/snowplow/snowplow/wiki/1-installing-the-storageloader#wiki-configuration'>StorageLoader config file</a>.) Within the bucket you&#8217;ll see a single folder generated for each enrichment &#8216;run&#8217;, labelled with the timestamp of the run. You&#8217;ll be able to tell directly how many times the enrichment process is run - in the below case - it is once per day:</p>

<p><img alt='aws-s3-screenshot' src='/static/img/blog/2013/07/number-of-runs-per-day.png' /></p>
<h3><a name='log-files-per-day'>3. Measuring The number of Cloudfront log files generated per day, adn the volume of data</a></h3>
<p>This is most easily done using an S3 front end, as the AWS S3 console is a bit limited. We use <a href='http://www.cloudberrylab.com/'>Cloudberry</a>. On Cloudberry, you can read the number of files generated per day, and their size, directly, by simply right clicking on the folder with the day&#8217;s worth of log file archives and selecting properties:</p>

<p><img alt='number-of-collector-logs-and-size' src='/static/img/blog/2013/07/number-of-collector-logs-and-size.JPG' /></p>

<p>In the above case we see there were 370 files generated on 2013-07-08, which occupied a total of 366.5KB.</p>
<h3><a name='emr-details'>4. The amount of time taken to enrich the data in EMR (and the size of cluster used to perform the enrichment)</a></h3>
<p>You can use the EMR command line tools to generate a JSON with details of each EMR job. In the below example, we pull a JSON for a specific job:</p>
<div class='highlight'><pre><code class='bash'><span class='nv'>$ </span>./elastic-mapreduce --describe --jobflow j-Y9QNJI44PA0X
<span class='o'>{</span>
  <span class='s2'>&quot;JobFlows&quot;</span>: <span class='o'>[</span>
    <span class='o'>{</span>
      <span class='s2'>&quot;Instances&quot;</span>: <span class='o'>{</span>
        <span class='s2'>&quot;TerminationProtected&quot;</span>: <span class='nb'>false</span>,
        <span class='s2'>&quot;MasterInstanceId&quot;</span>: <span class='s2'>&quot;i-944414d9&quot;</span>,
        <span class='s2'>&quot;HadoopVersion&quot;</span>: <span class='s2'>&quot;1.0.3&quot;</span>,
        <span class='s2'>&quot;NormalizedInstanceHours&quot;</span>: 2,
        <span class='s2'>&quot;MasterPublicDnsName&quot;</span>: <span class='s2'>&quot;ec2-54-228-105-10.eu-west-1.compute.amazonaws.com&quot;</span>,
        <span class='s2'>&quot;SlaveInstanceType&quot;</span>: <span class='s2'>&quot;m1.small&quot;</span>,
        <span class='s2'>&quot;MasterInstanceType&quot;</span>: <span class='s2'>&quot;m1.small&quot;</span>,
        <span class='s2'>&quot;InstanceGroups&quot;</span>: <span class='o'>[</span>
          <span class='o'>{</span>
            <span class='s2'>&quot;ReadyDateTime&quot;</span>: 1372215923.0,
            <span class='s2'>&quot;InstanceGroupId&quot;</span>: <span class='s2'>&quot;ig-2TGA68QGUOCUV&quot;</span>,
            <span class='s2'>&quot;State&quot;</span>: <span class='s2'>&quot;ENDED&quot;</span>,
            <span class='s2'>&quot;LastStateChangeReason&quot;</span>: <span class='s2'>&quot;Job flow terminated&quot;</span>,
            <span class='s2'>&quot;InstanceType&quot;</span>: <span class='s2'>&quot;m1.small&quot;</span>,
            <span class='s2'>&quot;InstanceRequestCount&quot;</span>: 1,
            <span class='s2'>&quot;InstanceRunningCount&quot;</span>: 0,
            <span class='s2'>&quot;StartDateTime&quot;</span>: 1372215848.0,
            <span class='s2'>&quot;Name&quot;</span>: null,
            <span class='s2'>&quot;BidPrice&quot;</span>: null,
            <span class='s2'>&quot;Market&quot;</span>: <span class='s2'>&quot;ON_DEMAND&quot;</span>,
            <span class='s2'>&quot;CreationDateTime&quot;</span>: 1372215689.0,
            <span class='s2'>&quot;InstanceRole&quot;</span>: <span class='s2'>&quot;MASTER&quot;</span>,
            <span class='s2'>&quot;EndDateTime&quot;</span>: 1372216249.0
          <span class='o'>}</span>,
          <span class='o'>{</span>
            <span class='s2'>&quot;ReadyDateTime&quot;</span>: 1372215929.0,
            <span class='s2'>&quot;InstanceGroupId&quot;</span>: <span class='s2'>&quot;ig-2M2UW6B8LFWOG&quot;</span>,
            <span class='s2'>&quot;State&quot;</span>: <span class='s2'>&quot;ENDED&quot;</span>,
            <span class='s2'>&quot;LastStateChangeReason&quot;</span>: <span class='s2'>&quot;Job flow terminated&quot;</span>,
            <span class='s2'>&quot;InstanceType&quot;</span>: <span class='s2'>&quot;m1.small&quot;</span>,
            <span class='s2'>&quot;InstanceRequestCount&quot;</span>: 1,
            <span class='s2'>&quot;InstanceRunningCount&quot;</span>: 0,
            <span class='s2'>&quot;StartDateTime&quot;</span>: 1372215929.0,
            <span class='s2'>&quot;Name&quot;</span>: null,
            <span class='s2'>&quot;BidPrice&quot;</span>: null,
            <span class='s2'>&quot;Market&quot;</span>: <span class='s2'>&quot;ON_DEMAND&quot;</span>,
            <span class='s2'>&quot;CreationDateTime&quot;</span>: 1372215689.0,
            <span class='s2'>&quot;InstanceRole&quot;</span>: <span class='s2'>&quot;CORE&quot;</span>,
            <span class='s2'>&quot;EndDateTime&quot;</span>: 1372216249.0
          <span class='o'>}</span>
        <span class='o'>]</span>,
        <span class='s2'>&quot;InstanceCount&quot;</span>: 2,
        <span class='s2'>&quot;KeepJobFlowAliveWhenNoSteps&quot;</span>: <span class='nb'>false</span>,
        <span class='s2'>&quot;Placement&quot;</span>: <span class='o'>{</span>
          <span class='s2'>&quot;AvailabilityZone&quot;</span>: <span class='s2'>&quot;eu-west-1a&quot;</span>
        <span class='o'>}</span>,
        <span class='s2'>&quot;Ec2SubnetId&quot;</span>: null,
        <span class='s2'>&quot;Ec2KeyName&quot;</span>: <span class='s2'>&quot;etl-nasqueron&quot;</span>
      <span class='o'>}</span>,
      <span class='s2'>&quot;JobFlowId&quot;</span>: <span class='s2'>&quot;j-Y9QNJI44PA0X&quot;</span>,
      <span class='s2'>&quot;BootstrapActions&quot;</span>: <span class='o'>[]</span>,
      <span class='s2'>&quot;JobFlowRole&quot;</span>: null,
      <span class='s2'>&quot;AmiVersion&quot;</span>: <span class='s2'>&quot;2.3.6&quot;</span>,
      <span class='s2'>&quot;LogUri&quot;</span>: <span class='s2'>&quot;s3n:\/\/snowplow-emr-logs\/pbz\/&quot;</span>,
      <span class='s2'>&quot;Steps&quot;</span>: <span class='o'>[</span>
        <span class='o'>{</span>
          <span class='s2'>&quot;ExecutionStatusDetail&quot;</span>: <span class='o'>{</span>
            <span class='s2'>&quot;State&quot;</span>: <span class='s2'>&quot;COMPLETED&quot;</span>,
            <span class='s2'>&quot;LastStateChangeReason&quot;</span>: null,
            <span class='s2'>&quot;StartDateTime&quot;</span>: 1372215928.0,
            <span class='s2'>&quot;CreationDateTime&quot;</span>: 1372215689.0,
            <span class='s2'>&quot;EndDateTime&quot;</span>: 1372216010.0
          <span class='o'>}</span>,
          <span class='s2'>&quot;StepConfig&quot;</span>: <span class='o'>{</span>
            <span class='s2'>&quot;HadoopJarStep&quot;</span>: <span class='o'>{</span>
              <span class='s2'>&quot;MainClass&quot;</span>: null,
              <span class='s2'>&quot;Args&quot;</span>: <span class='o'>[</span>
                <span class='s2'>&quot;--src&quot;</span>,
                <span class='s2'>&quot;s3n:\/\/snowplow-emr-processing\/pbz\/&quot;</span>,
                <span class='s2'>&quot;--dest&quot;</span>,
                <span class='s2'>&quot;hdfs:\/\/\/local\/snowplow-logs&quot;</span>,
                <span class='s2'>&quot;--groupBy&quot;</span>,
                <span class='s2'>&quot;.*\\.([0-9]+-[0-9]+-[0-9]+)-[0-9]+\\..*&quot;</span>,
                <span class='s2'>&quot;--targetSize&quot;</span>,
                <span class='s2'>&quot;128&quot;</span>,
                <span class='s2'>&quot;--outputCodec&quot;</span>,
                <span class='s2'>&quot;lzo&quot;</span>,
                <span class='s2'>&quot;--s3Endpoint&quot;</span>,
                <span class='s2'>&quot;s3-eu-west-1.amazonaws.com&quot;</span>
              <span class='o'>]</span>,
              <span class='s2'>&quot;Properties&quot;</span>: <span class='o'>[]</span>,
              <span class='s2'>&quot;Jar&quot;</span>: <span class='s2'>&quot;\/home\/hadoop\/lib\/emr-s3distcp-1.0.jar&quot;</span>
            <span class='o'>}</span>,
            <span class='s2'>&quot;Name&quot;</span>: <span class='s2'>&quot;Elasticity Custom Jar Step&quot;</span>,
            <span class='s2'>&quot;ActionOnFailure&quot;</span>: <span class='s2'>&quot;TERMINATE_JOB_FLOW&quot;</span>
          <span class='o'>}</span>
        <span class='o'>}</span>,
        <span class='o'>{</span>
          <span class='s2'>&quot;ExecutionStatusDetail&quot;</span>: <span class='o'>{</span>
            <span class='s2'>&quot;State&quot;</span>: <span class='s2'>&quot;COMPLETED&quot;</span>,
            <span class='s2'>&quot;LastStateChangeReason&quot;</span>: null,
            <span class='s2'>&quot;StartDateTime&quot;</span>: 1372216010.0,
            <span class='s2'>&quot;CreationDateTime&quot;</span>: 1372215689.0,
            <span class='s2'>&quot;EndDateTime&quot;</span>: 1372216196.0
          <span class='o'>}</span>,
          <span class='s2'>&quot;StepConfig&quot;</span>: <span class='o'>{</span>
            <span class='s2'>&quot;HadoopJarStep&quot;</span>: <span class='o'>{</span>
              <span class='s2'>&quot;MainClass&quot;</span>: null,
              <span class='s2'>&quot;Args&quot;</span>: <span class='o'>[</span>
                <span class='s2'>&quot;com.snowplowanalytics.snowplow.enrich.hadoop.EtlJob&quot;</span>,
                <span class='s2'>&quot;--hdfs&quot;</span>,
                <span class='s2'>&quot;--input_folder&quot;</span>,
                <span class='s2'>&quot;hdfs:\/\/\/local\/snowplow-logs&quot;</span>,
                <span class='s2'>&quot;--input_format&quot;</span>,
                <span class='s2'>&quot;cloudfront&quot;</span>,
                <span class='s2'>&quot;--maxmind_file&quot;</span>,
                <span class='s2'>&quot;http:\/\/snowplow-hosted-assets.s3.amazonaws.com\/third-party\/maxmind\/GeoLiteCity.dat&quot;</span>,
                <span class='s2'>&quot;--output_folder&quot;</span>,
                <span class='s2'>&quot;s3n:\/\/snowplow-events-pbz\/events\/2013-06-26-04-00-03\/&quot;</span>,
                <span class='s2'>&quot;--bad_rows_folder&quot;</span>,
                <span class='s2'>&quot;2013-06-26-04-00-03\/&quot;</span>
              <span class='o'>]</span>,
              <span class='s2'>&quot;Properties&quot;</span>: <span class='o'>[]</span>,
              <span class='s2'>&quot;Jar&quot;</span>: <span class='s2'>&quot;s3:\/\/snowplow-hosted-assets\/3-enrich\/hadoop-etl\/snowplow-hadoop-etl-0.3.2.jar&quot;</span>
            <span class='o'>}</span>,
            <span class='s2'>&quot;Name&quot;</span>: <span class='s2'>&quot;Elasticity Custom Jar Step&quot;</span>,
            <span class='s2'>&quot;ActionOnFailure&quot;</span>: <span class='s2'>&quot;TERMINATE_JOB_FLOW&quot;</span>
          <span class='o'>}</span>
        <span class='o'>}</span>
      <span class='o'>]</span>,
      <span class='s2'>&quot;Name&quot;</span>: <span class='s2'>&quot;Snowplow Enrichment for pbz&quot;</span>,
      <span class='s2'>&quot;ExecutionStatusDetail&quot;</span>: <span class='o'>{</span>
        <span class='s2'>&quot;ReadyDateTime&quot;</span>: 1372215929.0,
        <span class='s2'>&quot;State&quot;</span>: <span class='s2'>&quot;COMPLETED&quot;</span>,
        <span class='s2'>&quot;LastStateChangeReason&quot;</span>: <span class='s2'>&quot;Steps completed&quot;</span>,
        <span class='s2'>&quot;StartDateTime&quot;</span>: 1372215929.0,
        <span class='s2'>&quot;CreationDateTime&quot;</span>: 1372215689.0,
        <span class='s2'>&quot;EndDateTime&quot;</span>: 1372216249.0
      <span class='o'>}</span>,
      <span class='s2'>&quot;SupportedProducts&quot;</span>: <span class='o'>[]</span>,
      <span class='s2'>&quot;VisibleToAllUsers&quot;</span>: <span class='nb'>false</span>
    <span class='o'>}</span>
  <span class='o'>]</span>
<span class='o'>}</span>
╭─alex@nasqueron  ~/Apps/emr-cli  
╰─<span class='nv'>$ </span>
</code></pre></div>
<p>Rather than parse the JSON yourself, we&#8217;re very happy for community members to simply save the JSON and email it to us, with the other data points. We can then extract the relevant data points from the JSON directly. (We&#8217;ll use R and the RJSON package, and blog about how we do it.) You can either generate a JSON for a specific job (you will need to enter the job ID:</p>
<div class='highlight'><pre><code class='bash'><span class='nv'>$ </span>./elastic-mapreduce --describe --jobflow <span class='o'>{</span><span class='nv'>$jobflow</span>-id<span class='o'>}</span> &gt; emr-job-data.json
</code></pre></div>
<p>Or you can fetch the data for every job run in the last two days:</p>
<div class='highlight'><pre><code class='bash'><span class='nv'>$ </span>./elastic-mapreduce --describe &gt; emr-job-data.json
</code></pre></div>
<p>Or all the data for every job in the last fortnight:</p>
<div class='highlight'><pre><code class='bash'><span class='nv'>$ </span>./elastic-mapreduce --describe all &gt; emr-job-data.json
</code></pre></div><h3><a name='output-back-to-s3'>5. Measuring the number of files written back to S3, and their size</a></h3>
<p>We can use Cloudberry again. Simply identify a folder in the archive bucket specified in the <a href='https://github.com/snowplow/snowplow/wiki/1-installing-the-storageloader#wiki-configuration'>StorageLoader config</a>, right click on it and select properties:</p>

<p><img alt='number-of-snowplow-event-files-and-size' src='/static/img/blog/2013/07/number-of-snowplow-event-files-and-size.JPG' /></p>

<p>In the above example, 3 files were generated for a single run, with a total size of 981.4KB.</p>
<h3><a name='redshift-data-points'>6. The total number of lines of data in Redshift, and the amount of Redshift capacity used</a></h3>
<p>Measuring the amount of space occupied by your events in Redshift is very easy.</p>

<p>First, measure the number of events by executing the following query:</p>
<div class='highlight'><pre><code class='sql'><span class='k'>select</span> <span class='k'>count</span><span class='p'>(</span><span class='o'>*</span><span class='p'>)</span> <span class='k'>from</span> <span class='n'>events</span><span class='p'>;</span>
</code></pre></div>
<p>Then to find out how much disk space that occupies in your Redshift cluster execute the following query:</p>
<div class='highlight'><pre><code class='sql'><span class='k'>select</span> <span class='k'>owner</span> <span class='k'>as</span> <span class='n'>node</span><span class='p'>,</span> <span class='n'>diskno</span><span class='p'>,</span> <span class='n'>used</span><span class='p'>,</span> <span class='n'>capacity</span> 
<span class='k'>from</span> <span class='n'>stv_partitions</span> 
<span class='k'>order</span> <span class='k'>by</span> <span class='mi'>1</span><span class='p'>,</span> <span class='mi'>2</span><span class='p'>,</span> <span class='mi'>3</span><span class='p'>,</span> <span class='mi'>4</span><span class='p'>;</span>
</code></pre></div>
<p>The amount of used capacity (in MB) is given in the &#8220;used&#8221; column: it is 1,941MB in the below example. The total capacity is given at 1906184 i.e. 1.8TB: that is because we are running a single (2TB) node.</p>

<p><img alt='redshift-example' src='/static/img/blog/2013/07/redshift-disk-space.JPG' /></p>

<p>For our purposes, we only need one of the lines of data to calculate the relationship between disk space on Redshift and number of events stored on Redshift, and use that to model Redshift costs.</p>

<h2 id='help_us_build_an_accurate_robust_model_that_we_all_can_use_to_forecast_snowplow_aws_costs'>Help us build an accurate, robust model, that we all can use to forecast Snowplow AWS costs</h2>

<p>We realize that you, our users, are busy people who have plenty to do aside from spending 20-30 minutes fetching data points related to your Snowplow installation, and sending them to us. We really hope, however, that many of you do, because:</p>

<ol>
<li>A Total Cost of Ownership Model will be really useful for all of us!</li>

<li>We&#8217;ll send you a Snowplow T-shirt, by way of thanks</li>
</ol>

<p>If you can pop the above data points (in whatever format is most convenient), and email them to me on <code>yali at snowplowanalytics dot com</code>, along with your T-shirt size, we will send you through your T-shirts as soon as they are printed.</p>

<p>So please help us help you, and keep plowing!</p>
			<div class="author_summary">
				<h2>About the author</h2>
				<div class="author_image"><img src="https://lh4.googleusercontent.com/--uMP0uMpzEs/AAAAAAAAAAI/AAAAAAAABH0/lo82KAkjEIU/s120-c/photo.jpg" /></div> <div class="author_spiel">
  <a href="/yali.html">Yali</a> is co-founder and analytics lead at Snowplow Analytics. You can find in him on <a href="https://plus.google.com/u/0/106510540736941709264" rel="author">Google+</a>, <a href="https://twitter.com/yalisassoon">Twitter</a> and <a href="http://uk.linkedin.com/in/yalisassoon">LinkedIn</a>.
</div>

			</div> 
			<div id="comments">
	<h2>Questions? Comments? Join the debate!</h2>
	 <div id="disqus_thread"></div>
        <script type="text/javascript">
            /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
            var disqus_shortname = 'snowplow'; // required: replace example with your forum shortname
            /* var disqus_identifier =  ; // unique ID so that disqus fetches the correct comments for each post
            var disqus_url =  ;
            var disqus_title =  ; */

            /* * * DON'T EDIT BELOW THIS LINE * * */
            (function() {
                var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
                dsq.src = 'http://' + disqus_shortname + '.disqus.com/embed.js';
                (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
            })();
        </script>
        <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
        <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</div>
		</div>
		<p>Return to the <a href="/blog.html">main blog page</a></p>
		

</div>

<div id="sidebar">
	<h1>Recent posts</h1>
	<ul>
		
			<li><a href="/blog/2014/01/08/snowplow-0.8.13-released-with-looker-support">Snowplow 0.8.13 released with Looker support</a></li>
		
			<li><a href="/blog/2014/01/08/five-things-that-make-analyzing-snowplow-data-with-looker-an-absolute-pleasure">Five things that make analyzing Snowplow data in Looker an absolute pleasure</a></li>
		
			<li><a href="/blog/2014/01/07/snowplow-0.8.12-released-with-scalding-enrichment-improvements">Snowplow 0.8.12 released with a variety of improvements to the Scalding Enrichment process</a></li>
		
			<li><a href="/blog/2013/12/20/introducing-our-snowplow-winterns">Introducing our Snowplow winterns</a></li>
		
			<li><a href="/blog/2013/12/10/introducing-looker-a-fresh-approach-to-bi-on-snowplow-data">Introducing Looker - a fresh approach to Business Intelligence that works beautifully with Snowplow</a></li>
		
	</ul>

	
		<h1>Other</h1>
		<ul>
		
			
				<li><a href="/blog/2013/12/04/snowplow-at-the-graduate-data-science-initiative">The first Graduate Data Science Initiative event in London</a></li>
			
				<li><a href="/blog/2013/11/11/round-up-and-thank-you-for-the-budapest-bi-conference-last-week">A round up of our trip to the Budapest BI Conference last week, and a thank you to the many people who made the trip so worthwhile</a></li>
			
				<li><a href="/blog/2013/10/28/yali-and-alex-introduce-snowplow-to-code-n">Our video introduction of Snowplow to code_n</a></li>
			
				<li><a href="/blog/2013/10/23/snowplow-team-in-budapest-to-speak-at-open-analytics-conference">Join the Snowplow team in Budapest the first week of November</a></li>
			
				<li><a href="/blog/2013/10/01/snowplow-passes-500-stars">Snowplow passes 500 stars on GitHub</a></li>
			
		
		</ul>		
	
		<h1>Releases</h1>
		<ul>
		
			
				<li><a href="/blog/2014/01/08/snowplow-0.8.13-released-with-looker-support">Snowplow 0.8.13 released with Looker support</a></li>
			
				<li><a href="/blog/2014/01/08/five-things-that-make-analyzing-snowplow-data-with-looker-an-absolute-pleasure">Five things that make analyzing Snowplow data in Looker an absolute pleasure</a></li>
			
				<li><a href="/blog/2014/01/07/snowplow-0.8.12-released-with-scalding-enrichment-improvements">Snowplow 0.8.12 released with a variety of improvements to the Scalding Enrichment process</a></li>
			
				<li><a href="/blog/2013/10/22/snowplow-0.8.11-released-supports-all-cloudfront-file-formats-and-other-improvements">Snowplow 0.8.11 released - supports all Cloudfront log file formats and host of small improvements for power users</a></li>
			
				<li><a href="/blog/2013/10/18/snowplow-0.8.10-released-with-analytics-recipes-and-cubes">Snowplow 0.8.10 released with analytics cubes and recipes 'baked in'</a></li>
			
		
		</ul>		
	
		<h1>Analytics</h1>
		<ul>
		
			
				<li><a href="/blog/2013/12/10/introducing-looker-a-fresh-approach-to-bi-on-snowplow-data">Introducing Looker - a fresh approach to Business Intelligence that works beautifully with Snowplow</a></li>
			
				<li><a href="/blog/2013/11/19/quickstart-guide-to-using-sql-with-snowplow-data-published">Quick start guide to learning SQL to query Snowplow data published</a></li>
			
				<li><a href="/blog/2013/10/28/call-for-data-this-winter">Call for data! Support us develop experimental analyses. Have us help you answer your toughest business questions.</a></li>
			
				<li><a href="/blog/2013/10/22/cohort-analysis-with-using-new-sql-recipes-and-chartio">Using the new SQL views to perform cohort analysis with ChartIO</a></li>
			
				<li><a href="/blog/2013/09/03/using-qubole-to-analyze-snowplow-web-data">Using Qubole to crunch your Snowplow web data using Apache Hive</a></li>
			
		
		</ul>		
	
		<h1>Inside the Plow</h1>
		<ul>
		
			
				<li><a href="/blog/2013/11/20/loading-json-data-into-redshift">Loading JSON data into Redshift - the challenges of quering JSON data, and how Snowplow can be used to meet those challenges</a></li>
			
				<li><a href="/blog/2013/09/27/how-much-does-snowplow-cost-to-run">How much does Snowplow cost to run, vs the competition?</a></li>
			
				<li><a href="/blog/2013/08/12/towards-universal-event-analytics-building-an-event-grammar">Towards universal event analytics - building an event grammar</a></li>
			
				<li><a href="/blog/2013/07/09/understanding-how-different-parts-of-the-Snowplow-data-pipeline-drive-AWS-costs">Unpicking the Snowplow data pipeline and how it drives AWS costs</a></li>
			
				<li><a href="/blog/2013/05/30/dealing-with-hadoops-small-files-problem">Dealing with Hadoop's small files problem</a></li>
			
		
		</ul>		
	
		<h1>Recruitment</h1>
		<ul>
		
			
				<li><a href="/blog/2013/12/20/introducing-our-snowplow-winterns">Introducing our Snowplow winterns</a></li>
			
				<li><a href="/blog/2013/10/07/announcing-our-winter-open-source-internship-program">Announcing our winter open source internship program</a></li>
			
		
		</ul>		
	
		<h1>Research</h1>
		<ul>
		
			
				<li><a href="/blog/2013/10/21/scripting-hadoop-part-1-adventures-with-scala-rhino-and-javascript">Scripting Hadoop, Part One - Adventures with Scala, Rhino and JavaScript</a></li>
			
		
		</ul>		
	

	<h1>Useful links</h1>
	<ul>
		<li><a href="/blog/atom.xml">Atom feed</a></li>
	</ul>
	<!--<strong>Tags</strong> -->
</div>

			<div id="footer">
	<p>Copyright © Snowplow Analytics Limited 2012 - 2013.  All rights reserved</p>
</div>
		</div>
	</div>
		<!-- Following Javascript function used by Disqus to count the number of comments for each blog post and display in the main index -->
	  	<script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'snowplow'; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function () {
            var s = document.createElement('script'); s.async = true;
            s.type = 'text/javascript';
            s.src = 'http://' + disqus_shortname + '.disqus.com/count.js';
            (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
        }());
        </script>
        <!-- begin olark code -->
		<script data-cfasync="false" type='text/javascript'>/*<![CDATA[*/window.olark||(function(c){var f=window,d=document,l=f.location.protocol=="https:"?"https:":"http:",z=c.name,r="load";var nt=function(){
		f[z]=function(){
		(a.s=a.s||[]).push(arguments)};var a=f[z]._={
		},q=c.methods.length;while(q--){(function(n){f[z][n]=function(){
		f[z]("call",n,arguments)}})(c.methods[q])}a.l=c.loader;a.i=nt;a.p={
		0:+new Date};a.P=function(u){
		a.p[u]=new Date-a.p[0]};function s(){
		a.P(r);f[z](r)}f.addEventListener?f.addEventListener(r,s,false):f.attachEvent("on"+r,s);var ld=function(){function p(hd){
		hd="head";return["<",hd,"></",hd,"><",i,' onl' + 'oad="var d=',g,";d.getElementsByTagName('head')[0].",j,"(d.",h,"('script')).",k,"='",l,"//",a.l,"'",'"',"></",i,">"].join("")}var i="body",m=d[i];if(!m){
		return setTimeout(ld,100)}a.P(1);var j="appendChild",h="createElement",k="src",n=d[h]("div"),v=n[j](d[h](z)),b=d[h]("iframe"),g="document",e="domain",o;n.style.display="none";m.insertBefore(n,m.firstChild).id=z;b.frameBorder="0";b.id=z+"-loader";if(/MSIE[ ]+6/.test(navigator.userAgent)){
		b.src="javascript:false"}b.allowTransparency="true";v[j](b);try{
		b.contentWindow[g].open()}catch(w){
		c[e]=d[e];o="javascript:var d="+g+".open();d.domain='"+d.domain+"';";b[k]=o+"void(0);"}try{
		var t=b.contentWindow[g];t.write(p());t.close()}catch(x){
		b[k]=o+'d.write("'+p().replace(/"/g,String.fromCharCode(92)+'"')+'");d.close();'}a.P(2)};ld()};nt()})({
		loader: "static.olark.com/jsclient/loader0.js",name:"olark",methods:["configure","extend","declare","identify"]});
		/* custom configuration goes here (www.olark.com/documentation) */
		olark.identify('9752-503-10-5227');/*]]>*/</script><noscript><a href="https://www.olark.com/site/9752-503-10-5227/contact" title="Contact us" target="_blank">Questions? Feedback?</a> powered by <a href="http://www.olark.com?welcome" title="Olark live chat software">Olark live chat software</a></noscript>
		<!-- end olark code -->
		<!-- Track Olark chats in GTM (so can pass data onto Snowplow) -->
		<script type="text/javascript">
		olark('api.chat.onMessageToOperator', function(event) {
		    dataLayer.push({'event': 'olarkMessageToOperator'});
		});
		olark('api.chat.onMessageToVisitor', function(event) {
		    dataLayer.push({'event': 'olarkMessageToVisitor'});
		});
		</script>
		<!-- end track olark code -->


</body>
</html>