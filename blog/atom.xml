<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom">
 
  <title>The SnowPlow Analytics Blog</title>
  <link href="http://snowplowanalytics.com/"/>
  <link type="application/atom+xml" rel="self" href="http://snowplowanalytics.com/blog/atom.xml"/>
  <updated>2013-01-16T17:23:53+00:00</updated>
  <id>http://snowplowanalytics.com/</id>
  <author>
    <name>The SnowPlow Analytics Team</name>
    <email>contact@snowplowanalytics.com</email>
  </author>

  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/01/16/scala-maxmind-geoip-released</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/01/16/scala-maxmind-geoip-released"/>
    <title>Scala MaxMind GeoIP library released</title>
    <updated>2013-01-16T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;A short blog post this, to announce the release of &lt;a href='https://github.com/snowplow/scala-maxmind-geoip'&gt;scala-maxmind-geoip&lt;/a&gt;, our Scala wrapper for the MaxMind &lt;a href='http://www.maxmind.com/download/geoip/api/java/'&gt;Java Geo-IP&lt;/a&gt; library.&lt;/p&gt;

&lt;p&gt;We have extracted Scala MaxMind GeoIP from our current (ongoing) work porting our ETL process from Apache Hive to &lt;a href='https://github.com/twitter/scalding'&gt;Scalding&lt;/a&gt;. We extracted this as a separate library for two main reasons:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Being good open-source citizens&lt;/strong&gt; - as with our &lt;a href='https://github.com/snowplow/referer-parser'&gt;referer-parser&lt;/a&gt; library, we believe this library willl be useful to the wider community of software developers, not just SnowPlow users&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Keeping SnowPlow&amp;#8217;s footprint small&lt;/strong&gt; - at SnowPlow we believe very strongly in building modular, loosely-coupled software. Massive monolithic systems that &amp;#8216;do everything&amp;#8217; are a nightmare to test, maintain and extend - so we prefer to build small, standalone components and libraries which we (and the community) can then compose into larger pipelines and processes&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;On to the library: for Scala developers, the main benefits of using Scala MaxMind GeoIP over the MaxMind Java library are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Easier to setup/test&lt;/strong&gt; - the SBT project definition automatically pulls down the latest MaxMind Java code and &lt;code&gt;GeoLiteCity.dat&lt;/code&gt;&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Better type safety&lt;/strong&gt; - the MaxMind Java library is somewhat null-happy. This library uses Option boxing wherever possible&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Better performance&lt;/strong&gt; - as well as or instead of using MaxMind&amp;#8217;s own caching (&lt;code&gt;GEOIP_MEMORY_CACHE&lt;/code&gt;), you can also configure an LRU (Least Recently Used) cache of variable size&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;That&amp;#8217;s it! And if you have any problems with this Scala library for MaxMind GeoIP lookups, please &lt;a href='https://github.com/snowplow/snowplow/issues'&gt;raise an issue&lt;/a&gt; or get in touch with us via &lt;a href='https://github.com/snowplow/snowplow/wiki/Talk-to-us'&gt;the usual channels&lt;/a&gt;.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/01/09/from-etl-to-enrichment</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/01/09/from-etl-to-enrichment"/>
    <title>The SnowPlow development roadmap for the ETL step - from ETL to enrichment</title>
    <updated>2013-01-09T00:00:00+00:00</updated>
    <author>
      <name>yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;In this blog post, we outline our plans to develop the &lt;a href='https://github.com/snowplow/snowplow/wiki/etl'&gt;ETL&lt;/a&gt; (&amp;#8220;extract, transform and load&amp;#8221;) part of the SnowPlow stack. Although in many respects the least sexy element of the stack, it is critical to SnowPlow, and we intend to re-architect the ETL step in quite significant ways. In this post, we discuss our plans and the rationale behind them, in the hope to get:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Feedback from the community on them&lt;/li&gt;

&lt;li&gt;Ideas for alternative approaches or new features&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We will cover:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href='/blog/2013/01/09/from-etl-to-enrichment/#purpose'&gt;Recap: the point of the ETL step&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/01/09/from-etl-to-enrichment/#limitations'&gt;Limitations with the current, Hive-based ETL process&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/01/09/from-etl-to-enrichment/#enrichment'&gt;From ETL to enrichment&lt;/a&gt;: what we want the ETL step to achieve&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/01/09/from-etl-to-enrichment/#speed'&gt;Towards a real-time ETL&lt;/a&gt;: speeding things up&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/01/09/from-etl-to-enrichment/#scalding'&gt;Moving to Cascading / Scalding&lt;/a&gt;: what we plan to do&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/01/09/from-etl-to-enrichment/#benefits'&gt;Benefits of this approach&lt;/a&gt;: both in the short and long term&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To get the conversation started, a conceptual map of the new ETL process is shown below. You will probably want to click on it to see a blown-up PDF version, as it is rather large:&lt;/p&gt;
&lt;p&gt;&lt;a href='/static/pdf/snowplow-scalding-etl-specification.pdf'&gt;&lt;img src='/static/img/blog/2013/01/scalding-etl-spec.gif' /&gt;&lt;/a&gt;&lt;/p&gt;&lt;!--more--&gt;&lt;a name='purpose'&gt;&lt;h2&gt;Recap: the point of the ETL step&lt;/h2&gt;&lt;/a&gt;
&lt;p&gt;The primary purpose of the ETL step is to parse the logs generated by the SnowPlow collector(s) and push the data stored into one or more storage facilities (e.g. S3, Infobright) where it can be accessed by analytic tools. However, there are two complexities that have to be dealt with:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Checking data quality and resolving any issues&lt;/strong&gt;. Sometimes, the SnowPlow tracker has not been correctly configured; sometimes, there may even be a bug in a tracker or collector, which means that the log files contain errors. In an ideal world, the ETL step should validate the lines of data in the logs, push data through to storage when the data is good quality, and initiate a process for handling malformed data in the unfortunate cases when it is not. (Note: most web analytics programmes do not support this, so if you haven&amp;#8217;t set your tracking up properly and haven&amp;#8217;t been logging data correctly for a couple of months - tough - there&amp;#8217;s no way of fixing it.) By flagging malformed data quickly, the ETL step should also provide the ops team with a good guide to review the tracker and collector setup, and correct any mistakes.&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Supporting multiple storage options&lt;/strong&gt;. We want SnowPlow to support the widest range of analytics: encompassing &lt;a href='/analytics/tools-and-techniques/converting-snowplow-data-into-a-format-suitable-for-olap.html'&gt;OLAP style aggregations&lt;/a&gt; slicing and dicing of data, &lt;a href='http://mahout.apache.org/'&gt;Mahout-like machine learning&lt;/a&gt; and &lt;a href='https://github.com/skydb'&gt;Sky-like&lt;/a&gt; event stream analytics. The ETL step has to be powerful enough to push data into multiple locations in an efficient manner, and support pushing different cuts and structures of the data into each of those different storage options as required.&lt;/li&gt;
&lt;/ol&gt;
&lt;a name='limitations'&gt;&lt;h2&gt;Limitations with the current, Hive-based ETL process&lt;/h2&gt;&lt;/a&gt;
&lt;p&gt;The current ETL process is based on Hive, which processes Cloudfront-formatted log files containing querystrings matching the &lt;a href='https://github.com/snowplow/snowplow/wiki/snowplow-tracker-protocol'&gt;SnowPlow tracker protocol&lt;/a&gt; using a &lt;a href='https://github.com/snowplow/snowplow/tree/master/3-etl/hive-etl/snowplow-log-deserializers'&gt;custom deserializer&lt;/a&gt;. This was a good option to build an initial prototype of the ETL step: it enabled us to query data in the raw logs directly, and made it relatively straightforward to transfer the data from the SnowPlow log format into a more standard format suitable for faster querying in Hive or importing into Infobright.&lt;/p&gt;

&lt;p&gt;However, there are a number of limitations to the Hive-based ETL process:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;It makes error handling very difficult&lt;/strong&gt;. Either a row is processed, or it is not. There&amp;#8217;s no option to build more sophisticated data processing pipelines including flows to divert malformed data, spot the source of the data quality problem and address it.&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;It is a tightly-coupled process&lt;/strong&gt;: all the parsing on the entire row is performed by the custom deserializer. If something goes wrong, it is hard to debug what went wrong. If we want to extend part of the ETL process, we have to go in and upgrade the deserializer or the HiveQL wrapper scripts. As the conceptual map of our proposed ETL shown at the top of this post demonstrates, our ideal ETL process consists of multiple steps. These should be decoupled for robustness and ease of extension.&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;It is hard to extend the ETL process to build enrichments of the data&lt;/strong&gt;. (See the &lt;a href='#enrichments'&gt;next section&lt;/a&gt;.)&lt;/li&gt;
&lt;/ol&gt;
&lt;a name='enrichment'&gt;&lt;h2&gt;From ETL to enrichment: what we want to achieve&lt;/h2&gt; &lt;/a&gt;
&lt;p&gt;The initial purpose of the ETL step was quite narrow: to move data generated by the collectors into the different storage options for analytics. Since then, we have realised that there are a number of important enrichments that can be performed on the data, that are best done as part of the ETL step, so that they are available when the data comes to be analysed. Examples include:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Inferring location from &lt;code&gt;user_ipaddress&lt;/code&gt; e.g. using &lt;a href='http://www.maxmind.com/en/geolocation_landing'&gt;Maxmind&lt;/a&gt; or &lt;a href='http://www.digitalelement.com/our_technology/our_technology.html'&gt;Digital Element&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;Inferring marketing parameters (source, medium, keywords) by processing referrer url and query strings using &lt;a href='https://github.com/snowplow/referer-parser'&gt;referer-parser&lt;/a&gt;. This would include identifying search engine originated traffic and social network originated traffic, for example&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In addition, decomposing some of the fields into constituent elements can make analysis easier: for example, breaking up &lt;code&gt;page_url&lt;/code&gt; and &lt;code&gt;referrer_url&lt;/code&gt; into host, domain, path and query string, can enable us to easily group visits by referer domain or path, depending on granularity of analysis we&amp;#8217;re performing.&lt;/p&gt;
&lt;a name='speed'&gt;&lt;h2&gt;Towards a real-time ETL process: speeding things up&lt;/h2&gt;&lt;/a&gt;
&lt;p&gt;The majority of SnowPlow users run their ETL process daily, so that yesterday&amp;#8217;s data is available today.&lt;/p&gt;

&lt;p&gt;We need to move the whole SnowPlow stack so that data is available for analytics faster. Doing so will be welcomed by analysts crunching SnowPlow data, but perhaps more significantly, it will open up the possibility of building real-time response engines based on SnowPlow data: these might include things like retargeting users who&amp;#8217;ve performed specific actions with display ads or emails, or personalising the content shown to a user based on their recent browsing history, on the fly.&lt;/p&gt;

&lt;p&gt;There is limited scope to speed up the current Hive-based ETL process. However, there are lots of interesting opportunities that arise if we consider an alternative archtiecture, especially one that moves us closer to a stream-based data processing model.&lt;/p&gt;
&lt;a name='scalding'&gt;&lt;h2&gt;Moving to Cascading / Scalding: how we plan to rearchitect the ETL process&lt;/h2&gt;&lt;/a&gt;
&lt;p&gt;We intend to replace the current Hive-based ETL process with one based on the Scala library that runs on top of &lt;a href='http://www.cascading.org/'&gt;Cascading&lt;/a&gt;, known as &lt;a href='https://github.com/twitter/scalding'&gt;Scalding&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Cascading is an application framework specifically designed to build robust data pipelines using Hadoop. We intend to use it to build the pipeline &lt;a href='/static/pdf/snowplow-scalding-etl-specification.pdf'&gt;sketched above&lt;/a&gt;.&lt;/p&gt;
&lt;a name='benefits'&gt;&lt;h2&gt;Benefits of this approach: both in the short and long term&lt;/h2&gt;&lt;/a&gt;
&lt;p&gt;By rearchitecting the ETL using Scalding / Cascading, we hope to realise the following benefits in the short-term:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Deliver enrichments on the data: in particular, classify visits based on referer, and locate users via geo-ip&lt;/li&gt;

&lt;li&gt;Improved handling of malformed data: making it easier to spot bugs in SnowPlow, mistakes in tracker or collector setup, and the ability to fix and reprocess malformed data&lt;/li&gt;

&lt;li&gt;Make it easier to run the ETL process more frequently, so that SnowPlow data is more up-to-date&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In the long term there are a number of important benefits we hope moving to Scalding will help us realise:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Expand the ETL to output data in a format suitable for OLAP reporting&lt;/strong&gt;. Currently, users who want to use OLAP tools e.g. Tableau, Pentaho or Microstrategy, to report on SnowPlow data, need to &lt;a href='/analytics/tools-and-techniques/converting-snowplow-data-into-a-format-suitable-for-olap.html'&gt;transform that data&lt;/a&gt; prior to running those tools on top of it. We want to build out the ETL process to output two versions of the data: the raw event field (as it currently does) and a cube-formatted version that can be used directly with these tools. Delivering this with the current Hive-based process would be incredibly difficult.&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Move towards a real-time engine&lt;/strong&gt;. In order to deliver data in real-time, SnowPlow ETL would need to move from a Hadoop, batch-based process into a stream-based process, likely using &lt;a href='http://storm-project.net/'&gt;Storm&lt;/a&gt;. Porting the data pipeline from Cascading to Storm should be significantly easier than porting it from Hive to Storm: as such, Cascading provides a useful stepping stone on our journey to deliver real time event-level analytics.&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Make it easier to support a wider range of collector log formats&lt;/strong&gt;. Because the ETL process is decoupled, handling a different log file format means only updating the first processing step in the data pipeline that parses the raw collector logs. That means building out the ETL to support other collectors (e.g. &lt;a href='/blog/2012/08/13/introducing-snow-cannon-a-node-js-collector-for-snowplow/'&gt;SnowCannon&lt;/a&gt;) should be much simpler.&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Make it easier to support a growing range of event types&lt;/strong&gt;. As should be clear from the &lt;a href='/static/pdf/snowplow-scalding-etl-specification.pdf'&gt;data pipeline flowchart&lt;/a&gt;, seven event types are currently supported, each with their own set of fields. (Page views, page pings, link clicks, custom events, ad impressions, transaction events and transaction items.) That list is only likely to grow over time. By clearly differentiating each of them in the data pipeline, a Scalding-based ETL process should be easier to extend to support a greater range of events.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id='we_want_your_feedback'&gt;We want your feedback&lt;/h2&gt;

&lt;p&gt;We&amp;#8217;ve been very lucky to have community members contribute an enormous number of fantastic ideas and code that we&amp;#8217;ve been able to incorporate into SnowPlow. We&amp;#8217;ve shared our roadmap for the ETL step and our rationale for that roadmap to see what you think. Does our approach sound sensible? What should we do differently? What can we add to it to make it more robust and valuable?&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/01/08/using-chartio-to-visualise-and-interrogate-snowplow-data</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/01/08/using-chartio-to-visualise-and-interrogate-snowplow-data"/>
    <title>Using ChartIO to visualise and interrogate SnowPlow data</title>
    <updated>2013-01-08T00:00:00+00:00</updated>
    <author>
      <name>yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;In the last couple of weeks, we have been experimenting with &lt;a href='http://chartio.com/'&gt;ChartIO&lt;/a&gt; - a hosted BI tool for visualising data and creating dashboards. So far, we are very impressed - ChartIO is an excellent analytics tool to use to interrogate and visualise SnowPlow data. Given the number of requests we get from SnowPlow users to recommend tools to assist with analytics on SnowPlow data, we thought it well worth sharing why ChartIO is so good, and give some examples of analyses on SnowPlow data using ChartIO.&lt;/p&gt;

&lt;p&gt;&lt;img alt='chartio-pic-0' src='/static/img/blog/2013/01/chartio-0.png' /&gt;&lt;/p&gt;

&lt;p&gt;In this post we cover:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href='/blog/2013/01/08/using-chartio-to-visualise-and-interrogate-snowplow-data#why'&gt;Why is ChartIO so good?&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/01/08/using-chartio-to-visualise-and-interrogate-snowplow-data#setup'&gt;Setting up ChartIO to work with SnowPlow&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/01/08/using-chartio-to-visualise-and-interrogate-snowplow-data#engagement'&gt;Tutorial: using ChartIO to unpick the drivers of engagement with a site&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a name='why'&gt;Why is ChartIO so good?&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;ChartIO is great for two reasons:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Fast&lt;/strong&gt;. ChartIO is quick to setup. (Because it is a hosted product, with a very nice script for establishing an SSH connection between your database and the ChartIO web application.) At the same time, it is very quick, once a data connection is established, to create new graphs and charts and embed them in dashboards.&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Easy&lt;/strong&gt;. ChartIO is easy to use. This is partly because the UI is really nice. (Lots of drag and drop, easy-to-follow workflow.) But it is also because ChartIO is very simple: it lacks a lot of the complexity of more traditional BI tools like Microstrategy and Pentaho. It is a lot simpler even than more recent innovations in the space like Tableau. Whilst this means it is a bit less powerful, the upside is the tool is a lot easier to use than comparable tools.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;ChartIO has one enormous advantage that makes it especially well suited to querying SnowPlow data: it does not require the data to be in a specific format before it will let users chart / graph it. That compares with the vast majority of tools (including Tableau, Qlikview, Pentaho and Microstrategy) that all require that any data is structured in a format suitable for &lt;a href='/analytics/tools-and-techniques/converting-snowplow-data-into-a-format-suitable-for-olap.html'&gt;OLAP analysis&lt;/a&gt; before they can be used. (We covered how to convert SnowPlow data into that format in the &lt;a href='/analytics/tools-and-techniques/converting-snowplow-data-into-a-format-suitable-for-olap.html'&gt;analytics cookbook&lt;/a&gt;.) ChartIO &lt;strong&gt;does&lt;/strong&gt; work better with data that is formatted in this way, but it still works beautifully with the data as is. As a result, &lt;strong&gt;ChartIO is, we believe, the easiest way to build graphs and dashboards on top of SnowPlow data&lt;/strong&gt;.&lt;/p&gt;
&lt;!--more--&gt;&lt;h2&gt;&lt;a name='setup'&gt;Setting up ChartIO to work with SnowPlow&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;You can get started with ChartIO by signing up to a free 30 day trial. Connecting it to SnowPlow data is straightforward: full instructions can be found &lt;a href='https://github.com/snowplow/snowplow/wiki/ChartIO-setup'&gt;on the setup guide&lt;/a&gt;, including how to create your first graph using SnowPlow data in ChartIO.&lt;/p&gt;
&lt;h2&gt;&lt;a name='engagement'&gt;Tutorial: using ChartIO to unpick the drivers of engagement with a site&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id='before_we_get_started_how_will_we_measure_engagement'&gt;Before we get started: how will we measure engagement?&lt;/h3&gt;

&lt;p&gt;As we discuss in detail in the &lt;a href='/analytics/customer-analytics/user-engagement.html'&gt;analytics cookbook&lt;/a&gt;, there are many possible ways to measure engagement, and SnowPlow supports all of them. We need to pick one or two to use in this tutorial, although it would be possible to perform the analyses described with any measure that suits your business.&lt;/p&gt;

&lt;p&gt;For this tutorial we&amp;#8217;re going to use data from &lt;a href='http://www.psychicbazaar.com/'&gt;Psychic Bazaar&lt;/a&gt;, an online retailer of esoteric products. For an online retailer, whether a visitors makes a purchase is generally more interesting than whether they &amp;#8216;engage&amp;#8217; in vaguer terms. So we will use conversion rate as our first measure of engagement. However, to keep our tutorial interesting to people who want to perform the analysis on non-retail sites, we will also look at number of page views over a period of time as a measure of engagement.&lt;/p&gt;

&lt;h3 id='establishing_the_baseline_measuring_engagement_over_time'&gt;Establishing the baseline: measuring engagement over time&lt;/h3&gt;

&lt;p&gt;Lets start by looking out how engagement has changed over time on Psychic Bazaar. Let&amp;#8217;s create a new dashboard to explore this issue in particular. Log into ChartIO and click on the &lt;strong&gt;+Dashboard&lt;/strong&gt; link on the left hand menu to create a new dashboard.&lt;/p&gt;

&lt;p&gt;&lt;img alt='chartio-pic-1' src='/static/img/blog/2013/01/chartio-1.png' /&gt;&lt;/p&gt;

&lt;p&gt;Give the dashboard a suitable name and description and then click the relevant button to craete it. Now we need to add a chart to it. Click on the &lt;strong&gt;+Chart&lt;/strong&gt; link on the right hand menu. The Chart Creator opens in &lt;strong&gt;interactive mode&lt;/strong&gt;, with your database on the top left, a list of tables under it (including the SnowPlow events table) and under the table, a list of fields split by which ChartIO believes is a measure and dimension.&lt;/p&gt;

&lt;p&gt;&lt;img alt='chartio-pic-2' src='/static/img/blog/2013/01/chartio-2.png' /&gt;&lt;/p&gt;

&lt;p&gt;In interactive mode, ChartIO lets you drag and drop measures into the &lt;strong&gt;Measures&lt;/strong&gt;, &lt;strong&gt;Dimensions&lt;/strong&gt; and &lt;strong&gt;Filters&lt;/strong&gt; dialogue box to generate graphs. We&amp;#8217;re not going to do that, though, because we want to be explicit about how ChartIO uses SnowPlow data. So we&amp;#8217;re going to use &lt;strong&gt;Query mode&lt;/strong&gt; by clicking on the &lt;strong&gt;Query mode&lt;/strong&gt; hyperlink on the top left of the &lt;strong&gt;Layer 1&lt;/strong&gt; box. This enables us to enter a SQL query directly. ChartIO will graph the results:&lt;/p&gt;

&lt;p&gt;&lt;img alt='chartio-pic-3' src='/static/img/blog/2013/01/chartio-3.png' /&gt;&lt;/p&gt;

&lt;p&gt;Now we&amp;#8217;re ready to graph engagement levels over time. Let&amp;#8217;s start with our first measure of engagement: conversion levels. We want to look at what % of users who visit our site each month that perform a transaction. To do this, we first need to identify users who have performed a transaction each month, using the following query:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='mysql'&gt;&lt;span class='k'&gt;SELECT&lt;/span&gt;
&lt;span class='nf'&gt;MONTH&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;dt&lt;/span&gt;&lt;span class='p'&gt;),&lt;/span&gt;
&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='mi'&gt;1&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;buyer&lt;/span&gt;
&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='n'&gt;events_005&lt;/span&gt;
&lt;span class='k'&gt;WHERE&lt;/span&gt; &lt;span class='n'&gt;event&lt;/span&gt;&lt;span class='o'&gt;=&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;transaction&amp;#39;&lt;/span&gt;
&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='n'&gt;dt&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;user_id&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;Now we join this table with the events table to list all the users who have visited each month and identify which of them has bought:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='mysql'&gt;&lt;span class='k'&gt;SELECT&lt;/span&gt;
&lt;span class='n'&gt;visitors&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='n'&gt;visitors&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='n'&gt;buyer&lt;/span&gt;
&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='p'&gt;(&lt;/span&gt;
	&lt;span class='k'&gt;SELECT&lt;/span&gt;
	&lt;span class='nf'&gt;MONTH&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;dt&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='n'&gt;user_id&lt;/span&gt;
	&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='n'&gt;events_005&lt;/span&gt;
	&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='n'&gt;visitors&lt;/span&gt;
&lt;span class='k'&gt;LEFT&lt;/span&gt; &lt;span class='k'&gt;JOIN&lt;/span&gt; &lt;span class='p'&gt;(&lt;/span&gt;
	&lt;span class='k'&gt;SELECT&lt;/span&gt;
	&lt;span class='nf'&gt;MONTH&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;dt&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='mi'&gt;1&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;buyer&lt;/span&gt;
	&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='n'&gt;events_005&lt;/span&gt;
	&lt;span class='k'&gt;WHERE&lt;/span&gt; &lt;span class='n'&gt;event&lt;/span&gt;&lt;span class='o'&gt;=&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;transaction&amp;#39;&lt;/span&gt;
	&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;user_id&lt;/span&gt; &lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='n'&gt;buyers&lt;/span&gt;
&lt;span class='k'&gt;ON&lt;/span&gt; &lt;span class='n'&gt;visitors&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='ss'&gt;`month`&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='n'&gt;buyers&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='ss'&gt;`month`&lt;/span&gt;
&lt;span class='k'&gt;AND&lt;/span&gt; &lt;span class='n'&gt;visitors&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='n'&gt;buyers&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt;
&lt;span class='k'&gt;ORDER&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='n'&gt;visitors&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;visitors&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;Now we can aggregate over the results of the above query, calculating the conversion rate by dividing the number of buyers by the total number of visitors:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='mysql'&gt;&lt;span class='k'&gt;SELECT&lt;/span&gt;
&lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='ss'&gt;`converted_visitors`&lt;/span&gt; &lt;span class='o'&gt;/&lt;/span&gt; &lt;span class='n'&gt;visitors&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;conversion_rate&lt;/span&gt;
&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='p'&gt;(&lt;/span&gt;
	&lt;span class='k'&gt;SELECT&lt;/span&gt;
	&lt;span class='n'&gt;visitors&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='n'&gt;visitors&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='n'&gt;buyer&lt;/span&gt;
	&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='p'&gt;(&lt;/span&gt;
		&lt;span class='k'&gt;SELECT&lt;/span&gt;
		&lt;span class='nf'&gt;MONTH&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;dt&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
		&lt;span class='n'&gt;user_id&lt;/span&gt;
		&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='n'&gt;events_005&lt;/span&gt;
		&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='n'&gt;visitors&lt;/span&gt;
	&lt;span class='k'&gt;LEFT&lt;/span&gt; &lt;span class='k'&gt;JOIN&lt;/span&gt; &lt;span class='p'&gt;(&lt;/span&gt;
		&lt;span class='k'&gt;SELECT&lt;/span&gt;
		&lt;span class='nf'&gt;MONTH&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;dt&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
		&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
		&lt;span class='mi'&gt;1&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;buyer&lt;/span&gt;
		&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='n'&gt;events_005&lt;/span&gt;
		&lt;span class='k'&gt;WHERE&lt;/span&gt; &lt;span class='n'&gt;event&lt;/span&gt;&lt;span class='o'&gt;=&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;transaction&amp;#39;&lt;/span&gt;
		&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;user_id&lt;/span&gt; &lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='n'&gt;buyers&lt;/span&gt;
	&lt;span class='k'&gt;ON&lt;/span&gt; &lt;span class='n'&gt;visitors&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='ss'&gt;`month`&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='n'&gt;buyers&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='ss'&gt;`month`&lt;/span&gt;
	&lt;span class='k'&gt;AND&lt;/span&gt; &lt;span class='n'&gt;visitors&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='n'&gt;buyers&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt;
	&lt;span class='k'&gt;ORDER&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='n'&gt;visitors&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;visitors&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt;
&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='n'&gt;e&lt;/span&gt;
&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt; 
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;Pop the above query in the ChartIO query box:&lt;/p&gt;

&lt;p&gt;&lt;img alt='chartio-pic-4' src='/static/img/blog/2013/01/chartio-4.png' /&gt;&lt;/p&gt;

&lt;p&gt;and click the &lt;strong&gt;Chart Query&lt;/strong&gt; button below. ChartIO will respond with a table of data. We can graph the data by clicking on any of the graph icons above the data table. Choosing the line graph, I get:&lt;/p&gt;

&lt;p&gt;&lt;img alt='chartio-pic-6' src='/static/img/blog/2013/01/chartio-6.png' /&gt;&lt;/p&gt;

&lt;p&gt;We can then rename the graph (by clicking the &lt;strong&gt;edit&lt;/strong&gt; hyperlink that appears when you hover over &lt;strong&gt;Chart Title&lt;/strong&gt;) and save the graph to our dashboard by clicking &lt;strong&gt;Save to Exploring engagement&lt;/strong&gt; button. ChartIO lets us resize and position the graph on the dashboard:&lt;/p&gt;

&lt;p&gt;&lt;img alt='chartio-pic-7' src='/static/img/blog/2013/01/chartio-7.png' /&gt;&lt;/p&gt;

&lt;p&gt;Great! We can see conversion rates were reasonably stable between September and November of the year, but peaked at the end of the year at a height they were previously in June. The figure for September seems suspiciously high - we&amp;#8217;ll drill into this in more detail in a bit. Next we will plot our alternative measure of engagement over time: the number of pageviews per user per month, and see how that has changed over time.&lt;/p&gt;

&lt;p&gt;Calculating the number of pageviews per user per month is straightforward in SnowPlow - we can use the following query:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='mysql'&gt;&lt;span class='k'&gt;SELECT&lt;/span&gt;
&lt;span class='nf'&gt;MONTH&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;dt&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='nf'&gt;count&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='o'&gt;*&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;page_views&lt;/span&gt;
&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='n'&gt;events_005&lt;/span&gt;
&lt;span class='k'&gt;WHERE&lt;/span&gt; &lt;span class='n'&gt;event&lt;/span&gt;&lt;span class='o'&gt;=&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;page_view&amp;#39;&lt;/span&gt;
&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;user_id&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;Now we want to aggregate users by the number of pageviews each has done by month:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='mysql'&gt;&lt;span class='k'&gt;SELECT&lt;/span&gt;
&lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='n'&gt;page_views&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='nf'&gt;count&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='k'&gt;distinct&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;))&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;uniques&lt;/span&gt;
&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='p'&gt;(&lt;/span&gt;
	&lt;span class='k'&gt;SELECT&lt;/span&gt;
	&lt;span class='nf'&gt;MONTH&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;dt&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='nf'&gt;count&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='o'&gt;*&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;page_views&lt;/span&gt;
	&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='n'&gt;events_005&lt;/span&gt;
	&lt;span class='k'&gt;WHERE&lt;/span&gt; &lt;span class='n'&gt;event&lt;/span&gt;&lt;span class='o'&gt;=&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;page_view&amp;#39;&lt;/span&gt;
	&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;user_id&lt;/span&gt;
&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='n'&gt;t&lt;/span&gt;
&lt;span class='k'&gt;ORDER&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt; &lt;span class='k'&gt;ASC&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;page_views&lt;/span&gt; &lt;span class='k'&gt;DESC&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;Lastly we want to bucket values of page views e.g. into 1, 2-5, 6-10, 11-25 and 25+. We can introduce a bucketing into our previous query:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='mysql'&gt;&lt;span class='k'&gt;SELECT&lt;/span&gt;
&lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='n'&gt;page_views&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='k'&gt;CASE&lt;/span&gt;
	&lt;span class='k'&gt;WHEN&lt;/span&gt; &lt;span class='n'&gt;page_views&lt;/span&gt; &lt;span class='o'&gt;&amp;gt;&lt;/span&gt; &lt;span class='mi'&gt;25&lt;/span&gt; &lt;span class='k'&gt;THEN&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;25+&amp;#39;&lt;/span&gt;
	&lt;span class='k'&gt;WHEN&lt;/span&gt; &lt;span class='n'&gt;page_views&lt;/span&gt; &lt;span class='o'&gt;&amp;gt;&lt;/span&gt; &lt;span class='mi'&gt;10&lt;/span&gt; &lt;span class='k'&gt;THEN&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;11-25&amp;#39;&lt;/span&gt;
	&lt;span class='k'&gt;WHEN&lt;/span&gt; &lt;span class='n'&gt;page_views&lt;/span&gt; &lt;span class='o'&gt;&amp;gt;&lt;/span&gt;  &lt;span class='mi'&gt;5&lt;/span&gt; &lt;span class='k'&gt;THEN&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;6-10&amp;#39;&lt;/span&gt;
	&lt;span class='k'&gt;WHEN&lt;/span&gt; &lt;span class='n'&gt;page_views&lt;/span&gt; &lt;span class='o'&gt;&amp;gt;&lt;/span&gt;  &lt;span class='mi'&gt;1&lt;/span&gt; &lt;span class='k'&gt;THEN&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;2-5&amp;#39;&lt;/span&gt;
	&lt;span class='k'&gt;ELSE&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;1&amp;#39;&lt;/span&gt;
&lt;span class='n'&gt;END&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;bucket&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; 
&lt;span class='nf'&gt;count&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='k'&gt;distinct&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;))&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;uniques&lt;/span&gt;
&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='p'&gt;(&lt;/span&gt;
	&lt;span class='k'&gt;SELECT&lt;/span&gt;
	&lt;span class='nf'&gt;MONTH&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;dt&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='nf'&gt;count&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='o'&gt;*&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;page_views&lt;/span&gt;
	&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='n'&gt;events_005&lt;/span&gt;
	&lt;span class='k'&gt;WHERE&lt;/span&gt; &lt;span class='n'&gt;event&lt;/span&gt;&lt;span class='o'&gt;=&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;page_view&amp;#39;&lt;/span&gt;
	&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;user_id&lt;/span&gt;
&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='n'&gt;t&lt;/span&gt;
&lt;span class='k'&gt;ORDER&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt; &lt;span class='k'&gt;ASC&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;page_views&lt;/span&gt; &lt;span class='k'&gt;DESC&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;And then aggregate by bucket in the next query:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='mysql'&gt;&lt;span class='k'&gt;SELECT&lt;/span&gt;
&lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='n'&gt;bucket&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='nf'&gt;sum&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;uniques&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;uniques&lt;/span&gt;
&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='p'&gt;(&lt;/span&gt;
	&lt;span class='k'&gt;SELECT&lt;/span&gt;
	&lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='n'&gt;page_views&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='k'&gt;CASE&lt;/span&gt;
		&lt;span class='k'&gt;WHEN&lt;/span&gt; &lt;span class='n'&gt;page_views&lt;/span&gt; &lt;span class='o'&gt;&amp;gt;&lt;/span&gt; &lt;span class='mi'&gt;25&lt;/span&gt; &lt;span class='k'&gt;THEN&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;25+&amp;#39;&lt;/span&gt;
		&lt;span class='k'&gt;WHEN&lt;/span&gt; &lt;span class='n'&gt;page_views&lt;/span&gt; &lt;span class='o'&gt;&amp;gt;&lt;/span&gt; &lt;span class='mi'&gt;10&lt;/span&gt; &lt;span class='k'&gt;THEN&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;11-25&amp;#39;&lt;/span&gt;
		&lt;span class='k'&gt;WHEN&lt;/span&gt; &lt;span class='n'&gt;page_views&lt;/span&gt; &lt;span class='o'&gt;&amp;gt;&lt;/span&gt;  &lt;span class='mi'&gt;5&lt;/span&gt; &lt;span class='k'&gt;THEN&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;06-10&amp;#39;&lt;/span&gt;
		&lt;span class='k'&gt;WHEN&lt;/span&gt; &lt;span class='n'&gt;page_views&lt;/span&gt; &lt;span class='o'&gt;&amp;gt;&lt;/span&gt;  &lt;span class='mi'&gt;1&lt;/span&gt; &lt;span class='k'&gt;THEN&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;02-05&amp;#39;&lt;/span&gt;
		&lt;span class='k'&gt;ELSE&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;01&amp;#39;&lt;/span&gt;
	&lt;span class='n'&gt;END&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;bucket&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; 
	&lt;span class='nf'&gt;count&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='k'&gt;distinct&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;))&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;uniques&lt;/span&gt;
	&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='p'&gt;(&lt;/span&gt;
		&lt;span class='k'&gt;SELECT&lt;/span&gt;
		&lt;span class='nf'&gt;MONTH&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;dt&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
		&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
		&lt;span class='nf'&gt;count&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='o'&gt;*&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;page_views&lt;/span&gt;
		&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='n'&gt;events_005&lt;/span&gt;
		&lt;span class='k'&gt;WHERE&lt;/span&gt; &lt;span class='n'&gt;event&lt;/span&gt;&lt;span class='o'&gt;=&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;page_view&amp;#39;&lt;/span&gt;
		&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;user_id&lt;/span&gt;
	&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='n'&gt;t&lt;/span&gt;
	&lt;span class='k'&gt;ORDER&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt; &lt;span class='k'&gt;ASC&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;page_views&lt;/span&gt; &lt;span class='k'&gt;DESC&lt;/span&gt;
&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='n'&gt;u&lt;/span&gt;
&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;bucket&lt;/span&gt;
&lt;span class='k'&gt;ORDER&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;bucket&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;Create a new chart in ChartIO using the above query and graph it using the first bar chart icon. Give the graph a suitable name:&lt;/p&gt;

&lt;p&gt;&lt;img alt='chartio-pic-8' src='/static/img/blog/2013/01/chartio-8.png' /&gt;&lt;/p&gt;

&lt;p&gt;This graph tells an interesting story. Overall, the number of unique visitors per month has grown pretty dramatically over time, peaking at about 1700 uniques in November. It is not so easy to tell how the distribution of users by engagement level has changed over time: this is easier if we change the graph to be a &amp;#8220;percent bar&amp;#8221;:&lt;/p&gt;

&lt;p&gt;&lt;img alt='chartio-pic-9' src='/static/img/blog/2013/01/chartio-9.png' /&gt;&lt;/p&gt;

&lt;p&gt;This graph suggests that engagement levels dropped in October, but climbed dramatically from then to December. Curiously, there was no drop in overall engagement level as user numbers increased on the site between August and October: that means that the new users acquired were &amp;#8220;high quality&amp;#8221; or &amp;#8220;highly engaged&amp;#8221;. This is a useful graph: let&amp;#8217;s add it to our dashboard alongside the first graph we created:&lt;/p&gt;

&lt;p&gt;&lt;img alt='chartio-pic-10' src='/static/img/blog/2013/01/chartio-10.png' /&gt;&lt;/p&gt;

&lt;p&gt;Just to put the two baseline graphs in context, let&amp;#8217;s add a third graph the tracks the number of unique users per month to our dashboard. Add a new chart using the following simple query:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='mysql'&gt;&lt;span class='k'&gt;SELECT&lt;/span&gt;
&lt;span class='nf'&gt;MONTH&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;dt&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='nf'&gt;COUNT&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='k'&gt;DISTINCT&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;))&lt;/span&gt; &lt;span class='k'&gt;as&lt;/span&gt; &lt;span class='n'&gt;uniques&lt;/span&gt;
&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='n'&gt;events_004&lt;/span&gt;
&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt;
&lt;span class='k'&gt;ORDER&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;And pop it on the dashboard:&lt;/p&gt;

&lt;p&gt;&lt;img alt='chartio-pic-11' src='/static/img/blog/2013/01/chartio-11.png' /&gt;&lt;/p&gt;

&lt;p&gt;Our baseline data tells us an interesting story, which from the dashboard, we&amp;#8217;re in a position to summarise:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Overall, visitor numbers have increased pretty dramatically between June and September&lt;/li&gt;

&lt;li&gt;Over that same period, there was no corresponding drop in engagement, in terms of numbers of page views by visit. If anything, there was a slight increase&lt;/li&gt;

&lt;li&gt;Looking at conversion rates over the same time, the picture is much more hairy. (With a surprising spike in July.) In fact, we know this was to do with a bug on the website, which prevented data being collected from any page apart from the checkout page. Hence user numbers are underreported, but conversion rates are overstated, for July&lt;/li&gt;

&lt;li&gt;Conversion rates and average page views per visit rise in December&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id='unpicking_the_drivers_of_changing_engagement_levels'&gt;Unpicking the drivers of changing engagement levels&lt;/h3&gt;

&lt;p&gt;For our sample data set there appears to be a rather interesting rise in engagement level (as measured by both conversion rates and page views by month) between November and December. What&amp;#8217;s driving that increase? What clues can our SnowPlow data give us?&lt;/p&gt;

&lt;p&gt;We can divide drivers into two groups: those that effect all users on our website, and those that only effect some of them. If, for example, we performed a measure rearchitecture of our entire site, that is likely to effect &lt;strong&gt;all&lt;/strong&gt; users&amp;#8217; behaviour. But if we upgraded the site for mobile, then we would &lt;strong&gt;only&lt;/strong&gt; expect that to impact user behaviour for people browsing from mobile sites.&lt;/p&gt;

&lt;p&gt;A good approach, then, to unpick what&amp;#8217;s driving growth in engagement levels is to see if this growth is consistent across all users, or just some of them. One easy way to do this is to compare engagement rates between different types of users, to see if we can spot a difference. It makes sense to start off with factors we have a hunch might be driving those changes (e.g. because we&amp;#8217;re familiar with what has changed at those business over the months in question.) To give a specific examples:&lt;/p&gt;

&lt;h4 id='comparing_engagement_levels_between_users_from_paid_search_campaigns_and_notpaid_search_campaigns'&gt;Comparing engagement levels between users from paid search campaigns and not-paid search campaigns&lt;/h4&gt;

&lt;p&gt;Psychic Bazaar&amp;#8217;s only direct marketing spend is on paid search campaigns on Google and Bing. We might therefore wonder whether a change to those campaigns drove the uplift in engagement we see on the site in September. To do this, first we need to identify all the users acquired via paid search:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='mysql'&gt;&lt;span class='k'&gt;SELECT&lt;/span&gt;
&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='s1'&gt;&amp;#39;1&amp;#39;&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;paid_search&lt;/span&gt;
&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='n'&gt;events_005&lt;/span&gt;
&lt;span class='k'&gt;WHERE&lt;/span&gt; &lt;span class='n'&gt;mkt_medium&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;cpc&amp;#39;&lt;/span&gt;
&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='n'&gt;user_id&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;Compare this with our data on which users have converted:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='mysql'&gt;&lt;span class='k'&gt;SELECT&lt;/span&gt;
&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='s1'&gt;&amp;#39;1&amp;#39;&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;buyer&lt;/span&gt;
&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='n'&gt;events_005&lt;/span&gt;
&lt;span class='k'&gt;WHERE&lt;/span&gt; &lt;span class='n'&gt;event&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='n'&gt;transaction&lt;/span&gt;
&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='n'&gt;user_id&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;And our list of &lt;strong&gt;all&lt;/strong&gt; users:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='mysql'&gt;&lt;span class='k'&gt;SELECT&lt;/span&gt;
&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='s1'&gt;&amp;#39;1&amp;#39;&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;visitor&lt;/span&gt;
&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='n'&gt;events_005&lt;/span&gt;
&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='n'&gt;user_id&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;We join the three data sets:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='mysql'&gt;&lt;span class='k'&gt;SELECT&lt;/span&gt;
&lt;span class='n'&gt;visitors&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='n'&gt;buyer&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='n'&gt;paid_search&lt;/span&gt;
&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='p'&gt;(&lt;/span&gt;
	&lt;span class='k'&gt;SELECT&lt;/span&gt;
	&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='s1'&gt;&amp;#39;1&amp;#39;&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;visitor&lt;/span&gt;
	&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='n'&gt;events_005&lt;/span&gt;
	&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='n'&gt;user_id&lt;/span&gt;
	&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='n'&gt;visitors&lt;/span&gt;
&lt;span class='k'&gt;LEFT&lt;/span&gt; &lt;span class='k'&gt;JOIN&lt;/span&gt; &lt;span class='p'&gt;(&lt;/span&gt;
	&lt;span class='k'&gt;SELECT&lt;/span&gt;
	&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='s1'&gt;&amp;#39;1&amp;#39;&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;buyer&lt;/span&gt;
	&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='n'&gt;events_005&lt;/span&gt;
	&lt;span class='k'&gt;WHERE&lt;/span&gt; &lt;span class='n'&gt;event&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='n'&gt;transaction&lt;/span&gt;
	&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='n'&gt;user_id&lt;/span&gt;
&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='n'&gt;buyers&lt;/span&gt;
&lt;span class='k'&gt;ON&lt;/span&gt; &lt;span class='n'&gt;visitors&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='n'&gt;buyers&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt;
&lt;span class='k'&gt;LEFT&lt;/span&gt; &lt;span class='k'&gt;JOIN&lt;/span&gt; &lt;span class='p'&gt;(&lt;/span&gt;
	&lt;span class='k'&gt;SELECT&lt;/span&gt;
	&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='s1'&gt;&amp;#39;1&amp;#39;&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;paid_search&lt;/span&gt;
	&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='n'&gt;events_005&lt;/span&gt;
	&lt;span class='k'&gt;WHERE&lt;/span&gt; &lt;span class='n'&gt;mkt_medium&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;cpc&amp;#39;&lt;/span&gt;
	&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='n'&gt;user_id&lt;/span&gt;
	&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='n'&gt;paid_search&lt;/span&gt;
&lt;span class='k'&gt;ON&lt;/span&gt; &lt;span class='n'&gt;visitors&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='n'&gt;paid_search&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;And then aggregate over the result set to compare conversion rates:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='mysql'&gt;&lt;span class='k'&gt;SELECT&lt;/span&gt;
&lt;span class='n'&gt;paid_search&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='nf'&gt;sum&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;buyer&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt;&lt;span class='o'&gt;/&lt;/span&gt;&lt;span class='nf'&gt;sum&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;visitor&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;conversion_rate&lt;/span&gt;
&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='p'&gt;(&lt;/span&gt;
&lt;span class='k'&gt;SELECT&lt;/span&gt;
&lt;span class='n'&gt;visitors&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='n'&gt;buyer&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='n'&gt;paid_search&lt;/span&gt;
&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='p'&gt;(&lt;/span&gt;
		&lt;span class='k'&gt;SELECT&lt;/span&gt;
		&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
		&lt;span class='s1'&gt;&amp;#39;1&amp;#39;&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;visitor&lt;/span&gt;
		&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='n'&gt;events_005&lt;/span&gt;
		&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='n'&gt;user_id&lt;/span&gt;
		&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='n'&gt;visitors&lt;/span&gt;
	&lt;span class='k'&gt;LEFT&lt;/span&gt; &lt;span class='k'&gt;JOIN&lt;/span&gt; &lt;span class='p'&gt;(&lt;/span&gt;
		&lt;span class='k'&gt;SELECT&lt;/span&gt;
		&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
		&lt;span class='s1'&gt;&amp;#39;1&amp;#39;&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;buyer&lt;/span&gt;
		&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='n'&gt;events_005&lt;/span&gt;
		&lt;span class='k'&gt;WHERE&lt;/span&gt; &lt;span class='n'&gt;event&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='n'&gt;transaction&lt;/span&gt;
		&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='n'&gt;user_id&lt;/span&gt;
	&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='n'&gt;buyers&lt;/span&gt;
	&lt;span class='k'&gt;ON&lt;/span&gt; &lt;span class='n'&gt;visitors&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='n'&gt;buyers&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt;
	&lt;span class='k'&gt;LEFT&lt;/span&gt; &lt;span class='k'&gt;JOIN&lt;/span&gt; &lt;span class='p'&gt;(&lt;/span&gt;
		&lt;span class='k'&gt;SELECT&lt;/span&gt;
		&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
		&lt;span class='s1'&gt;&amp;#39;1&amp;#39;&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;paid_search&lt;/span&gt;
		&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='n'&gt;events_005&lt;/span&gt;
		&lt;span class='k'&gt;WHERE&lt;/span&gt; &lt;span class='n'&gt;mkt_medium&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;cpc&amp;#39;&lt;/span&gt;
		&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='n'&gt;user_id&lt;/span&gt;
		&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='n'&gt;paid_search&lt;/span&gt;
	&lt;span class='k'&gt;ON&lt;/span&gt; &lt;span class='n'&gt;visitors&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='n'&gt;paid_search&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt;
	&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='n'&gt;t&lt;/span&gt;
&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='n'&gt;paid_search&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;Plotting the data in ChartIO we can see that users acquired from paid campaigns are much more likely to convert:&lt;/p&gt;

&lt;p&gt;&lt;img alt='chartio-pic-12' src='/static/img/blog/2013/01/chartio-12.png' /&gt;&lt;/p&gt;

&lt;p&gt;This naturally leads to the question: has the number of users acquired from paid search increased over the time period? (Especially between November and December, when our increase in conversion rates is most noticeable?) We can find out by graphing the following query, which looks at the number of uniques by month divided by whether they were acquired by paid search or not:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='mysql'&gt;&lt;span class='k'&gt;SELECT&lt;/span&gt;
&lt;span class='nf'&gt;MONTH&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;dt&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='n'&gt;paid_search&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='nf'&gt;COUNT&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='k'&gt;DISTINCT&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;e&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;))&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;uniques&lt;/span&gt;
&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='n'&gt;events_005&lt;/span&gt; &lt;span class='n'&gt;e&lt;/span&gt;
&lt;span class='k'&gt;LEFT&lt;/span&gt; &lt;span class='k'&gt;JOIN&lt;/span&gt; &lt;span class='p'&gt;(&lt;/span&gt;
	&lt;span class='k'&gt;SELECT&lt;/span&gt;
	&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='s1'&gt;&amp;#39;1&amp;#39;&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;paid_search&lt;/span&gt;
	&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='n'&gt;events_005&lt;/span&gt;
	&lt;span class='k'&gt;WHERE&lt;/span&gt; &lt;span class='n'&gt;mkt_medium&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;cpc&amp;#39;&lt;/span&gt;
	&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='n'&gt;user_id&lt;/span&gt;
&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='n'&gt;paid_search&lt;/span&gt;
&lt;span class='k'&gt;ON&lt;/span&gt; &lt;span class='n'&gt;e&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='n'&gt;paid_search&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt;
&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;paid_search&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;Plotting the above graph shows that growth in paid search traffic accounts for some of the growth in traffic volumes between July and September. However, there was &lt;strong&gt;no&lt;/strong&gt; increase in traffic from paid search terms between November and December, so this does &lt;strong&gt;not&lt;/strong&gt; account for the rising conversion rate in December.&lt;/p&gt;

&lt;p&gt;&lt;img alt='chartio-pic-13' src='/static/img/blog/2013/01/chartio-13.png' /&gt;&lt;/p&gt;

&lt;h4 id='other_factors_that_might_account_for_the_rise'&gt;Other factors that might account for the rise&lt;/h4&gt;

&lt;p&gt;There is a wealth of other factors that we can explore using SnowPlow data, to see if they account for the rise in engagement levels / conversion rates in December. Doing so is beyond the scope of this blog post. However, we can outline them:&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;&lt;strong&gt;Factor&lt;/strong&gt;&lt;/th&gt;&lt;th&gt;&lt;strong&gt;How we would test it&lt;/strong&gt;&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Improvement to the site structure (e.g. homepage)&lt;/td&gt;&lt;td style='text-align: left;'&gt;Investigate how engagement levels vary by visit by landing page, and see if those changes by landing page on dates when those web pages were updated&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Improvements to the checkout flow&lt;/td&gt;&lt;td style='text-align: left;'&gt;Compare the conversion funnel between November and December - see if there&amp;#8217;s a specific point in the funnel where conversion rates change, and see if that change can be attributed to a development change&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;A change in the makeup of the users e.g. so that in December, a bigger portion of the userbase are repeat visitors&lt;/td&gt;&lt;td style='text-align: left;'&gt;Explore whether there is a change in makeup (e.g. more repeat visitors as a proportion of uniques) and see if there&amp;#8217;s a corresponding difference in conversion rates by different types of users (e.g. new vs returning). Note: this is the same approach as described above for user acquired from &lt;em&gt;paid search&lt;/em&gt;.&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Christmas&lt;/td&gt;&lt;td style='text-align: left;'&gt;Hard to prove definitively - but if no other factor can be identified, and the engagement level drops back in January, then the December bump might be season.&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;SnowPlow makes it possible to drill into all of the above, and other factors we can think of, to see which is responsible for driving changing engagement levels.&lt;/p&gt;

&lt;h2 id='summarising_our_thoughts_on_chartio'&gt;Summarising our thoughts on ChartIO&lt;/h2&gt;

&lt;p&gt;From our experience with it in the last couple of weeks, we believe that ChartIO is an excellent tool for visualising SnowPlow data. We highly recommend SnowPlow users give it a try,: ChartIO&amp;#8217;s simplicitly, speed, and lack of assumptions about the way data is structured make it an ideal analytics tool to run directly on top of SnowPlow data stored in Infobright.&lt;/p&gt;

&lt;p&gt;We&amp;#8217;re going to continue to use ChartIO (and blog about the results). We&amp;#8217;d love to hear from other SnowPlow users who are using it.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/01/07/the-clojure-collector-in-detail</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/01/07/the-clojure-collector-in-detail"/>
    <title>Understanding the thinking behind the Clojure Collector, and mapping out its development going forwards</title>
    <updated>2013-01-07T00:00:00+00:00</updated>
    <author>
      <name>yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;Last week we released &lt;a href='/blog/2013/01/03/snowplow-0.7.0-released/'&gt;SnowPlow 0.7.0&lt;/a&gt;: which included a new Clojure Collector, with some significant new functionality for content networks and ad networks in particular. In this post we explain a lot of the thinking behind the Clojure Collector architecture, before taking a look ahead at the short and long-term development roadmap for the collector.&lt;/p&gt;

&lt;p&gt;This is the first in a series of posts we write where describe in some detail the thinking behind the architecture and design of SnowPlow components, and discuss how we plan to develop those components over time. The purpose of doing so is to engage people like yourself: developers and analysts in the SnowPlow community, in a discussion about how best to evolve SnowPlow. The reasoning is simple: we have had many fantastic ideas and contributions from community members that have proved invaluable in driving SnowPlow development, and we want to encourage more of these conversations and contributions, to help make SnowPlow great.&lt;/p&gt;

&lt;p&gt;&lt;img alt='engine' src='/static/img/blog/2013/01/engine.jpg' /&gt;&lt;/p&gt;

&lt;h2 id='contents'&gt;Contents&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href='/blog/2013/01/07/the-clojure-collector-in-detail#biz-case'&gt;The business case for a new collector: understanding the limitations of the Cloudfront Collector&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/01/07/the-clojure-collector-in-detail#under-the-hood'&gt;Under the hood: the design decisions behind the Clojure Collector&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/01/07/the-clojure-collector-in-detail#short-term-roadmap'&gt;Moving forwards: short term Clojure Collector roadmap&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/01/07/the-clojure-collector-in-detail#long-term-roadmap'&gt;Looking ahead: long term collector roadmap&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;!--more--&gt;&lt;h2&gt;&lt;a name='biz-case'&gt;1. The business case for a new collector: understanding the limitations of the Cloudfront Collector&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;We launched SnowPlow with the &lt;a href='https://github.com/snowplow/snowplow/wiki/setting-up-the-cloudfront-collector'&gt;Cloudfront Collector&lt;/a&gt;. The Cloudfront Collector is simple:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The SnowPlow tracking pixel is served from Amazon Cloudfront&lt;/li&gt;

&lt;li&gt;Cloudfront logging is switched on (so that every time the pixel is fetched by a SnowPlow tracking tag, the request is logged).&lt;/li&gt;

&lt;li&gt;Events and associated data points we want to capture are stored as name / value pairs and appended to the query string for the tracking pixel, so that they are automatically logged.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The Cloudfront Collector is so simple that many people express surprise that there are so few files in the appropriate section of the &lt;a href='https://github.com/snowplow/snowplow/tree/master/2-collectors/cloudfront-collector'&gt;SnowPlow repo&lt;/a&gt;. (It&amp;#8217;s just a &lt;code&gt;readme&lt;/code&gt; and the tracking pixel.) In spite of that simplicity, however, the Cloudfront Collector boasts two key strengths:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Simplicity&lt;/strong&gt;. Simplicity is a strength: because it has no moving parts, the Cloudfront Collector is incredibly robust. It makes no decisions. All it does is faithfully log requests made to the tracking pixel. There is very little that can go wrong with it. (Nothing if Cloudfront stays live.)&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Scalability&lt;/strong&gt;. By using Amazon&amp;#8217;s content distribution network (CDN) to serve the tracking pixel and log requests for the tracking pixel, we can be confident that businesses using the Cloudfront Collector will be able to comfortably track millions of requests per hour. Amazon&amp;#8217;s CDN has been designed to be elastic: it responds automatically to spikes in demand, so you can be confident that even in during peak demand periods, all your data will successfully be captured and stored.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Nonetheless, there are two major limitations to the Cloudfront Collector:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Unable to track users across domains&lt;/strong&gt;. Because SnowPlow has been designed to be scalable, we&amp;#8217;ve had a lot of interest in it from media groups, content networks and ad networks. All of these companies want to track individual users across multiple websites. This is not directly supported by the Cloudfront Collector: because it has no moving parts, user identification has to be performed client side, by the &lt;a href='https://github.com/snowplow/snowplow/wiki/javascript-tracker-setup'&gt;Javascript tracker&lt;/a&gt; using first party cookies. As a result, &lt;code&gt;user_id&lt;/code&gt;s that are set on one domain cannot be accessed on another domain, even if both domains are owned and operated by the same company.&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Not real-time&lt;/strong&gt;. Cloudfront log files typically appear in S3 3-4 hours after the requests logged were made. As a result, if you rely on the Cloudfront cCollector for your web analytics data, you will always be looking at data that is at least 3-4 hours old.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The Clojure Collector explicitly addresses the first issue identified above: it has a single moving part, which checks if a &lt;code&gt;user_id&lt;/code&gt; has been set for this user: if so, it logs that &lt;code&gt;user_id&lt;/code&gt;. If not, it sets a &lt;code&gt;user_id&lt;/code&gt; (server side), and stores that &lt;code&gt;user_id&lt;/code&gt; in a cookie on the collectors own domain, accessible from any website running SnowPlow that uses the same collector.&lt;/p&gt;

&lt;p&gt;The Clojure Collector does not explicitly address the second issue related to the speed at which data is logged for analysis. Although it logs data faster than the Cloudfront Collector (logs are rotated to S3 hourly), this is still not fast enough for real time analysis. However, it is fast enough that the processing bottleneck shifts from the collector to the ETL step: this is something we plan on addressing in the near future. (More on this &lt;a href='#long-term-roadmap'&gt;later&lt;/a&gt;.)&lt;/p&gt;
&lt;h2&gt;&lt;a name='under-the-hood'&gt;2. Under the hood: the design decisions behind the Clojure Collector&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;We made three important design decisions when building the Clojure Collector:&lt;/p&gt;

&lt;h3 id='1_built_for_elastic_beanstalk'&gt;1. Built for Elastic Beanstalk&lt;/h3&gt;

&lt;p&gt;We built the Clojure Collector specifically for Elastic Beanstalk. This has a number of important advantages:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Comfortably scales to handle spikes in demand&lt;/strong&gt;. Elastic Beanstalk is &lt;strong&gt;elastic&lt;/strong&gt; in the same way as Cloudfront is &lt;strong&gt;elastic&lt;/strong&gt;. It makes it easy to scale services to handle spikes in demand, which is crucial if we&amp;#8217;re going to continue to track events data during spikes in service usage.&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Automatic logging to S3&lt;/strong&gt;. Elastic Beanstalk supports a configuration option that automatically rotates Tomcat access logs to Elastic Beanstalk hourly. By using this feature, we were able to save ourselves having to build build a process to manage that log rotation, and save our users the hassle of installing and maintaining the process.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Amazon Elastic Beanstalk supports open source applications built for the JVM, or PHP, Python and Ruby web apps. Of the four, it was clear that JVM was the most performative platform to build a Collector in.&lt;/p&gt;

&lt;h3 id='2_minimal_moving_parts'&gt;2. Minimal moving parts&lt;/h3&gt;

&lt;p&gt;The Clojure collector &lt;em&gt;only&lt;/em&gt; sets &lt;code&gt;user_id&lt;/code&gt;s and expiry dates on those &lt;code&gt;user_id&lt;/code&gt;s. It does &lt;em&gt;nothing&lt;/em&gt; else: keeping it as simple as possible.&lt;/p&gt;

&lt;h3 id='3_log_files_formats_match_those_produced_by_cloudfront'&gt;3. Log files formats match those produced by Cloudfront&lt;/h3&gt;

&lt;p&gt;The least wieldy part of the SnowPlow stack today is the &lt;a href='https://github.com/snowplow/snowplow/wiki/choosing-an-etl-module'&gt;ETL step&lt;/a&gt;. This parses the log files produced by the collector, extracts the relevant data points and loads them into S3 for processing by Hadoop/Hive and/or Infobright for processing in a wide range of tools e.g. &lt;a href='http://chartio.com'&gt;ChartIO&lt;/a&gt; or &lt;a href='http://www.r-project.org/'&gt;R&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We have plans to replace the current &lt;a href='https://github.com/snowplow/snowplow/wiki/hive-etl-setup'&gt;Hive-based ETL process&lt;/a&gt; with an all new process based on &lt;a href='https://github.com/twitter/scalding'&gt;Scalding&lt;/a&gt;. (More on this in the next blog post in this series.) In the meantime, however, we did not want to have to write a new Hive deserializer to parse log files that match a new format: instead, we customised Tomcat in the Clojure Collector to output log files that matched the Cloudfront logging format. (This involved writing a custom &lt;a href='[tomcat-cf-access-valve]'&gt;Tomact Access Valve&lt;/a&gt; and tailoring &lt;a href='https://github.com/snowplow/snowplow/blob/master/2-collectors/clojure-collector/war-resources/.ebextensions/server.xml'&gt;Tomcat&amp;#8217;s server.xml&lt;/a&gt;.) As a result, the new Clojure Collector plays well with the existing ETL process.&lt;/p&gt;
&lt;h2&gt;&lt;a name='short-term-roadmap'&gt;3. Moving forwards: short term Clojure Collector roadmap&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;This is the initial release of the Clojure Collector. If it will be deployed by large media companies, content networks and ad networks, it is important that we learn how to configure it to function well at scale. To this end, we are looking for help, from members of the SnowPlow community (particularly those with an interest in tracking users across domains), to help with the following:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Load testing the collector. Test how fast the collector responds to increasing number of requests per second, and how this varies by the size of instance offered by Amazon. (E.g. how does the curve differ for an m1.small instance than an m1.large instance?) It should be possible to use a tool like &lt;a href='http://www.joedog.org/siege-home/'&gt;Siege&lt;/a&gt; or &lt;a href='http://httpd.apache.org/docs/2.2/programs/ab.html'&gt;Apache Bench&lt;/a&gt; to test response levels and response times at increasing levels of request concurrency, and plot one against the other.&lt;/li&gt;

&lt;li&gt;On the basis of the above, working out the optimal way of setting up the Clojure Collector on Elastic Beanstalk. It would be good to answer two questions in particular: what size instance is it most cost effective to use, and what should trigger the starting up of an additional instance to cope with a spike in traffic? Amazon makes it possible to specify custom KPI to use to trigger scaling of services on Elastic Beanstalk, and it may be that doing so results in much improved performance and reliability from the Collector.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Because we haven&amp;#8217;t been able to perform the above tests to date, we&amp;#8217;re still calling the Clojure Collector an experimental release, adn recommend that companies using it in production run it alongside the Cloudfront Collector.&lt;/p&gt;
&lt;h2&gt;&lt;a name='long-term-roadmap'&gt;4. Looking ahead: long term collector roadmap&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Long term we need to move the whole SnowPlow so that it&amp;#8217;s processing data faster, closer to real-time. This primarily means moving the &lt;a href='https://github.com/snowplow/snowplow/wiki/choosing-an-etl-module'&gt;ETL&lt;/a&gt; process from a Hadoop, batch-based process that is run at regular intervals to a stream-based, always on process, using a technology like &lt;a href='http://storm-project.net/'&gt;Storm&lt;/a&gt;. In the next post in this blog post series, we will elaborate further on our proposed developments for this part of the SnowPlow stack. When the time comes, however, we will need to build a new collector, or modify an existing collector, to work in a stream-based system. (So that rather than rely on the processing of logs, each new event logged generates a message in a queue that kicks of a set of analytic processing tasks that end with the data being stored in S3 / Infobright.)&lt;/p&gt;

&lt;h2 id='want_to_get_involved'&gt;Want to get involved?&lt;/h2&gt;

&lt;p&gt;Want to help us develop the Clojure Collector, or some other part of the SnowPlow stack? Have an idea about what we should be doing better, or differently? Then &lt;a href='/contact/index.html'&gt;get in touch&lt;/a&gt;. We&amp;#8217;d love to hear from you.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/01/03/snowplow-0.7.0-released</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/01/03/snowplow-0.7.0-released"/>
    <title>SnowPlow 0.7.0 released, with new Clojure-based collector</title>
    <updated>2013-01-03T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;Today we are hugely excited to announce the release of SnowPlow version &lt;strong&gt;0.7.0&lt;/strong&gt;, which includes an experimental new &lt;a href='https://github.com/snowplow/snowplow/tree/master/2-collectors/clojure-collector'&gt;Clojure-based collector&lt;/a&gt; designed to run on &lt;a href='http://aws.amazon.com/elasticbeanstalk/'&gt;Amazon Elastic Beanstalk&lt;/a&gt;. This release allows you to use SnowPlow to uniquely identify and track users across multiple domains - even across a whole content or advertising network.&lt;/p&gt;

&lt;p&gt;Many thanks to community member &lt;a href='https://github.com/shermozle'&gt;Simon Rumble&lt;/a&gt; for developing many of the ideas underpinning the new collector in &lt;a href='https://github.com/shermozle/SnowCannon'&gt;SnowCannon&lt;/a&gt;, his node.js-based collector for SnowPlow.&lt;/p&gt;

&lt;p&gt;To date, the primary collector for SnowPlow events has been our CloudFront-based collector. The CloudFront-based collector has been easy to setup and very reliable, but has one main drawback: it does not support user tracking across multiple domains.&lt;/p&gt;

&lt;p&gt;The Clojure-based collector changes this: it sets a unique user ID server-side and returns it to the browser as a third-party cookie; this user ID is then stored with your SnowPlow events, instead of the first-party cookie set by the JavaScript tracker. This means that user=123 on, say, &lt;a href='http://maven.snplow.com'&gt;maven.snplow.com&lt;/a&gt; will be the same as user=123 on &lt;a href='http://snowplowanalytics.com'&gt;snowplowanalytics.com&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;And the other good news is that our Clojure collector automatically logs the raw SnowPlow events to Amazon S3 - and it logs in the exact same format as the CloudFront-based collector, so we can use the same ETL process for both collectors!&lt;/p&gt;

&lt;p&gt;Read on below the fold for installation instructions and some additional information on this release.&lt;/p&gt;
&lt;!--more--&gt;
&lt;h2 id='installation_instructions'&gt;Installation instructions&lt;/h2&gt;

&lt;h3 id='clojurebased_collector'&gt;Clojure-based collector&lt;/h3&gt;

&lt;p&gt;You will find full instructions on setting up the new Clojure-based collector on our Wiki, &lt;a href='https://github.com/snowplow/snowplow/wiki/setting-up-the-clojure-collector'&gt;Setting up the Clojure collector&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id='etl'&gt;ETL&lt;/h3&gt;

&lt;p&gt;If you are using EmrEtlRunner, you need to update to the latest version, which is 0.0.7 - this is available by checking out the master branch of the &lt;a href='https://github.com/snowplow/snowplow'&gt;SnowPlow repository&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;You will also need to update your configuration file, &lt;code&gt;config.yml&lt;/code&gt;, to use the latest versions of the HiveQL scripts:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:snowplow:
  # ...
  :hive_hiveql_version: 0.5.4
  :non_hive_hiveql_version: 0.0.5&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id='storage'&gt;Storage&lt;/h3&gt;

&lt;p&gt;If you are using StorageLoader, you need to update to the latest version, which is 0.0.3 - this is available by checking out the master branch of the &lt;a href='https://github.com/snowplow/snowplow'&gt;SnowPlow repository&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If you are using Infobright Community Edition, you will need to update your table definition. This is because the &lt;code&gt;user_id&lt;/code&gt; field was not wide enough to store the new user IDs (UUIDs) set by the Clojure collector. To make this easier for you, we have created a script:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;4-storage/infobright-storage/migrate_to_005.sh&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Running this script will create a new table, &lt;code&gt;events_005&lt;/code&gt; (version 0.0.5 of the table definition) in your &lt;code&gt;snowplow&lt;/code&gt; database, copying across all your data from your existing &lt;code&gt;events_004&lt;/code&gt; table, which will not be modified in any way.&lt;/p&gt;

&lt;p&gt;Once you have run this, don&amp;#8217;t forget to update your StorageLoader&amp;#8217;s &lt;code&gt;config.yml&lt;/code&gt; to load into the new &lt;code&gt;events_005&lt;/code&gt; table, not your old &lt;code&gt;events_004&lt;/code&gt; table:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:storage:
  # ...
  :table:    events_005 # NOT &amp;quot;events_004&amp;quot; any more&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That&amp;#8217;s it! Your Clojure collector should be ready to run now. However, please read on for an important note about its experimental nature.&lt;/p&gt;

&lt;h2 id='warning_experimental'&gt;Warning: Experimental!&lt;/h2&gt;

&lt;p&gt;We want to stress that the new Clojure-based collector is a piece of experimental technology - we are looking to the community to try it out and feedback to us on how it&amp;#8217;s working for you, especially at scale.&lt;/p&gt;

&lt;p&gt;In particular, we would recommend running the Clojure-based collector alongside the CloudFront collector to be confident that it is performing under load and that no events are being dropped. We have run both collectors alongside each other for the &lt;a href='http://snowplowanalytics.com'&gt;SnowPlow Analytics&lt;/a&gt; website for four complete days, and total event counts are as follows:&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Date&lt;/th&gt;&lt;th&gt;CloudFront&lt;/th&gt;&lt;th&gt;Clojure&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;2013-01-02&lt;/td&gt;&lt;td style='text-align: left;'&gt;275&lt;/td&gt;&lt;td style='text-align: left;'&gt;274&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;2013-01-01&lt;/td&gt;&lt;td style='text-align: left;'&gt;116&lt;/td&gt;&lt;td style='text-align: left;'&gt;108&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;2012-12-31&lt;/td&gt;&lt;td style='text-align: left;'&gt;107&lt;/td&gt;&lt;td style='text-align: left;'&gt;109&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;2012-12-30&lt;/td&gt;&lt;td style='text-align: left;'&gt;142&lt;/td&gt;&lt;td style='text-align: left;'&gt;141&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;Overall for the result set, the absolute percentage difference between results for the Cloudfront and Clojure collectors is less than 2% (1.9%). Possible reasons for this discrepancy include:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Differences in datestamps - possibly an event fell on either side of a date boundary for each collector&lt;/li&gt;

&lt;li&gt;Duplicate rows - the two collectors may be occassionally duplicating different rows (see &lt;a href='https://github.com/snowplow/snowplow/issues/24'&gt;issue 24&lt;/a&gt;)&lt;/li&gt;

&lt;li&gt;Browsing behaviour - it may be that the user navigates away from the page before one or other collector can register the event&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We plan on testing all of this further with larger datasets; we also intend to explore the Clojure collector&amp;#8217;s duplicate rows to check there are no particular issues there.&lt;/p&gt;

&lt;h2 id='other_features_in_this_release'&gt;Other features in this release&lt;/h2&gt;

&lt;p&gt;There are two minor changes in this release not related to the Clojure-based collector:&lt;/p&gt;

&lt;p&gt;Both EmrEtlRunner and StorageLoader now print &amp;#8220;Completed successfully&amp;#8221; to &lt;code&gt;stdout&lt;/code&gt; on completion. This should help to make it clearer (e.g. in logs) that these Ruby programs have completed successfully.&lt;/p&gt;

&lt;p&gt;StorageLoader has been updated so that its &lt;code&gt;--skip&lt;/code&gt; argument works the same way as it does in EmrEtlRunner:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Specific options:
    ...
    -s, --skip download,load,archive   skip work step(s)&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id='getting_help'&gt;Getting help&lt;/h2&gt;

&lt;p&gt;That&amp;#8217;s it! If you have any problems with SnowPlow version 0.7.0, please &lt;a href='https://github.com/snowplow/snowplow/issues'&gt;raise an issue&lt;/a&gt; or get in touch with us via &lt;a href='https://github.com/snowplow/snowplow/wiki/Talk-to-us'&gt;the usual channels&lt;/a&gt;.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/01/02/referer-parser-ported-to-3-more-languages</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/01/02/referer-parser-ported-to-3-more-languages"/>
    <title>referer-parser now with Java, Scala and Python support</title>
    <updated>2013-01-02T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;Happy New Year all! It&amp;#8217;s been three months since we &lt;a href='/blog/2012/10/11/attlib-0.0.1-released/'&gt;introduced our Attlib project&lt;/a&gt;, now renamed to &lt;a href='https://github.com/snowplow/referer-parser'&gt;referer-parser&lt;/a&gt;, and we are pleased to announce that referer-parser is now available in three additional languages: Java, Scala and Python.&lt;/p&gt;

&lt;p&gt;To recap: referer-parser is a simple library for extracting seach marketing attribution data from referer &lt;em&gt;(sic)&lt;/em&gt; URLs. You supply referer-parser with a referer URL; it then tells you whether the URL is from a search engine - and if so, which search engine it is, and what keywords the user supplied to arrive at your page.&lt;/p&gt;

&lt;p&gt;Huge thanks to &lt;a href='https://github.com/donspaulding'&gt;Don Spaulding&lt;/a&gt; @ &lt;a href='http://mirusresearch.com/'&gt;Mirus Research&lt;/a&gt; for contributing the &lt;a href='https://github.com/snowplow/referer-parser/tree/master/python'&gt;Python port&lt;/a&gt; of referer-parser; the &lt;a href='https://github.com/snowplow/referer-parser/tree/master/java-scala'&gt;Java/Scala port&lt;/a&gt; was developed by us in-house and it will be a key addition to our &lt;a href='https://github.com/snowplow/snowplow/wiki/etl'&gt;SnowPlow ETL&lt;/a&gt; process in the coming months.&lt;/p&gt;

&lt;p&gt;You can checkout the code on GitHub, in the &lt;a href='https://github.com/snowplow/referer-parser'&gt;referer-parser repository&lt;/a&gt;, or read on below the fold for some code examples in the new languages:&lt;/p&gt;
&lt;!--more--&gt;
&lt;h2 id='python'&gt;Python&lt;/h2&gt;

&lt;p&gt;To use referer-parser from a Python script:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='python'&gt;&lt;span class='kn'&gt;from&lt;/span&gt; &lt;span class='nn'&gt;referer_parser&lt;/span&gt; &lt;span class='kn'&gt;import&lt;/span&gt; &lt;span class='n'&gt;Referer&lt;/span&gt;

&lt;span class='n'&gt;referer_url&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='s'&gt;&amp;#39;http://www.google.com/search?q=gateway+oracle+cards+denise+linn&amp;amp;hl=en&amp;amp;client=safari&amp;#39;&lt;/span&gt;

&lt;span class='n'&gt;r&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='n'&gt;Referer&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;referer_url&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt;

&lt;span class='k'&gt;print&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;r&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;known&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt;              &lt;span class='c'&gt;# True&lt;/span&gt;
&lt;span class='k'&gt;print&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;r&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;referer&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt;            &lt;span class='c'&gt;# &amp;#39;Google&amp;#39;&lt;/span&gt;
&lt;span class='k'&gt;print&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;r&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;search_parameter&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt;   &lt;span class='c'&gt;# &amp;#39;q&amp;#39;     &lt;/span&gt;
&lt;span class='k'&gt;print&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;r&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;search_term&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt;        &lt;span class='c'&gt;# &amp;#39;gateway oracle cards denise linn&amp;#39;&lt;/span&gt;
&lt;span class='k'&gt;print&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;r&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;uri&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt;                &lt;span class='c'&gt;# ParseResult(scheme=&amp;#39;http&amp;#39;, netloc=&amp;#39;www.google.com&amp;#39;, path=&amp;#39;/search&amp;#39;, params=&amp;#39;&amp;#39;, query=&amp;#39;q=gateway+oracle+cards+denise+linn&amp;amp;hl=en&amp;amp;client=safari&amp;#39;, fragment=&amp;#39;&amp;#39;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;For more information, please see the Python &lt;a href='https://github.com/snowplow/referer-parser/blob/master/python/README.md'&gt;README&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id='scala'&gt;Scala&lt;/h2&gt;

&lt;p&gt;To use referer-parser from a Scala app:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='scala'&gt;&lt;span class='k'&gt;val&lt;/span&gt; &lt;span class='n'&gt;refererUrl&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='s'&gt;&amp;quot;http://www.google.com/search?q=gateway+oracle+cards+denise+linn&amp;amp;hl=en&amp;amp;client=safari&amp;quot;&lt;/span&gt;

&lt;span class='k'&gt;import&lt;/span&gt; &lt;span class='nn'&gt;com.snowplowanalytics.refererparser.scala.Parser&lt;/span&gt;
&lt;span class='k'&gt;for&lt;/span&gt; &lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;r&lt;/span&gt; &lt;span class='k'&gt;&amp;lt;-&lt;/span&gt; &lt;span class='nc'&gt;Parser&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;parse&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;refererUrl&lt;/span&gt;&lt;span class='o'&gt;))&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
  &lt;span class='n'&gt;println&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;r&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;referer&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;name&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;      &lt;span class='c1'&gt;// =&amp;gt; &amp;quot;Google&amp;quot;&lt;/span&gt;
  &lt;span class='k'&gt;for&lt;/span&gt; &lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;s&lt;/span&gt; &lt;span class='k'&gt;&amp;lt;-&lt;/span&gt; &lt;span class='n'&gt;r&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;search&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
    &lt;span class='n'&gt;println&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;s&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;term&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;            &lt;span class='c1'&gt;// =&amp;gt; &amp;quot;gateway oracle cards denise linn&amp;quot;&lt;/span&gt;
    &lt;span class='n'&gt;println&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;s&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;parameter&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;       &lt;span class='c1'&gt;// =&amp;gt; &amp;quot;q&amp;quot;    &lt;/span&gt;
  &lt;span class='o'&gt;}&lt;/span&gt;
&lt;span class='o'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;For more information, please see the Java/Scala &lt;a href='https://github.com/snowplow/referer-parser/blob/master/java-scala/README.md'&gt;README&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id='usage_java'&gt;Usage: Java&lt;/h2&gt;

&lt;p&gt;To use referer-parser from a Java program:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='java'&gt;&lt;span class='kn'&gt;import&lt;/span&gt; &lt;span class='nn'&gt;com.snowplowanalytics.refererparser.Parser&lt;/span&gt;&lt;span class='o'&gt;;&lt;/span&gt;

&lt;span class='o'&gt;...&lt;/span&gt;

  &lt;span class='n'&gt;String&lt;/span&gt; &lt;span class='n'&gt;refererUrl&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='s'&gt;&amp;quot;http://www.google.com/search?q=gateway+oracle+cards+denise+linn&amp;amp;hl=en&amp;amp;client=safari&amp;quot;&lt;/span&gt;&lt;span class='o'&gt;;&lt;/span&gt;

  &lt;span class='n'&gt;Parser&lt;/span&gt; &lt;span class='n'&gt;refererParser&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='k'&gt;new&lt;/span&gt; &lt;span class='n'&gt;Parser&lt;/span&gt;&lt;span class='o'&gt;();&lt;/span&gt;
  &lt;span class='n'&gt;Referal&lt;/span&gt; &lt;span class='n'&gt;r&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='n'&gt;refererParser&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='na'&gt;parse&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;refererUrl&lt;/span&gt;&lt;span class='o'&gt;);&lt;/span&gt;

  &lt;span class='n'&gt;System&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='na'&gt;out&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='na'&gt;println&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;r&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='na'&gt;referer&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='na'&gt;name&lt;/span&gt;&lt;span class='o'&gt;);&lt;/span&gt;       &lt;span class='c1'&gt;// =&amp;gt; &amp;quot;Google&amp;quot;&lt;/span&gt;
  &lt;span class='n'&gt;System&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='na'&gt;out&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='na'&gt;println&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;r&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='na'&gt;search&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='na'&gt;parameter&lt;/span&gt;&lt;span class='o'&gt;);&lt;/span&gt;   &lt;span class='c1'&gt;// =&amp;gt; &amp;quot;q&amp;quot;    &lt;/span&gt;
  &lt;span class='n'&gt;System&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='na'&gt;out&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='na'&gt;println&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;r&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='na'&gt;search&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='na'&gt;term&lt;/span&gt;&lt;span class='o'&gt;);&lt;/span&gt;        &lt;span class='c1'&gt;// =&amp;gt; &amp;quot;gateway oracle cards denise linn&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;For more information, please see the Java/Scala &lt;a href='https://github.com/snowplow/referer-parser/blob/master/java-scala/README.md'&gt;README&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id='getting_help'&gt;Getting help&lt;/h2&gt;

&lt;p&gt;That&amp;#8217;s it! If you have any problems with the new versions of referer-parser, please &lt;a href='https://github.com/snowplow/referer-parser/issues'&gt;raise an issue&lt;/a&gt; or get in touch with us via &lt;a href='https://github.com/snowplow/snowplow/wiki/Talk-to-us'&gt;the usual channels&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;And do let us know if you find referer-parser useful!&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/12/26/snowplow-0.6.5-released</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/12/26/snowplow-0.6.5-released"/>
    <title>SnowPlow 0.6.5 released, with improved event tracking</title>
    <updated>2012-12-26T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We&amp;#8217;re excited to announce our next SnowPlow release - version &lt;strong&gt;0.6.5&lt;/strong&gt;, a Boxing Day release for SnowPlow!&lt;/p&gt;

&lt;p&gt;This is a big release for us, as it introduces the idea of &lt;strong&gt;event types&lt;/strong&gt; - every event sent by the JavaScript tracker to the collector now has an &lt;code&gt;event&lt;/code&gt; field which specifies what type of event it is. This should be really helpful for a couple of things:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;It should make querying SnowPlow events much easier&lt;/li&gt;

&lt;li&gt;It should make SnowPlow event data a better fit for JSON-oriented datastores such as MongoDB and Riak&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;As well as event types, in this release we are also introducing &lt;strong&gt;event IDs&lt;/strong&gt;. With this, the ETL phase adds an &lt;code&gt;event_id&lt;/code&gt; UUID (universally unique ID) to each event row, which should help with subsequent querying.&lt;/p&gt;

&lt;p&gt;Here is a taster of how SnowPlow event data looks with the new event types and event IDs:&lt;/p&gt;

&lt;p&gt;&lt;img alt='events-screenshot' src='/static/img/blog/2012/event_and_event_id_fields.png' /&gt;&lt;/p&gt;

&lt;p&gt;These are not the only improvements in this version - here are the rest:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;We have cleaned up the code for on-page activity tracking (&amp;#8220;page pings&amp;#8221;)&lt;/li&gt;

&lt;li&gt;We have fixed a bug that affected ad impression tracking - thanks &lt;a href='https://github.com/talkspoon'&gt;Alan Z&lt;/a&gt;!&lt;/li&gt;

&lt;li&gt;The ETL no longer dies if a raw event has a corrupted querystring (e.g. the &lt;code&gt;&amp;amp;refr=&lt;/code&gt; parameter was not escaped)&lt;/li&gt;

&lt;li&gt;The JavaScript tracker&amp;#8217;s build script, &lt;code&gt;snowpak.sh&lt;/code&gt;, now has a combine-only option (no minification), which is helpful for testing purposes&lt;/li&gt;

&lt;li&gt;The JavaScript tracker has a new method, &lt;code&gt;attachUserId(boolean)&lt;/code&gt;, which can be used to stop the tracker sending a &lt;code&gt;&amp;amp;uid=xxx&lt;/code&gt; parameter&lt;/li&gt;

&lt;li&gt;We have added the ability to override the IP address by passing in an &lt;code&gt;&amp;amp;ip=&lt;/code&gt; parameter on the querystring&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Below, we first explain how to upgrade, before taking a brief tour through these updates:&lt;/p&gt;
&lt;!--more--&gt;
&lt;h2 id='0_upgrading'&gt;0. Upgrading&lt;/h2&gt;

&lt;p&gt;Upgrading is a two-step process:&lt;/p&gt;

&lt;h3 id='javascript_tracker'&gt;JavaScript tracker&lt;/h3&gt;

&lt;p&gt;Please update your website(s) to use the latest version of the JavaScript tracker, which is version 0.9.0. As always, the updated minified tracker is available here:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http(s)://d1fc8wv8zag5ca.cloudfront.net/0.9.0/sp.js&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id='etl'&gt;ETL&lt;/h3&gt;

&lt;p&gt;If you are using EmrEtlRunner, you need to update your configuration file, &lt;code&gt;config.yml&lt;/code&gt;, to use the latest versions of the Hive serde and HiveQL scripts:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:snowplow:
  :serde_version: 0.5.3
  :hive_hiveql_version: 0.5.3
  :non_hive_hiveql_version: 0.0.4&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That&amp;#8217;s it! You don&amp;#8217;t need to make any changes to your Infobright setup, assuming you are up-to-date with previous releases.&lt;/p&gt;

&lt;h2 id='1_event_types'&gt;1. Event types&lt;/h2&gt;

&lt;p&gt;To recap, every event sent by the JavaScript tracker now has an &lt;code&gt;event&lt;/code&gt; field which specifies what type of event it is. Currently we have six different types of events, which are set out in the table below:&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Type of event&lt;/th&gt;&lt;th&gt;JavaScript tracker function&lt;/th&gt;&lt;th&gt;Value of SnowPlow &lt;code&gt;event&lt;/code&gt; field&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Page view&lt;/td&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;trackPageView()&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;page_view&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Page ping&lt;/td&gt;&lt;td style='text-align: left;'&gt;None (automatic)*&lt;/td&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;page_ping&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Custom event&lt;/td&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;trackEvent()&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;custom&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Ad impression&lt;/td&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;trackImpression()&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;ad_impression&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Transaction&lt;/td&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;addTrans&lt;/code&gt; &amp;amp; &lt;code&gt;trackTrans()&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;transaction&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Transaction item&lt;/td&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;addItem&lt;/code&gt; &amp;amp; &lt;code&gt;trackTrans()&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;transaction_item&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;&lt;em&gt;* for more information on on-page activity tracking, please see the relevant section later in this blog post.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;This new event field should make it much easier to query SnowPlow data by the type of event. For example, to retrieve the number of e-commerce transactions per day:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='sql'&gt;&lt;span class='k'&gt;SELECT&lt;/span&gt;
&lt;span class='n'&gt;dt&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='k'&gt;COUNT&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;event_id&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt;
&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='n'&gt;events&lt;/span&gt;
&lt;span class='k'&gt;WHERE&lt;/span&gt; &lt;span class='n'&gt;event&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;transaction&amp;#39;&lt;/span&gt;
&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='n'&gt;dt&lt;/span&gt; &lt;span class='p'&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;We will be updating our &lt;a href='/analytics/index.html'&gt;Analytics Cookbook&lt;/a&gt; to use the &lt;code&gt;event&lt;/code&gt; field to simplify queries where possible.&lt;/p&gt;

&lt;h2 id='2_event_ids'&gt;2. Event IDs&lt;/h2&gt;

&lt;p&gt;As stated above, the SnowPlow ETL now attaches a unique ID to each event - specifically a &lt;a href='http://en.wikipedia.org/wiki/Universally_unique_identifier#Version_4_.28random.29'&gt;type 4 UUID&lt;/a&gt;. This new &lt;code&gt;event_id&lt;/code&gt; is much more unique than the existing &lt;code&gt;txn_id&lt;/code&gt; field, which is a short random number set in the JavaScript tracker (&lt;code&gt;txn_id&lt;/code&gt; is currently unused, but we may eventually use it to check for duplicate events &lt;code&gt;txn_id&lt;/code&gt; introduced prior to the ETL, see &lt;a href='https://github.com/snowplow/snowplow/issues/24'&gt;issue 24&lt;/a&gt; for more details).&lt;/p&gt;

&lt;p&gt;You can use the new &lt;code&gt;event_id&lt;/code&gt; field to uniquely identify individual events in your event store, and of course to count distinct events. For example, to count the number of page views by day, we simply execute the following query:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='sql'&gt;&lt;span class='k'&gt;SELECT&lt;/span&gt;
&lt;span class='n'&gt;dt&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='k'&gt;COUNT&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;event_id&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt;
&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='n'&gt;events&lt;/span&gt;
&lt;span class='k'&gt;WHERE&lt;/span&gt; &lt;span class='n'&gt;event&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;page_view&amp;#39;&lt;/span&gt;
&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='n'&gt;dt&lt;/span&gt; &lt;span class='p'&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;We will be updating our &lt;a href='/analytics/index.html'&gt;Analytics Cookbook&lt;/a&gt; to use &lt;code&gt;event_id&lt;/code&gt; in any examples which currently (erroneously) use &lt;code&gt;txn_id&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id='3_onpage_activity_tracking'&gt;3. On-page activity tracking&lt;/h2&gt;

&lt;p&gt;In this release of the JavaScript tracker we have deprecated the old (undocumented) &lt;code&gt;setHeartBeatTimer()&lt;/code&gt; inherited from &lt;code&gt;piwik.js&lt;/code&gt;, and introduced a new function, &lt;code&gt;enableActivityTracking(minimumVisitLength, heartBeatDelay)&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;With activity tracking enabled, &amp;#8220;page pings&amp;#8221; are sent to SnowPlow every &lt;code&gt;heartBeatDelay&lt;/code&gt; seconds, as long as the visitor remains active (moving the mouse, clicking etc) on the page. Page pings are not sent until the &lt;code&gt;minimumVisitLength&lt;/code&gt; seconds have elapsed.&lt;/p&gt;

&lt;p&gt;Here is an example configuration:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='javascript'&gt;&lt;span class='nx'&gt;_snaq&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='nx'&gt;push&lt;/span&gt;&lt;span class='p'&gt;([&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;enableActivityTracking&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='mi'&gt;10&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='mi'&gt;10&lt;/span&gt;&lt;span class='p'&gt;]);&lt;/span&gt; &lt;span class='c1'&gt;// Ping every 10 seconds after 10 seconds&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;This is still an experimental feature - but it should provide some interesting data to start to explore page residency, true bounce rates and so on.&lt;/p&gt;

&lt;p&gt;Please note that enabling activity tracking can &lt;strong&gt;significantly&lt;/strong&gt; increase the number of SnowPlow events generated, especially with a short &lt;code&gt;heartBeatDelay&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id='4_and_the_rest'&gt;4. And the rest&lt;/h2&gt;

&lt;p&gt;The rest of the changes in this release are much smaller, being either bug fixes or small preparatory features for future releases:&lt;/p&gt;

&lt;h3 id='ad_impression_tracking_bug_fix'&gt;Ad impression tracking bug fix&lt;/h3&gt;

&lt;p&gt;Many thanks to &lt;a href='https://github.com/talkspoon'&gt;Alan Z&lt;/a&gt; @ &lt;a href='http://www.verycd.com'&gt;VeryCD&lt;/a&gt; for spotting a bug in the &lt;code&gt;trackImpression()&lt;/code&gt; method, which was stopping ad impressions from being logged. This is now fixed.&lt;/p&gt;

&lt;h3 id='etl_resilient_against_corrupted_querystrings'&gt;ETL resilient against corrupted querystrings&lt;/h3&gt;

&lt;p&gt;We had a problem with two historic versions of the JavaScript tracker, 0.8.0 and 0.8.1, where querystrings were being transmitted to the SnowPlow collector unescaped. These &amp;#8220;corrupted&amp;#8221; querystrings caused the ETL process to error and die.&lt;/p&gt;

&lt;p&gt;We have updated the ETL process so that events with corrupted querystrings can be processed without error: these rows are stored as SnowPlow events, but of course with most of the standard fields empty.&lt;/p&gt;

&lt;h3 id='snowpaksh_combineonly_option'&gt;snowpak.sh combine-only option&lt;/h3&gt;

&lt;p&gt;To make it easier to hack on the JavaScript tracker, we have updated the build script, &lt;code&gt;snowpak.sh&lt;/code&gt;, so that it has a &amp;#8220;combine-only&amp;#8221; option. If you set the combine-only flag on the command line, then the minification step will &lt;strong&gt;not&lt;/strong&gt; be run, and debug code will left in. This is helpful for local testing when you want to debug the JavaScript in its pre-minified form.&lt;/p&gt;

&lt;p&gt;Here are the updated usage options for the &lt;code&gt;snowpak.sh&lt;/code&gt; script:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Usage: ./snowpak.sh [options]

Specific options:
  -y PATH             path to YUICompressor 2.4.2 *
  -c                  combine only (no minification or removing debug)

* or set env variable YUI_COMPRESSOR_PATH instead

Common options:
  -h                  Show this message&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id='attachuseridboolean'&gt;attachUserId(boolean)&lt;/h3&gt;

&lt;p&gt;The JavaScript tracker has a new method, &lt;code&gt;attachUserId(boolean)&lt;/code&gt;, which can be used to stop the tracker from sending a &lt;code&gt;&amp;amp;uid=xxx&lt;/code&gt; parameter. By default, the JavaScript tracker sends the user ID to the collector via this &lt;code&gt;uid&lt;/code&gt; parameter; you can now disable this by calling &lt;code&gt;attachUserId()&lt;/code&gt; like so:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='javascript'&gt;&lt;span class='nx'&gt;_snaq&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='nx'&gt;push&lt;/span&gt;&lt;span class='p'&gt;([&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;attachUserId&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='kc'&gt;false&lt;/span&gt;&lt;span class='p'&gt;]);&lt;/span&gt; &lt;span class='c1'&gt;// Don&amp;#39;t attach &amp;amp;uid=xxx to the querystring&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;This function is not of immediate use - but it will be an important part of the setup for using the new Clojure Collector, which we are currently working on.&lt;/p&gt;

&lt;h3 id='ip_address_override'&gt;IP address override&lt;/h3&gt;

&lt;p&gt;Finally, we have added the ability to override the IP address by passing in an &lt;code&gt;ip=&lt;/code&gt; parameter on the querystring.&lt;/p&gt;

&lt;p&gt;This one is a little confusing, as there is no capability in the JavaScript tracker to attach an IP address to the querystring (because JavaScript cannot know the user&amp;#8217;s IP address). Rather, we have added this IP address override ability to cater for future server-side trackers and collectors which &lt;em&gt;do&lt;/em&gt; know the user&amp;#8217;s IP address and want to append it to the querystring themselves.&lt;/p&gt;

&lt;h2 id='getting_help'&gt;Getting help&lt;/h2&gt;

&lt;p&gt;That&amp;#8217;s it! If you have any problems with SnowPlow version 0.6.5, please &lt;a href='https://github.com/snowplow/snowplow/issues'&gt;raise an issue&lt;/a&gt; or get in touch with us via &lt;a href='https://github.com/snowplow/snowplow/wiki/Talk-to-us'&gt;the usual channels&lt;/a&gt;.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/12/20/snowplow-0.6.4-released</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/12/20/snowplow-0.6.4-released"/>
    <title>SnowPlow 0.6.4 released, with Infobright improvements</title>
    <updated>2012-12-20T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We&amp;#8217;re happy to announce our next SnowPlow release - version &lt;strong&gt;0.6.4&lt;/strong&gt;. This release includes updates:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;An upgraded Infobright table definition which scales to millions of pageviews easily&lt;/li&gt;

&lt;li&gt;Clarified Hive table definitions&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Before we start - a big thanks to the community members who helped out on this release:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href='https://github.com/moncaubeig'&gt;Gilles Moncaubeig&lt;/a&gt; @ &lt;a href='http://en.overblog.com/'&gt;OverBlog&lt;/a&gt; worked closely with us on the updated Infobright table definition&lt;/li&gt;

&lt;li&gt;&lt;a href='https://github.com/mmoulton'&gt;Mike Moulton&lt;/a&gt; @ &lt;a href='http://meltmedia.com/'&gt;meltmedia&lt;/a&gt; for flagging the missing Hive table definition&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We&amp;#8217;ll take a look at both updates below:&lt;/p&gt;
&lt;!--more--&gt;
&lt;h2 id='upgraded_infobright_table_definition'&gt;Upgraded Infobright table definition&lt;/h2&gt;

&lt;p&gt;With help from &lt;a href='https://github.com/moncaubeig'&gt;Gilles Moncaubeig&lt;/a&gt; we have upgraded the Infobright table definition so that it can easily scale to loading millions of new SnowPlow events per day. It also supports much longer &lt;code&gt;br_lang&lt;/code&gt; and &lt;code&gt;page_url&lt;/code&gt; fields, which should prevent you from occasional load errors.&lt;/p&gt;

&lt;p&gt;If you are using Infobright Community Edition for analysis, you will need to update your table definition. This is a little complex, because Infobright does not support in-place table or column renames. To make this easier for you, we have created a script:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;4-storage/infobright-storage/migrate_to_004.sh&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Running this script will create a new table, &lt;code&gt;events_004&lt;/code&gt; (version 0.0.4 of the table definition) in your &lt;code&gt;snowplow&lt;/code&gt; database, copying across all your data from your existing &lt;code&gt;events&lt;/code&gt; table, which will not be modified in any way.&lt;/p&gt;

&lt;p&gt;Once you have run this, don&amp;#8217;t forget to update your StorageLoader&amp;#8217;s &lt;code&gt;config.yml&lt;/code&gt; to load into the new &lt;code&gt;events_004&lt;/code&gt; table, not your old &lt;code&gt;events&lt;/code&gt; table:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:storage:
  :type: infobright
  :database: snowplow
  :table:    events_004 # NOT &amp;quot;events_003&amp;quot; any more&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Done!&lt;/p&gt;

&lt;h2 id='clarified_hive_table_definitions'&gt;Clarified Hive table definitions&lt;/h2&gt;

&lt;p&gt;We have clarified the two different Hive table definitions, available in this folder:&lt;/p&gt;

&lt;p&gt;4-storage/hive-storage&lt;/p&gt;

&lt;p&gt;Which format your SnowPlow event files are in will depend on how your EmrEtlRunner is configured. If your &lt;code&gt;config.yml&lt;/code&gt; contains:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:storage_format: non-hive&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;then your SnowPlow events will be stored in the format shown in &lt;a href='https://github.com/snowplow/snowplow/blob/master/4-storage/hive-storage/non-hive-format-table-def.q'&gt;&lt;code&gt;non-hive-format-table-def.q&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Whereas if your &lt;code&gt;config.yml&lt;/code&gt; contains:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:storage_format: non-hive&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;then your SnowPlow events will be stored in the format shown in &lt;a href='https://github.com/snowplow/snowplow/blob/master/4-storage/hive-storage/hive-format-table-def.q'&gt;&lt;code&gt;hive-format-table-def.q&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id='getting_help'&gt;Getting help&lt;/h2&gt;

&lt;p&gt;That&amp;#8217;s it! If you have any problems with SnowPlow version 0.6.4, please &lt;a href='https://github.com/snowplow/snowplow/issues'&gt;raise an issue&lt;/a&gt; or get in touch with us via &lt;a href='https://github.com/snowplow/snowplow/wiki/Talk-to-us'&gt;the usual channels&lt;/a&gt;.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/12/18/snowplow-0.6.3-released</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/12/18/snowplow-0.6.3-released"/>
    <title>SnowPlow 0.6.3 released, with JavaScript and HiveQL bug fixes</title>
    <updated>2012-12-18T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;Today we are releasing SnowPlow version &lt;strong&gt;0.6.3&lt;/strong&gt; - another clean-up release following on from the 0.6.2 release. This release bumps the JavaScript Tracker to version 0.8.2, and the Hive-data-format HiveQL file to version 0.5.2.&lt;/p&gt;

&lt;p&gt;Many thanks to the community members who contributed bug fixes to this release: &lt;a href='https://github.com/mmoulton'&gt;Mike Moulton&lt;/a&gt; @ &lt;a href='http://meltmedia.com/'&gt;meltmedia&lt;/a&gt;, &lt;a href='https://github.com/ramn'&gt;Simon Andersson&lt;/a&gt; @ &lt;a href='http://www.qwaya.com'&gt;Qwaya&lt;/a&gt; and &lt;a href='https://github.com/mtibben'&gt;Michael Tibben&lt;/a&gt; @ &lt;a href='http://99designs.com'&gt;99designs&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We&amp;#8217;ll take a look at both fixes below:&lt;/p&gt;
&lt;!--more--&gt;
&lt;h2 id='javascript_tracker_fixes'&gt;JavaScript tracker fixes&lt;/h2&gt;

&lt;p&gt;This release fixes the issues in the JavaScript tracker raised in &lt;a href='https://github.com/snowplow/snowplow/issues/103'&gt;issue 103&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;These issues stemmed from the splitting of the JavaScript into multiple files in SnowPlow version 0.6.1 (JavaScript tracker version 0.8.0). We do not believe these bugs affected SnowPlow data collection, but they are well worth fixing.&lt;/p&gt;

&lt;p&gt;Many thanks to &lt;a href='https://github.com/mtibben'&gt;Michael Tibben&lt;/a&gt; and &lt;a href='https://github.com/ramn'&gt;Simon Andersson&lt;/a&gt; for their help in squashing these bugs!&lt;/p&gt;

&lt;p&gt;With these fixes we have bumped the JavaScript Tracker to version 0.8.2; the updated minified tracker is available as always here:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http(s)://d1fc8wv8zag5ca.cloudfront.net/0.8.2/sp.js&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id='hiveql_script_fix'&gt;HiveQL script fix&lt;/h2&gt;

&lt;p&gt;This release also fixes a bug (&lt;a href='https://github.com/snowplow/snowplow/pull/112'&gt;issue 112&lt;/a&gt;) in the HiveQL file used to generate the &lt;strong&gt;Hive-format&lt;/strong&gt; SnowPlow event files.&lt;/p&gt;

&lt;p&gt;This bug prevented the ETL from running if you were using EmrEtlRunner to generate Hive-format SnowPlow event files - in other words if you had set:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:storage_format: non-hive&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;in your EmrEtlRunner&amp;#8217;s &lt;code&gt;config.yml&lt;/code&gt; file.&lt;/p&gt;

&lt;p&gt;Many thanks to &lt;a href='https://github.com/mmoulton'&gt;Mike Moulton&lt;/a&gt; for spotting and fixing this one!&lt;/p&gt;

&lt;p&gt;With this fix we have bumped the Hive-format HiveQL file to version 0.5.2. To start using the new file, all you need to do is update your EmrEtlRunner&amp;#8217;s &lt;code&gt;config.yml&lt;/code&gt; file by changing:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:hive_hiveql_version: 0.5.1&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;to:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:hive_hiveql_version: 0.5.2&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id='getting_help'&gt;Getting help&lt;/h2&gt;

&lt;p&gt;If you have any problems with SnowPlow version 0.6.3, please &lt;a href='https://github.com/snowplow/snowplow/issues'&gt;raise an issue&lt;/a&gt; or get in touch with us via &lt;a href='https://github.com/snowplow/snowplow/wiki/Talk-to-us'&gt;the usual channels&lt;/a&gt;.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/12/17/transforming-snowplow-data-so-it-can-be-interrogated-by-olap-tools-like-tableau</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/12/17/transforming-snowplow-data-so-it-can-be-interrogated-by-olap-tools-like-tableau"/>
    <title>Transforming SnowPlow data so that it can be interrogataed in BI / OLAP tools like Tableau, Qlikview and Pentaho</title>
    <updated>2012-12-17T00:00:00+00:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;Because SnowPlow does not ship with any sort of user interface, we get many enquiries from current and prospective users who would like to interrogate SnowPlow data with popular BI tools like Tableau or Qlikview.&lt;/p&gt;
&lt;p style='text-align:center;'&gt;&lt;img width='250' alt='olap cube' src='/static/img/olap/example-cube-2.png' /&gt;&lt;/p&gt;
&lt;p&gt;Unfortunately, it is not possible to run a tool like Tableau directly on top of the SnowPlow events table. That is because these tools require the data to be in a particular format: one in which each line of data is made up of a combination of dimension and metrics fields, such that it is straightforward for the reporting tool to understand how to aggregate metrics by different combinations of dimensions. To give a very simple example of a data set that would play nicely with a reporting tool like Tableau:&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Country&lt;/th&gt;&lt;th&gt;Date&lt;/th&gt;&lt;th&gt;Product&lt;/th&gt;&lt;th&gt;Number Sold&lt;/th&gt;&lt;th&gt;Revenue&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;UK&lt;/td&gt;&lt;td style='text-align: left;'&gt;Sept 2012&lt;/td&gt;&lt;td style='text-align: left;'&gt;Hats&lt;/td&gt;&lt;td style='text-align: left;'&gt;137&lt;/td&gt;&lt;td style='text-align: right;'&gt;1779.63&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;UK&lt;/td&gt;&lt;td style='text-align: left;'&gt;Oct 2012&lt;/td&gt;&lt;td style='text-align: left;'&gt;Hats&lt;/td&gt;&lt;td style='text-align: left;'&gt;193&lt;/td&gt;&lt;td style='text-align: right;'&gt;2507.07&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;UK&lt;/td&gt;&lt;td style='text-align: left;'&gt;Oct 2012&lt;/td&gt;&lt;td style='text-align: left;'&gt;Shoes&lt;/td&gt;&lt;td style='text-align: left;'&gt;15&lt;/td&gt;&lt;td style='text-align: right;'&gt;1125.00&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;France&lt;/td&gt;&lt;td style='text-align: left;'&gt;Oct 2012&lt;/td&gt;&lt;td style='text-align: left;'&gt;Hats&lt;/td&gt;&lt;td style='text-align: left;'&gt;288&lt;/td&gt;&lt;td style='text-align: right;'&gt;2877.12&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&amp;#8230;&lt;/td&gt;&lt;td style='text-align: left;'&gt;&amp;#8230;&lt;/td&gt;&lt;td style='text-align: left;'&gt;&amp;#8230;&lt;/td&gt;&lt;td style='text-align: left;'&gt;&amp;#8230;&lt;/td&gt;&lt;td style='text-align: right;'&gt;&amp;#8230;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;Reporting tools like Tableau recognise that the two right hand columns (number sold and revenue) are metrics, and that analysts will want to examine how those metrics vary by country, time and product (all of which are dimensions). They will give analysts easy-to-use tools to enable them to slice, dice, drill up and drill down those metrics by different combinations of those dimensions. Enabling those operations is straightforward for the reporting tool, because it knows it knows if the analyst wants to report product sales in the UK over time, to filter all results by country (so only lines of data from the UK are included), and display sales of each type of product by month.&lt;/p&gt;

&lt;p&gt;This type of dimensional analysis is called &lt;a href='http://en.wikipedia.org/wiki/Online_analytical_processing'&gt;OLAP&lt;/a&gt;, and has a long and venerable history in business intelligence. Although the term &amp;#8216;OLAP&amp;#8217; is no longer fashionable, this type of analysis is still the predominant one used by anyone who relies on PivotTables in Excel or any mainstream BI tool including Tableau, Qlikview, Microstrategy, Pentaho etc.&lt;/p&gt;
&lt;!--more--&gt;
&lt;p&gt;SnowPlow data is in a log file format. Whilst each line does include metrics (e.g. revenue) and dimensions (e.g. browser features or operating system details), there are a large number of dimensions that we might want to slice and dice the data on that are not included in each line: these data points have to be inferred by reading across several lines of data. To give just one example: the source of traffic i.e. &lt;code&gt;mkt_source&lt;/code&gt; for a particular visit is only given on the &lt;strong&gt;first line&lt;/strong&gt; of data for that visit. Hence, in order to enable users to analyse page views, customer lifetime value and other metrics by different marketing sources (e.g. to do attribution analysis), we need to work out which source of traffic to attribute that visit to, and label every line of data associated with that visit with that source of traffic.&lt;/p&gt;

&lt;p&gt;So in order to use a BI tool like Tableau or Qlikview to interrogate SnowPlow data, you need to transform the data first. We&amp;#8217;ve documented how to perform the transformation in the &lt;a href='/analytics/tools-and-techniques/converting-snowplow-data-into-a-format-suitable-for-olap.html'&gt;analytics cookbook&lt;/a&gt;. We hope it provides a useful guide for anyone interested in interrogating or visualising SnowPlow data using BI tools.&lt;/p&gt;

&lt;p&gt;We also wonder whether the documentation will be of interest to the wider community of analysts and data scientists interested in using tools like Tableau to query log data. One of the things that surprised us, going in to this exercise, was the lack of material we could find on the internet that covered transforming log data so that it could be analysed using BI tools. We expected this to be well-covered territory: especially given the fact that the vast majority of data processed by Hadoop is log files, and nearly all the BI vendors are working hard to integrate their products with Hadoop. (For example, Tableau now integrates with MapR Hadoop distributions, so you can analyse Hive tables directly in Tableau.) Without the transformation step covered in our &lt;a href='/analytics/tools-and-techniques/converting-snowplow-data-into-a-format-suitable-for-olap.html'&gt;guide&lt;/a&gt; however, this type of integration is useless.&lt;/p&gt;

&lt;p&gt;If we&amp;#8217;ve missed any online literature on the topic, or if there are other people who&amp;#8217;ve looked at this particular problem and come up with different approaches - we&amp;#8217;d love to debate them here. Get in touch! For everyone else, we hope you find our &lt;a href='/analytics/tools-and-techniques/converting-snowplow-data-into-a-format-suitable-for-olap.html'&gt;guide&lt;/a&gt; helpful&amp;#8230;&lt;/p&gt;

&lt;p&gt;References:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;SnowPlow data &lt;a href='https://github.com/snowplow/snowplow/wiki/canonical-event-model'&gt;canonical data structure&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/11/28/snowplow-0.6.2-released</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/11/28/snowplow-0.6.2-released"/>
    <title>SnowPlow 0.6.2 released, with JavaScript tracker bug fixes</title>
    <updated>2012-11-28T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;Today we are releasing SnowPlow version &lt;strong&gt;0.6.2&lt;/strong&gt; - a clean-up release after yesterday&amp;#8217;s 0.6.1 release. This release bumps the JavaScript Tracker to version 0.8.1; the updated minified tracker is available as always here:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http(s)://d1fc8wv8zag5ca.cloudfront.net/0.8.1/sp.js&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This release fixes two bugs:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href='https://github.com/snowplow/snowplow/pull/101'&gt;Issue #101&lt;/a&gt; - we had left in a &lt;code&gt;console.log()&lt;/code&gt; in the production version, which should only have been printed in debug mode. Harmless but worth taking out. Many thanks to &lt;a href='https://github.com/mtibben'&gt;Michael Tibben&lt;/a&gt; @ &lt;a href='http://99designs.com'&gt;99designs&lt;/a&gt; for spotting this so quickly and fixing&lt;/li&gt;

&lt;li&gt;&lt;a href='https://github.com/snowplow/snowplow/issues/102'&gt;Issue #102&lt;/a&gt; - there was a trailing space in our initialization code, &lt;a href='https://github.com/snowplow/snowplow/blob/master/1-trackers/javascript-tracker/js/src/init.js'&gt;&lt;code&gt;init.js&lt;/code&gt;&lt;/a&gt;, which could cause some issues in Internet Explorer. Many thanks to community member Alan Z for raising&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The JavaScript Tracker&amp;#8217;s API and the Tracker Protocol are unchanged. We recommend upgrading to using the new JavaScript Tracker version 0.8.1 over the previous 0.8.0.&lt;/p&gt;

&lt;p&gt;Finally, if you haven&amp;#8217;t yet read our tutorial on &lt;a href='https://github.com/snowplow/snowplow/wiki/Integrating-javascript-tags-with-Google-Tag-Manager'&gt;Integrating the JavaScript Tracker with Google Tag Manager&lt;/a&gt;, we would recommend taking a look - it makes these sorts of upgrades much easier.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/11/27/snowplow-0.6.1-released</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/11/27/snowplow-0.6.1-released"/>
    <title>SnowPlow 0.6.1 released, with lots of small improvements</title>
    <updated>2012-11-27T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We&amp;#8217;re happy to announce our next SnowPlow release - version &lt;strong&gt;0.6.1&lt;/strong&gt;. This release includes updates:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Additional data collection&lt;/strong&gt;. The Javascript tracker has been updated to capture additional data points, including a user fingerprint (which can be used as a &lt;code&gt;user_id&lt;/code&gt; for companies tracking users across domains), the tracker version, browser timezone and color depth&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Javascript tracker updates&lt;/strong&gt;. A number of updates have been made to make the Javascript tracker more robust&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Updates to the ETL flow&lt;/strong&gt; so that the &lt;code&gt;user_agent&lt;/code&gt; string and &lt;code&gt;platform&lt;/code&gt; captured and stored in Hive / Infobright&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Improved EmrEtlRunner command line options&lt;/strong&gt; now provide more flexibility when writing your data to storage&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Bug fixes&lt;/strong&gt; related to loading SnowPlow data into Infobright&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Before we start - a big thanks to the community members who helped out on this release in a big way:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href='https://github.com/ramn'&gt;Simon Andersson&lt;/a&gt; @ &lt;a href='http://www.qwaya.com'&gt;Qwaya&lt;/a&gt; substantially re-factored the JavaScript tracker, splitting it into multiple smaller files, which made our work significantly easier :-)&lt;/li&gt;

&lt;li&gt;&lt;a href='https://github.com/moncaubeig'&gt;Gilles Moncaubeig&lt;/a&gt; @ &lt;a href='http://en.overblog.com/'&gt;OverBlog&lt;/a&gt; contributed the user fingerprinting code - thanks Gilles!&lt;/li&gt;

&lt;li&gt;&lt;a href='https://github.com/mtibben'&gt;Michael Tibben&lt;/a&gt; @ &lt;a href='http://99designs.com'&gt;99designs&lt;/a&gt; continued his great work on EmrEtlRunner with improved command line options&lt;/li&gt;
&lt;/ul&gt;
&lt;!--more--&gt;
&lt;h2 id='javascript_tracker_updates'&gt;JavaScript tracker updates&lt;/h2&gt;

&lt;p&gt;We have released a new version of the JavaScript tracker, &lt;strong&gt;0.8.0&lt;/strong&gt;. As always, we are hosting this new version on CloudFront if you don&amp;#8217;t want to host it yourself:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http(s)://d1fc8wv8zag5ca.cloudfront.net/0.8.0/sp.js&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But before you update your tags, we need to share a few important things about this new version, including a &lt;strong&gt;breaking change&lt;/strong&gt;:&lt;/p&gt;

&lt;h3 id='changes_to_the_javascript_api'&gt;Changes to the JavaScript API&lt;/h3&gt;

&lt;p&gt;Three main changes have been made to the JavaScript tracker&amp;#8217;s API - please note that the first is a &lt;strong&gt;breaking change&lt;/strong&gt;, while the other two deprecate some existing functions (and introduce new ones):&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The tracker now GETs &lt;code&gt;i&lt;/code&gt; not &lt;code&gt;ice.png&lt;/code&gt; &lt;strong&gt;(BREAKING CHANGE)&lt;/strong&gt; - version 0.8.0 of the JavaScript tracker now GETs a transparent 1x1 GIF called &lt;code&gt;i&lt;/code&gt;, no longer &lt;code&gt;ice.png&lt;/code&gt;. If you are using the CloudFront Collector, you &lt;strong&gt;must&lt;/strong&gt; upload our &lt;a href='https://github.com/snowplow/snowplow/blob/master/2-collectors/cloudfront-collector/static/i?raw=true'&gt;&lt;code&gt;i&lt;/code&gt;&lt;/a&gt; pixel to sit in your S3 bucket alongside &lt;code&gt;ice.png&lt;/code&gt;. Don&amp;#8217;t forget to make it publically readable. &lt;a href='https://github.com/shermozle/SnowCannon'&gt;SnowCannon&lt;/a&gt;, the node.js collector, already supports &lt;code&gt;i&lt;/code&gt;&lt;/li&gt;

&lt;li&gt;The &lt;code&gt;setAccount()&lt;/code&gt; function was badly named. We have added a new function, &lt;code&gt;setCollectorCf()&lt;/code&gt;, which does the exact same thing, and we will remove &lt;code&gt;setAccount()&lt;/code&gt; in a future version. If you continue to use &lt;code&gt;setAccount()&lt;/code&gt;, then a warning message will be printed to &lt;code&gt;console.log&lt;/code&gt;, but it will still work&lt;/li&gt;

&lt;li&gt;The &lt;code&gt;getTracker()&lt;/code&gt; function (which not many people need to use) was badly named. As wth point 2 above: we have added &lt;code&gt;getTrackerCf()&lt;/code&gt; and &lt;code&gt;getTrackerUrl()&lt;/code&gt;, and deprecated &lt;code&gt;getTracker()&lt;/code&gt; for now&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;As always, our JavaScript tracker&amp;#8217;s current API is fully documented on our Wiki, on the &lt;a href='https://github.com/snowplow/snowplow/wiki/javascript-tracker'&gt;JavaScript Tracker&lt;/a&gt; page.&lt;/p&gt;

&lt;h3 id='new_tracking_data'&gt;New tracking data&lt;/h3&gt;

&lt;p&gt;Version 0.8.0 of the JavaScript tracker now passes additional data along to the SnowPlow collector. This additional data is as follows:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Tracker version&lt;/strong&gt; - so you will always know which version of the JavaScript tracker generated a given SnowPlow event&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;User fingerprint&lt;/strong&gt; - we are still working on a new collector which supports cross-domain user tracking. In the meantime, we are releasing an experimental feature: a &amp;#8216;user fingerprint&amp;#8217; based on various (hopefully unique) attributes of the user&amp;#8217;s browser. Many thanks to &lt;a href='https://github.com/moncaubeig'&gt;Gilles&lt;/a&gt; for contributing this feature; to read more about this, please take a look at &lt;a href='https://github.com/snowplow/snowplow/issues/70'&gt;issue #70&lt;/a&gt; in GitHub&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Timezone&lt;/strong&gt; - tells you what timezone the user in, recording the &lt;a href='http://en.wikipedia.org/wiki/Tz_database'&gt;Olsen key&lt;/a&gt; for the user&amp;#8217;s timezone&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Color depth&lt;/strong&gt; - the bit depth of the browser&amp;#8217;s color palette for displaying images (in bits per pixel)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We have updated the ETL and storage systems (e.g. Hive and Infobright table definitions) to extract and store these new fields.&lt;/p&gt;

&lt;p&gt;We have updated the &lt;a href='https://github.com/snowplow/snowplow/wiki/snowplow-tracker-protocol'&gt;SnowPlow Tracker Protocol&lt;/a&gt; page on our Wiki with these additions.&lt;/p&gt;

&lt;h2 id='etl_and_storage_improvements'&gt;ETL and storage improvements&lt;/h2&gt;

&lt;p&gt;This release includes various improvements to SnowPlow ETL and storage which are unrelated to the JavaScript tracker changes above. To break these down:&lt;/p&gt;

&lt;h3 id='additional_data_being_saved'&gt;Additional data being saved&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Useragent&lt;/strong&gt; - the raw browser useragent is now being logged in a new &lt;code&gt;useragent&lt;/code&gt; field. Previously we were throwing this useful data away&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Platform&lt;/strong&gt; - the tracker&amp;#8217;s &lt;code&gt;platform&lt;/code&gt; type is now extracted and stored. The JavaScript tracker always sets this field to &lt;code&gt;web&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id='bug_fixes'&gt;Bug fixes&lt;/h3&gt;

&lt;p&gt;Just one bug fix - in the StorageLoader, we changed the field encloser for Infobright to &lt;code&gt;NULL&lt;/code&gt;, where previously it was &lt;code&gt;&amp;#39;&amp;#39;&lt;/code&gt; (two empty quotes). This was to fix &lt;a href='https://github.com/snowplow/snowplow/issues/88'&gt;issue #88&lt;/a&gt;, where Infobright was throwing an error and dying if a field&amp;#8217;s value started with a double-quote.&lt;/p&gt;

&lt;h3 id='improved_emretlrunner_commandline_options'&gt;Improved EmrEtlRunner command-line options&lt;/h3&gt;

&lt;p&gt;EmrEtlRunner now has some improved command-line options:&lt;/p&gt;

&lt;p&gt;Firstly, the &lt;code&gt;--skip&lt;/code&gt; argument now can take a list of individual steps to skip. So for example you could run &lt;strong&gt;only&lt;/strong&gt; the Hive job with the command-line option:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ bundle exec snowplow-emr-etl-runner --skip staging,archive --config ./config.yml &lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Secondly, there is now a new option, &lt;code&gt;--process-bucket&lt;/code&gt;. This runs the Hive job only on the contents of the specified bucket. This implies &lt;code&gt;--skip staging,archive&lt;/code&gt; as well. An example of usage:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ bundle exec snowplow-emr-etl-runner --process-bucket s3n://my-logs-to-process --config ./config.yml&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Many thanks to &lt;a href='https://github.com/mtibben'&gt;Mike Tibben&lt;/a&gt; for contributing these new options!&lt;/p&gt;

&lt;h3 id='placeholders_for_event_and_event_id'&gt;Placeholders for event and event_id&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Event&lt;/strong&gt; - we have renamed the &lt;code&gt;event_name&lt;/code&gt; field in Infobright to simply &lt;code&gt;event&lt;/code&gt;. This is still a placeholder (it will be populated in a future version of SnowPlow)&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Event ID&lt;/strong&gt; - there has been some confusion over the uniqueness of the current &lt;code&gt;txn_id&lt;/code&gt; field - see &lt;a href='https://github.com/snowplow/snowplow/issues/89'&gt;issue #89&lt;/a&gt; for the discussion. We plan on adding a properly unique &lt;code&gt;event_id&lt;/code&gt; for each event in the ETL layer; in the meantime we have added the &lt;code&gt;event_id&lt;/code&gt; field in as a placeholder&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id='upgrading_to_the_new_version'&gt;Upgrading to the new version&lt;/h2&gt;

&lt;h3 id='tracker_and_collector'&gt;Tracker and collector&lt;/h3&gt;

&lt;p&gt;We have discussed above how to update your JavaScript tracker and CloudFront collector to support this new version 0.6.1. If you are using &lt;a href='https://github.com/shermozle/SnowCannon'&gt;SnowCannon&lt;/a&gt;, the node.js collector, you don&amp;#8217;t have to modify it - it already supports the new &lt;code&gt;i&lt;/code&gt; pixel.&lt;/p&gt;

&lt;h3 id='etl'&gt;ETL&lt;/h3&gt;

&lt;p&gt;To upgrade your ETL system, first re-deploy EmrEtlRunner from GitHub as per the &lt;a href='https://github.com/snowplow/snowplow/wiki/deploying-emretlrunner'&gt;EmrEtlRunner Setup Guide&lt;/a&gt;, and then update the ETL dependencies at the bottom of your &lt;code&gt;config.yml&lt;/code&gt; file like so:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:snowplow:
  :serde_version: 0.5.2
  :hive_hiveql_version: 0.5.1
  :non_hive_hiveql_version: 0.0.3&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id='storage'&gt;Storage&lt;/h3&gt;

&lt;h4 id='hive'&gt;Hive&lt;/h4&gt;

&lt;p&gt;If you are only using Hive for storage and analytics, you do not need to do anything to support this new release - because we add all new fields to the end of the file format, and field renames (like &lt;code&gt;event_name&lt;/code&gt; to &lt;code&gt;event&lt;/code&gt;) don&amp;#8217;t affect Hive on Amazon EMR.&lt;/p&gt;

&lt;h4 id='infobright'&gt;Infobright&lt;/h4&gt;

&lt;p&gt;If you are using Infobright Community Edition for analysis, you will need to update your table definition. This is a little complex, because Infobright does not support in-place table or column renames. To make this easier for you, we have created a script:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;4-storage/infobright-storage/migrate_to_003.sh&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Running this script will create a new table, &lt;code&gt;events_003&lt;/code&gt; (version 0.0.3 of the table definition) in your &lt;code&gt;snowplow&lt;/code&gt; database, copying across all your data from your existing &lt;code&gt;events&lt;/code&gt; table, which will not be modified in any way.&lt;/p&gt;

&lt;p&gt;Once you have run this, don&amp;#8217;t forget to update your StorageLoader&amp;#8217;s &lt;code&gt;config.yml&lt;/code&gt; to load into the new &lt;code&gt;events_003&lt;/code&gt; table, not your old &lt;code&gt;events&lt;/code&gt; table:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:storage:
  :type: infobright
  :database: snowplow
  :table:    events_003 # NOT &amp;quot;events&amp;quot; any more&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Done!&lt;/p&gt;

&lt;h2 id='getting_help'&gt;Getting help&lt;/h2&gt;

&lt;p&gt;If you have any problems with version 0.6.1, please &lt;a href='https://github.com/snowplow/snowplow/issues'&gt;raise an issue&lt;/a&gt; or get in touch with us via &lt;a href='https://github.com/snowplow/snowplow/wiki/Talk-to-us'&gt;the usual channels&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;And do let us know if the new features - such as user fingerprinting - are useful!&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/11/16/integrating-snowplow-with-google-tag-manager</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/11/16/integrating-snowplow-with-google-tag-manager"/>
    <title>Integrating SnowPlow with Google Tag Manager</title>
    <updated>2012-11-16T00:00:00+00:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;A month and a half ago, Google launched &lt;a href='http://www.google.com/tagmanager/'&gt;Google Tag Manager&lt;/a&gt; (GTM), a free tag management solution. That was a defining moment in tag management history as it will no doubt bring tag management, until now the preserve of big enterprises, into the mainstream.&lt;/p&gt;

&lt;p&gt;&lt;img alt='gtm' src='/static/img/gtm.JPG' /&gt;&lt;/p&gt;

&lt;p&gt;We have spent some time testing how to get SnowPlow tags working well with Google Tag Manager, and have documented our recommended approach to setting up SnowPlow with GTM on the &lt;a href='https://github.com/snowplow/snowplow/wiki/Integrating%20javascript%20tags%20with%20Google%20Tag%20Manager#wiki-snowplow-setup'&gt;wiki&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In the course of reading the literature on tag management and Google Tag Manager in particular, we were struck by a number of issues and misconceptions:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Setting up a Tag Management System is a big and complicated job. Much of the literature (especially around Google Tag Manager), and discussion (especially amongst members of the web analytics community) suggests it is very easy. Whilst Google Tag Manager is a straightforward product to use, the process of setting it up is difficult, because it involves thinking through, in a very rigorous way, exactly what data should be passed between the website and GTM. If this is not done properly, GTM will not have the relevant data to pass on to the tags it manages, including SnowPlow.&lt;/li&gt;

&lt;li&gt;Exacerbating the above, there is a lack of detailed literature on how to go about identifying all the data points that should be passed between your website and tag management solution. Nearly all the guides to setting up GTM that we reviewed covered only the most basic of GTM setups, which is just enough to enable page tracking. Clearly, that is not going to be sufficient for anything but the simplest blogs and brochureware sites.&lt;/li&gt;

&lt;li&gt;One of the tag management features most regularly trumpetted by digital and marketing analysts is that it frees them from having to liaise with their webmasters to add new tags and change existing tag setups. Even with a tag management solution in place, however, this may still be necessary if not all the data that the analyst wants passed through to their analytics package is available in the tag maangement system.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;As a step towards addressing the above issues, we have produced a step-by-step guide to both &lt;a href='https://github.com/snowplow/snowplow/wiki/Integrating%20javascript%20tags%20with%20Google%20Tag%20Manager#wiki-setup-gtm'&gt;setting up Google Tag Manager&lt;/a&gt; and &lt;a href='https://github.com/snowplow/snowplow/wiki/Integrating%20javascript%20tags%20with%20Google%20Tag%20Manager#wiki-snowplow-setup'&gt;setting up SnowPlow within GTM&lt;/a&gt;. We hope you find it useful. We plan to produce a similar guide to setting up SnowPlow within &lt;a href='http://www.opentag.qubitproducts.com/'&gt;Qubit&amp;#8217;s OpenTag solution&lt;/a&gt; in due course.&lt;/p&gt;

&lt;p&gt;We&amp;#8217;re very excited by Google&amp;#8217;s launch of Tag Manager, and recommend all new SnowPlow users who are not currently using a tag management system integrate one as part of their SnowPlow setup. Specifically:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The exercise that companies need to go through when setting up a tag management platform i.e. thinking through all the data points that they want to pass to their tag management system so that they can be passed on to whatever tags are managed in the system, is the same process they should go through when they setup SnowPlow, with a view to enabling the widest possible set of analyses on their web analytics data. So even though that exercise is not easy, it is valuable&lt;/li&gt;

&lt;li&gt;The processing of mapping that data from the structure defined in the tag management system to one which works with SnowPlow&amp;#8217;s data structure is the exact reverse of the analysis process that takes SnowPlow data and transforms it back into the structure that&amp;#8217;s most natural for the company performing the analysis.&lt;/li&gt;

&lt;li&gt;Once the tag management system has been installed, it becomes easy to upgrade the tags and / or change the configuration. There are a number of improvements we plan to make to our &lt;a href='https://github.com/snowplow/snowplow/tree/master/1-trackers/javascript-tracker'&gt;Javascript tracker&lt;/a&gt;, and having a tag management program will make it easier for companies to take advantage of those upgrades.&lt;/li&gt;
&lt;/ol&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/11/12/snowplow-0.6.0-released-with-storage-loader</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/11/12/snowplow-0.6.0-released-with-storage-loader"/>
    <title>SnowPlow 0.6.0 released, with the new StorageLoader</title>
    <updated>2012-11-12T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We&amp;#8217;re very pleased to start the week by releasing a new version of SnowPlow - version &lt;strong&gt;0.6.0&lt;/strong&gt;. This is a big release for us - as it includes the first version of our all-new StorageLoader. The release also includes a small set of tweaks and bug fixes across the existing SnowPlow components, but let&amp;#8217;s start by introducing StorageLoader:&lt;/p&gt;

&lt;h2 id='introducing_storageloader'&gt;Introducing StorageLoader&lt;/h2&gt;

&lt;p&gt;Up until now, SnowPlow has stored all its data in S3, where it can be queried in Hive. However, our vision with SnowPlow has always been to enable to the broadest set of analyses on SnowPlow data as possible. That means making it as easy as possible to keep up to date versions of SnowPlow data in many different types of database. The StorageLoader is a key component to fulfilling that vision.&lt;/p&gt;

&lt;p&gt;&lt;img alt='snowplow-loader-image' src='/static/img/SnowPlowLoader.jpg' /&gt;&lt;/p&gt;

&lt;p&gt;StorageLoader is a Ruby application that downloads SnowPlow event files from S3 and loads them into an alternative database. It has been built to make keeping an up to date version of your SnowPlow data in other databases as easy as possible. Currently, it only supports loading the data into &lt;a href='http://www.infobright.org/'&gt;Infobright Community Edition (ICE)&lt;/a&gt; - a high-performance columnar database based on MySQL. However, we plan to extend it over the next few months to support a range of other databases including:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href='https://developers.google.com/bigquery'&gt;Google Big Query&lt;/a&gt; for fast analysis of massive data sets. (This could be very powerful for rapid analytics across SnowPlow&amp;#8217;s granular data)&lt;/li&gt;

&lt;li&gt;&lt;a href='http://skydb.io'&gt;SkyDB&lt;/a&gt; for event path analysis and other, broader types of event stream analytics&lt;/li&gt;

&lt;li&gt;&lt;a href='http://www.postgresql.org'&gt;PostgreSQL&lt;/a&gt; for web analytics for web properties where the levels of traffic are not Facebook-scale&lt;/li&gt;
&lt;/ul&gt;
&lt;!--more--&gt;
&lt;p&gt;There are significant advantages to storing data in Infobright instead of (or as well as) S3:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;In many cases, query times are much faster&lt;/li&gt;

&lt;li&gt;There are a wide range of analytics tools that plug directly into Infobright. (Any tool that plugs into MySQL.) These can now be run directly on top of SnowPlow data. (These tools include R and Tableau.)&lt;/li&gt;

&lt;li&gt;For more details on the pros and cons of storage in S3 vs Infobright, see our &lt;a href='https://github.com/snowplow/snowplow/wiki/choosing-a-storage-module'&gt;guide to choosing between the two&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As you can hopefully get a sense looking at our roadmap for other databases to support, there are obvious advantages to using some of the other databases on our roadmapGoing forwards, we expect that many companies using SnowPlow will store that SnowPlow data in more than one store, to enable a very broad range of analytics from different types of tools.&lt;/p&gt;

&lt;h2 id='using_the_storageloader'&gt;Using the StorageLoader&lt;/h2&gt;

&lt;p&gt;You can configure StorageLoader with the details of the Infobright table to insert your SnowPlow events into, and then you schedule StorageLoader (e.g. in a cronjob) to regularly download your SnowPlow events and load them into Infobright. StorageLoader can run as soon as EmrEtlRunner has completed its job (and we include a script to run both in one go).&lt;/p&gt;

&lt;p&gt;With this setup, you will have your SnowPlow events easily accessible and queryable in a local Infobright instance - but you can still fall back to querying the data in Hive if you wish.&lt;/p&gt;

&lt;p&gt;The following setup guides should be helpful in terms of setting up StorageLoader:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href='https://github.com/snowplow/snowplow/wiki/infobright-storage-setup'&gt;Infobright storage setup guide&lt;/a&gt; walks you through the process of installing Infobright and setting it up to house SnowPlow data&lt;/li&gt;

&lt;li&gt;&lt;a href='https://github.com/snowplow/snowplow/wiki/StorageLoader-setup'&gt;StorageLoader setup guide&lt;/a&gt; walks you through installing and configuring StorageLoader to regularly load SnowPlow data into Infobright&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id='the_codebase'&gt;The codebase&lt;/h2&gt;

&lt;p&gt;If you want to take a look at the code, you can find it in the main repository here: &lt;a href='https://github.com/snowplow/snowplow/tree/master/4-storage/storage-loader'&gt;4-storage/storage-loader/&lt;/a&gt;&lt;/p&gt;

&lt;h2 id='getting_help'&gt;Getting help&lt;/h2&gt;

&lt;p&gt;If you have any problems getting StorageLoader working, please &lt;a href='https://github.com/snowplow/snowplow/issues'&gt;raise an issue&lt;/a&gt; or get in touch with us via &lt;a href='https://github.com/snowplow/snowplow/wiki/Talk-to-us'&gt;the usual channels&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id='other_fixes_in_060'&gt;Other fixes in 0.6.0&lt;/h2&gt;

&lt;p&gt;We have made a number of other fixes across SnowPlow to prepare the ground for StorageLoader:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;EmrEtlRunner&lt;/strong&gt; has been bumped to 0.0.5, including upgrading it to Sluice 0.0.4 (which has some bug fixes around S3 path handling).&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;Hive deserializer&lt;/strong&gt; has been bumped to 0.5.1, and now outputs booleans such as &lt;code&gt;br_cookies&lt;/code&gt; as 0 or 1 (instead of true or false) for the non-Hive output.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;non-Hive format HiveQL script&lt;/strong&gt; has been bumped to 0.0.2 and now uses the new 0 or 1 approach to booleans. This is necessary so that true/false values can be successfully loaded into Infobright.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;setup_infobright.sql&lt;/strong&gt; script has been bumped to 0.0.2 - we have changed the columns defined as booleans to be tinyint(1)s. This is just a formality, because Infobright creates &amp;#8216;boolean&amp;#8217; columns as tinyint(1)s anyway.&lt;/p&gt;

&lt;p&gt;We will keep you posted as we roll out support for additional database options in StorageLoader! (And welcome suggestinos for other databases we should build support for.)&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/11/06/snowplow-0.5.2-released-and-introducing-sluice</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/11/06/snowplow-0.5.2-released-and-introducing-sluice"/>
    <title>SnowPlow 0.5.2 released, and introducing the Sluice Ruby gem</title>
    <updated>2012-11-06T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;Another week, another release: SnowPlow &lt;strong&gt;0.5.2&lt;/strong&gt;! This is a small release, consisting just of a small set of bug fixes and improvements to EmrEtlRunner - although we&amp;#8217;ll also use this post to introduce our new Ruby gem, called Sluice.&lt;/p&gt;

&lt;p&gt;Many thanks to community member &lt;a href='https://github.com/testower'&gt;Tom Erik Støwer&lt;/a&gt; for his testing of EmrEtlRunner over the weekend, which helped us to identify and fix these bugs:&lt;/p&gt;

&lt;h2 id='bugs_fixed'&gt;Bugs fixed&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href='https://github.com/snowplow/snowplow/issues/71'&gt;Issue 71&lt;/a&gt;&lt;/strong&gt;: the template &lt;code&gt;config.yml&lt;/code&gt; (in the GitHub repo and in the wiki) was specifying an out-of-date version for the Hive deserializer. We have updated this to specify version &lt;strong&gt;0.5.0&lt;/strong&gt; of the serde, like so:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;...
:snowplow:
  :serde_version: 0.4.9
...&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href='https://github.com/snowplow/snowplow/issues/72'&gt;Issue 72&lt;/a&gt;&lt;/strong&gt;: Tom&amp;#8217;s testing also identified a bug in EmrEtlRunner&amp;#8217;s log archiving, which only occurs if the Processing Bucket contains sub-folders. This has now been fixed too.&lt;/p&gt;
&lt;!--more--&gt;
&lt;h2 id='a_new_feature_skip'&gt;A new feature: &amp;#8211;skip&lt;/h2&gt;

&lt;p&gt;A new release which only contains bug fixes is a boring release, so we have also implemented a new &lt;code&gt;--skip&lt;/code&gt; option for EmrEtlRunner (&lt;a href='https://github.com/snowplow/snowplow/issues/58'&gt;issue #58&lt;/a&gt;). You can use this when you call EmrEtlRunner like so:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ bundle exec snowplow-emr-etl-runner &amp;lt;...&amp;gt; --skip staging OR --skip emr&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This option skips the work steps &lt;strong&gt;up to and including&lt;/strong&gt; the specified step. To give an example: &lt;code&gt;--skip emr&lt;/code&gt; skips both moving the raw logs to the Staging Bucket &lt;strong&gt;and&lt;/strong&gt; running the ETL process on Amazon EMR, i.e. EmrEtlRunner will &lt;strong&gt;only&lt;/strong&gt;* perform the final archiving step.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;--skip&lt;/code&gt; is useful if you encounter a problem midway through your ETL process: you can fix the problem and then skip the steps which ran okay, rather than re-processing from the start. We find it especially helpful when we&amp;#8217;re testing new versions of EmrEtlRunner.&lt;/p&gt;

&lt;h2 id='and_introducing_sluice'&gt;And introducing Sluice&lt;/h2&gt;

&lt;p&gt;At SnowPlow Analytics we are committed to making our software as modular and loosely-coupled as possible. Where we have functionality which could be more widely used, we aim to extract it into standalone modules for developers to use even if they are not implementing SnowPlow.&lt;/p&gt;

&lt;p&gt;We have followed this approach with the parallel file-copy code for Amazon S3 added to EmrEtlRunner by community member &lt;a href='https://github.com/mtibben'&gt;Michael Tibben&lt;/a&gt; from &lt;a href='http://99designs.com'&gt;99designs&lt;/a&gt;: we have moved this code out of EmrEtlRunner into a new Ruby gem, called Sluice. Sluice now has its own &lt;a href='https://github.com/snowplow/sluice'&gt;GitHub repository&lt;/a&gt;, and has been published on &lt;a href='http://rubygems.org/gems/sluice'&gt;RubyGems.org&lt;/a&gt;. It&amp;#8217;s called Sluice because, like &lt;a href='https://github.com/cwensel'&gt;Chris Wensel&lt;/a&gt; (Cascading), we believe in flowing-water metaphors for ETL tools :-)&lt;/p&gt;

&lt;p&gt;Sluice is used by our EmrEtlRunner, and is also a dependency for the StorageLoader Ruby application which we are currently developing.&lt;/p&gt;

&lt;p&gt;We hope to build out Sluice as a general-purpose Ruby toolkit for cloud-friendly ETL over the coming months - and would love contributors! Our view is that, in a world of cloud services like Amazon S3, Google BigQuery and Elastic MapReduce, it makes most sense to take a programmatic approach to ETL, rather than contort the historic, application-based approach of &lt;a href='http://www.talend.com'&gt;Talend&lt;/a&gt;, &lt;a href='http://www.pentaho.com/explore/pentaho-data-integration/'&gt;Pentaho DI&lt;/a&gt; et al. We see Sluice as part of that toolkit for programmatic ETL, alongside great tools such as &lt;a href='http://www.cascading.org'&gt;Cascading&lt;/a&gt;, Rob Slifka&amp;#8217;s &lt;a href='https://github.com/rslifka/elasticity'&gt;Elasticity&lt;/a&gt; and &lt;a href='http://palletops.com'&gt;Pallet&lt;/a&gt;.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/11/01/snowplow-0.5.1-released</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/11/01/snowplow-0.5.1-released"/>
    <title>SnowPlow 0.5.1 released, with lots of small improvements</title>
    <updated>2012-11-01T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We have just released SnowPlow &lt;strong&gt;0.5.1&lt;/strong&gt;! Rather than one large new feature, version 0.5.1 is an incremental release which contains lots of small fixes and improvements to the ETL and storage sub-systems. The two big themes of these updates are:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Improving the robustness of the ETL process&lt;/li&gt;

&lt;li&gt;Laying the foundations for loading SnowPlow events into &lt;a href='http://www.infobright.org/'&gt;Infobright Community Edition&lt;/a&gt; (ICE)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To take each of these themes in turn:&lt;/p&gt;

&lt;h2 id='1_a_more_robust_etl_process'&gt;1. A more robust ETL process&lt;/h2&gt;

&lt;p&gt;The Hive deserializer now has improved error handling - many thanks to community member &lt;a href='https://github.com/mtibben'&gt;Michael Tibben&lt;/a&gt; from &lt;a href='http://99designs.com'&gt;99designs&lt;/a&gt; for his help here!&lt;/p&gt;

&lt;p&gt;Firstly, the Hive deserializer is now setup to log warnings (rather than die) on non-critical data quality issues.&lt;/p&gt;

&lt;p&gt;Additionally, there is now an option (switched off by default) to continue processing even on unexpected row-level errors (such as an input file not matching the expected CloudFront format). We have added a configuration option to the EmrEtlRunner&amp;#8217;s &lt;a href='https://github.com/snowplow/snowplow/blob/master/3-etl/emr-etl-runner/config/config.yml'&gt;configuration file&lt;/a&gt; to support this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:etl:
  :continue_on_unexpected_error: false&lt;/code&gt;&lt;/pre&gt;
&lt;!--more--&gt;
&lt;p&gt;Switch this to &amp;#8216;true&amp;#8217; to continue processing on unexpected row-level errors.&lt;/p&gt;

&lt;h2 id='2_groundwork_for_infobright_compatibility'&gt;2. Groundwork for Infobright compatibility&lt;/h2&gt;

&lt;p&gt;We have added a table definition (and supporting scripts) for setting up a SnowPlow events table in Infobright - you can find these in the main repository under &lt;a href='https://github.com/snowplow/snowplow/tree/master/4-storage/infobright-storage'&gt;&lt;code&gt;snowplow/4-storage/infobright-storage&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Some early ETL design decisions meant that the SnowPlow event files being generated before 0.5.1 were not compatible with being loaded into Infobright (or similar relational databases like Postgres or MySQL). We have made some updates to the ETL process in 0.5.1 to fix this:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;In the Hive deserializer, we now convert tabs to 4 spaces to prevent a stray tab from breaking our load into Infobright&lt;/li&gt;

&lt;li&gt;Databases like Infobright don&amp;#8217;t support Hive&amp;#8217;s &lt;code&gt;ARRAY&amp;lt;STRING&amp;gt;&lt;/code&gt; syntax, so we have updated the Hive deserializer to also output individual booleans for the browser features, alongside the browser features array&lt;/li&gt;

&lt;li&gt;We have created a new HiveQL script which outputs SnowPlow event files in a format which can be easily loaded into Infobright - this is called &lt;a href='https://github.com/snowplow/snowplow/blob/master/3-etl/hive-etl/hiveql/non-hive-rolling-etl.q'&gt;&lt;code&gt;non-hive-rolling-etl.q&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;We have added a configuration option to the EmrEtlRunner&amp;#8217;s &lt;a href='https://github.com/snowplow/snowplow/blob/master/3-etl/emr-etl-runner/config/config.yml'&gt;configuration file&lt;/a&gt; so that you can choose whether to output Hive-format or non-Hive-format event files&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;On point 4: we believe that most people will want to load their SnowPlow event files into other database systems, such as Infobright (or eventually, Postgres, Google BigQuery, SkyDB etc). Therefore, the default setting for the &lt;a href='https://github.com/snowplow/snowplow/blob/master/3-etl/emr-etl-runner/config/config.yml'&gt;configuration option&lt;/a&gt; in the EmrEtlRunner is to output your SnowPlow event files in the non-Hive-format:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:etl:
  :storage_format: non-hive&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As the comment says, if you will &lt;strong&gt;only&lt;/strong&gt; be doing analysis in Hive, you could switch this setting to &amp;#8216;hive&amp;#8217; and benefit from the slightly-tweaked, Hive-friendly file format.&lt;/p&gt;

&lt;h2 id='getting_help'&gt;Getting help&lt;/h2&gt;

&lt;p&gt;If you have any problems getting version 0.5.1 working, please &lt;a href='https://github.com/snowplow/snowplow/issues'&gt;raise an issue&lt;/a&gt; or get in touch with us via &lt;a href='https://github.com/snowplow/snowplow/wiki/Talk-to-us'&gt;the usual channels&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id='in_the_pipeline'&gt;In the pipeline&lt;/h2&gt;

&lt;p&gt;At SnowPlow we want to support multiple different storage and analytics options for SnowPlow events, alongside our current Hive-based approach. This version 0.5.1 provides the building blocks for our Infobright support - for the next release, we are working on a Storage Loader component to download your event files from Amazon S3 and load them into your local Infobright instance. We&amp;#8217;ll keep you posted on our progress here!&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/10/31/snowplow-in-a-universal-analytics-world-what-the-new-version-of-google-analytics-means-for-companies-adopting-snowplow</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/10/31/snowplow-in-a-universal-analytics-world-what-the-new-version-of-google-analytics-means-for-companies-adopting-snowplow"/>
    <title>SnowPlow in a Universal Analytics world - what the new version of Google Analytics means for companies adopting SnowPlow</title>
    <updated>2012-10-31T00:00:00+00:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;Earlier this week, Google announced a series of significant advances in Google Analytics at the GA Summit, that are collectively referred to as &lt;a href='http://cutroni.com/blog/2012/10/29/universal-analytics-the-next-generation-of-google-analytics/'&gt;Universal Analytics&lt;/a&gt;. In this post, we look at:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href='#what'&gt;The actual features Google has announced&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2012/10/31/snowplow-proposition-in-a-universal-analytics-world-what-the-new-version-of-ga-means-for-snowplow-adoption#whysnowplow'&gt;How those advances change the case for companies considering adopting SnowPlow&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img alt='universal-analytics-image' src='/static/img/google-analytics-universal-analytics.png' /&gt;&lt;/p&gt;
&lt;a name='what' /&gt;
&lt;h2 id='1_what_changes_has_google_announced'&gt;1. What changes has Google announced?&lt;/h2&gt;

&lt;p&gt;The most significant change Google has announced is the new &lt;a href='https://developers.google.com/analytics/devguides/collection/protocol/v1/'&gt;Measurement Protocol&lt;/a&gt;, which enables businesses using GA to capture much more data. This will make it possible for Google to deliver a much broader range of reports, of higher business value, than was previously possible. To understand the changes, we start by considering what &lt;a href='#new-data-points'&gt;new data points&lt;/a&gt; businesses can &lt;em&gt;feed&lt;/em&gt; GA, before considering &lt;a href='/blog/2012/10/31/snowplow-proposition-in-a-universal-analytics-world-what-the-new-version-of-ga-means-for-snowplow-adoption#reporting-capabilities'&gt;what that means for GA&amp;#8217;s reporting capabilities&lt;/a&gt;.&lt;/p&gt;
&lt;a name='new-data-points' /&gt;
&lt;h3 id='11_custom_user_identifiers'&gt;1.1 Custom user identifiers&lt;/h3&gt;

&lt;p&gt;The first new data points that businesses can feed into Google Analytics is a user&amp;#8217;s &lt;code&gt;client_id&lt;/code&gt; (basically, a customer ID) as defined on the business&amp;#8217;s own systems.&lt;/p&gt;

&lt;p&gt;Previously, Google Analytics identified unique users using their own &lt;code&gt;cookie_id&lt;/code&gt;s. Google&amp;#8217;s &lt;code&gt;cookie_id&lt;/code&gt;s are an excellent starting point for identifying users, because so many users have Google accounts (thanks to their myriad mass-market services, including Gmail, YouTube, Google Play etc): consumers using these services on multiple devices identify themselves to Google when they login, meaning that Google can marry their &lt;code&gt;cookie_id&lt;/code&gt;s for these users on all the different devices they use. Our assumption is that Google already use this to reliably identify individual users across multiple platforms and devices.&lt;/p&gt;

&lt;p&gt;For businesses using GA, being able to augment Google&amp;#8217;s user identification with their own internal &lt;code&gt;client_id&lt;/code&gt;s is a significant step forwards for two reasons:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Many GA users (especially those with applications or properties where users login, or those with a loyalty card scheme) can identify their own users reliably on specific platforms, devices and physical stores. By adding this additional user identification data into GA, GA will be more accurate at identifying the same user in different places reliably, moving us further from a world in which we rely on persistent cookies dropped on browsers with unique identifiers, to one where users are more robustly identified via logins, payments and loyalty schemes. These approaches will still use cookies, but as part of a broader set of user identification business processes that actively involve the user in the identification process&lt;/li&gt;

&lt;li&gt;It should make it easier for GA users to join GA data with other customer data sets on those &lt;code&gt;client_id&lt;/code&gt;s. This is more of a nuanced point, as it was still possible previously to add customer IDs to GA as custom variables and use that to do joining&lt;/li&gt;
&lt;/ol&gt;
&lt;!--more--&gt;&lt;a name='customer-journey' /&gt;
&lt;h3 id='12_capturing_events_across_a_users_entire_customer_journey_not_just_the_web_not_just_digital_interactions'&gt;1.2 Capturing events across a user&amp;#8217;s entire customer journey (not just the web, not just digital interactions)&lt;/h3&gt;

&lt;p&gt;We have long argued that web analytics is just one customer data source - and that analysts performing customer analytics need to crunch data covering the customer&amp;#8217;s complete journey, including other digital channels and offline interactions. That means joining data sets from different digital products and offline data sets to generate a single customer view. To date, companies that have implemented &amp;#8220;single customer views&amp;#8221; have typically struggled incorporating web behaviour in those views.&lt;/p&gt;

&lt;p&gt;Google has taken a significant step towards enabling businesses to capture much more of their customer&amp;#8217;s journeys in Google Analytics itself. The &lt;a href='https://developers.google.com/analytics/devguides/collection/protocol/v1/'&gt;Measurement Protocol&lt;/a&gt; makes it possible to pass offline events into Google: so for example, when a customer buys an item in store, it would be possible to fire an event to Google Analytics recording that sale. If the customer was on a CRM programme (e.g. loyalty scheme), his / her &lt;code&gt;client_id&lt;/code&gt; could be passed in, and then Google Analytics would know that this is the same user who browsed the website on their mobile phone yesterday and viewed it from their office today, prior to coming in store to make the purchase.&lt;/p&gt;

&lt;p&gt;The Measurement Protocol can also be used to capture events on digital platforms that are not so well suited to traditional web analytics solution e.g. mobile applications, set-top box applications, videogames on consoles etc. It thus opens the door for Google Analytics to capture and report on event data from a range of devices, not just those that are web based.&lt;/p&gt;

&lt;p&gt;Taken together, this means it will be possible for Google Analytics to offer reports detailing customer behaviour across the complete customer journey. Building on this, it should also be possible for GA to enable analysts to calculate &lt;a href='/analytics/customer-analytics/customer-lifetime-value.html'&gt;customer lifetime value&lt;/a&gt; (if the value of different events was passed in with the events): this is one of the most important metrics in customer analytics, and one that has been conspicuous by its absence from web analytics outside of solutions like &lt;a href='/analytics/customer-analytics/customer-lifetime-value.html'&gt;SnowPlow&lt;/a&gt; until now. The Measurement Protocol potentially means a huge increase in the scope and value of reports that it should be possible to generate in Google Analytics.&lt;/p&gt;

&lt;h3 id='13_capturing_customeracquisition_cost_data'&gt;1.3 Capturing customer-acquisition cost data&lt;/h3&gt;

&lt;p&gt;One of the most common types of analytics performed on web data is working out the return on marketing investment for different customer-acquisition channels. To perform this analysis, we need to combine:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Data on what was spent on each channel - typically cost data from those channels themselves, e.g. display, PPC, affiliate, social etc&lt;/li&gt;

&lt;li&gt;Web analytics data on how many people visited the website in response to those ads and what fraction of them went on to become customers. By dividing the total spent on each channel (1) by the number of customers acquired from each channel (2), we can calculate the cost of acquiring each customer for that channel.&lt;/li&gt;

&lt;li&gt;Financial data on the revenue/profits generated by those customers, over their lifetimes. By comparing the average value of each customer acquired from each channel against the average cost of acquiring each of those customers, we can calculate the return on that acquisition cost&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Now that Google lets businesses reliably track their users over their entire lifecycles (on and offline), it becomes possible to calculate the user&amp;#8217;s lifetime value, as detailed &lt;a href='#customer-journey'&gt;above&lt;/a&gt;, delivering on point #3. Google has always enabled business to capture #2. Now, Google lets you send the cost data into Google Analytics (point #1) - so that the return of each campaign can be accurately calculated. (Previously, only spend data from AdWords could be imported into GA.) With this information, companies should be better placed to drive marketing spend decisions based on Google Analytics reports. Again though, the reality is more nuanced, because typically those spend decisions have to be made &lt;em&gt;before&lt;/em&gt; a customer&amp;#8217;s lifetime value (#3) can be accurately calculated, so companies really need to develop predictive models of how valuable customers are likely to be. Anything but the most basic models are likely to require tools outside of GA to develop, and then pulling that data out of GA to power those models.&lt;/p&gt;

&lt;h3 id='14_custom_dimensions_and_metrics'&gt;1.4 Custom dimensions and metrics&lt;/h3&gt;

&lt;p&gt;The &lt;a href='https://developers.google.com/analytics/devguides/collection/protocol/v1/'&gt;Measurement Protocol&lt;/a&gt; enables businesses to define and capture their own dimensions and metrics each time an event that is tracked. Those additional metrics and dimensions are then available to report in in GA.&lt;/p&gt;

&lt;p&gt;As well as enabling businesses to add custom dimension and metric values to individual event tracking calls, Google also lets businesses bulk upload multiple dimensions at a time into the GA, if a relationship between those custom dimensions and dimensions already in GA can be defined, and GA knows what values to ascribe events already in it to those new dimensions, based on that defined relationship. To give an example: you could upload the product names/SKUs associated with each web page, enabling reporting on page views by SKU. Or, you could upload a range of product metadata (e.g. book titles and authors) and associate that with an ISBN custom field.&lt;/p&gt;
&lt;a name='reporting-capabilities' /&gt;
&lt;h3 id='15_what_new_reporting_is_enabled_through_the_capture_of_all_these_additional_data_points'&gt;1.5 What new reporting is enabled through the capture of all these additional data points?&lt;/h3&gt;

&lt;p&gt;Taken together, the additional data that businesses can feed into Google Analytics gives Google enough to offer a much broader and more valuable range of reporting than was previously possible:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Customer analytics&lt;/strong&gt;. We have long argued that web analytics packages including GA are too focused on sessions, page views and conversions, and neglect the broader, more valuable customer analytics that underpin the most successful businesses in the world. With these new data points, GA has the raw data to produce useful customer reports including customer lifetime value, and analysis of user behaviours over their entire journeys. No longer will web analysts using GA be confined to viewing actions over an isolated session: now they can slice and dice metrics by users over their user journeys spanning multiple site visits.&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Event analytics&lt;/strong&gt; across platforms, on and offline. GA can now report on user&amp;#8217;s complete journey, not just what they do on websites, but also their behaviours on other digital platforms (esp. mobile) and offline.&lt;/li&gt;
&lt;/ol&gt;
&lt;a name='whysnowplow' /&gt;
&lt;h2 id='2_how_do_the_advances_in_ga_change_the_case_for_adopting_snowplow'&gt;2. How do the advances in GA change the case for adopting SnowPlow?&lt;/h2&gt;

&lt;p&gt;Prior to the latest announcement, the case for adopting SnowPlow alongside your GA implementation was as follows:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The reporting provided by Google Analytics is very limited, with little/no customer analytics, catalogue analytics and platform analytics supported.&lt;/li&gt;

&lt;li&gt;SnowPlow enables you to perform all these three types of analytics, by providing you with access to your raw customer-level and event-level clickstream data, so that you can use whatever analytics tool you like to crunch the data and perform that analysis&lt;/li&gt;

&lt;li&gt;SnowPlow makes it easier to join your web analytics data sets with other data sets (e.g. marketing data sets, CRM and offline data sets), by enabling businesses to load customer IDs into SnowPlow, and then perform the join on the raw data sets. This means that businesses running SnowPlow can analyse user behaviour across their entire customer journey (on and offline, across all digital and non-digital channels)&lt;/li&gt;

&lt;li&gt;SnowPlow makes it easy to warehouse your customer data for posterity: an asset which will doubtless grow in value over time.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Following the latest announcement, some of these arguments fall away:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Google has significantly strengthened its customer analytics capability. To what extent is not yet clear - we only know at this stage what extra data points Google Analytics will, hypothetically, let you collect - not what additional reporting UIs GA will provide to process that data&lt;/li&gt;

&lt;li&gt;The additional data points &lt;em&gt;should&lt;/em&gt; improve GA&amp;#8217;s platform and catalogue analytics capabilities; we will only be able to confirm this once we start working with the updated version of GA&lt;/li&gt;

&lt;li&gt;Therefore, the gap between what is possible with GA, and what is possible with SnowPlow, has shrunk&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Nevertheless, the case for implementing SnowPlow alongside GA is still compelling, for three main reasons. To take each of these in turn:&lt;/p&gt;

&lt;h3 id='21_analytics_capabilities'&gt;2.1 Analytics capabilities&lt;/h3&gt;

&lt;p&gt;There are several different considerations here:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Google Analytics still does not give you access to your customer-level and event-level data. Therefore, &lt;strong&gt;there will always be ways that you can crunch SnowPlow data that you cannot accomplish in GA&lt;/strong&gt;: drilling down to segments of one visitor is just the most obvious example&lt;/li&gt;

&lt;li&gt;There are a range of analytics techniques which are hard to imagine Google implementing at all, even with the new data sets that are available. To give just three examples:&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
&lt;li&gt;Using machine learning techniques (e.g. &lt;a href='http://mahout.apache.org/'&gt;Mahout&lt;/a&gt;) to &lt;strong&gt;segment audience by behaviour&lt;/strong&gt;&lt;/li&gt;

&lt;li&gt;Performing &lt;strong&gt;event analytics&lt;/strong&gt; / pathing in a way that takes into account the &lt;strong&gt;structure of the website&lt;/strong&gt;. This is described brilliantly by &lt;a href='http://semphonic.blogs.com/about.html'&gt;Gary Angel&lt;/a&gt; on the &lt;a href='http://semphonic.blogs.com/semangel/2011/01/statistical-analysis-functionalism-and-how-web-analytics-works.html'&gt;Semphonic blog&lt;/a&gt;. This methodology includes identifying those events that are predictive of customer lifetime value&lt;/li&gt;

&lt;li&gt;Building and testing models that &lt;strong&gt;predict customer lifetime value ahead of time&lt;/strong&gt;, so that you can quickly (and robustly) calculate the ROI on marketing campaigns, and adjust your spend accordingly&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
&lt;li&gt;There will always be barriers analysts run up against in trying to fit all of their data into Google&amp;#8217;s schema. For example, it&amp;#8217;s not obvious how Google&amp;#8217;s single &lt;code&gt;client_id&lt;/code&gt; will cope with different packages (CRM, email, CMS et al) each having their own internal set of user IDs&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id='22_creating_live_datadriven_products'&gt;2.2 Creating live, data-driven products&lt;/h3&gt;

&lt;p&gt;There are also important capabilities around using your event data and derived analyses in &lt;strong&gt;live, data-driven products&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Having access to the event stream and your own analyses allows you to make use of that data in data-driven products and systems, including &lt;strong&gt;product / content recommendation&lt;/strong&gt;, &lt;strong&gt;user personalisation engines&lt;/strong&gt; and &lt;strong&gt;internal search algorithms&lt;/strong&gt;&lt;/li&gt;

&lt;li&gt;Because SnowPlow is open-source software which can be installed on your own servers, it should be possible to co-locate SnowPlow with your own software (CMSes, ecommerce packages, custom apps etc) and thus tightly integrate these data-driven products into your offering&lt;/li&gt;

&lt;li&gt;Because GA doesn&amp;#8217;t provide the granular customer-level and event-level data, GA data cannot be used to prototype or drive these data-driven services&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id='23_data_ownership_and_technical_architecture'&gt;2.3 Data ownership and technical architecture&lt;/h3&gt;

&lt;p&gt;Finally, there are also a number of &lt;strong&gt;data ownership&lt;/strong&gt; and &lt;strong&gt;architectural issues&lt;/strong&gt; which we believe make a SnowPlow solution an important compliment, if not yet a full alternative, to a GA implementation. These relate to the fact that, with GA, businesses get more value out by feeding more and more data in: to realise all of the new potential above, they need to be feeding GA with data covering their &lt;em&gt;complete&lt;/em&gt; set of customer interactions. However:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;This is the &lt;strong&gt;most valuable data&lt;/strong&gt; your company owns. Does it make sense to leave the warehousing and storage of that data to a third-party who in many cases is providing the service for free? What guarantees does you have that that data will always be available, 3, 5 or 10 years down the line?&lt;/li&gt;

&lt;li&gt;Does it make sense to feed your detailed event- and customer-level data to Google Analytics, when GA does not share that data back with you at the same atomic level of detail. (GA rolls the data up to &lt;strong&gt;aggregates&lt;/strong&gt; which are less flexible to work with from an analytics perspective)&lt;/li&gt;

&lt;li&gt;What happens when &lt;strong&gt;innacurate data&lt;/strong&gt; is loaded into Google Analytics? Without the ablity to query and diligence the data directly, leave alone clean and reprocess data, there are very limited options available for a business that has innaccurate data in GA. This becomes a bigger issue as implementation become more complex (because data is being ingested across digital and offline platforms), and GA becomes the de facto tool for all customer analytics&lt;/li&gt;

&lt;li&gt;If you need to setup regular ETL processes to load the data from all of your third-party systems into GA, you could &lt;strong&gt;expend the same energy setting up SnowPlow instead&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id='closing_thoughts'&gt;Closing thoughts&lt;/h2&gt;

&lt;p&gt;We at SnowPlow Analytics are enormously excited by the progress Google are making with their Universal Analytics proposition, and especially the good work Google are doing educating the market into the value of customer-centric analytics. But to unleash the full power of that type of customer, platform and catalogue analytics, the serious analyst will still need access to the customer-level and event-level data: ideally on infrastructure that is totally under your own control. SnowPlow is still the best way of getting hold and storing that data.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/10/25/snowplow-0.5.0-released</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/10/25/snowplow-0.5.0-released"/>
    <title>SnowPlow 0.5.0 released, now with a Ruby gem to run SnowPlow's ETL process on Amazon EMR</title>
    <updated>2012-10-25T00:00:00+01:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We have just released SnowPlow &lt;strong&gt;0.5.0&lt;/strong&gt;, with an all-new component, the SnowPlow EmrEtlRunner. EmrEtlRunner is a Ruby application to run SnowPlow&amp;#8217;s Hive-based ETL (extract, transform, load) process on &lt;a href='http://aws.amazon.com/elasticmapreduce/'&gt;Amazon Elastic MapReduce&lt;/a&gt; with minimum fuss.&lt;/p&gt;

&lt;p&gt;We are hugely grateful to community member &lt;a href='https://github.com/mtibben'&gt;Michael Tibben&lt;/a&gt; from &lt;a href='http://99designs.com'&gt;99designs&lt;/a&gt; for his contributions to EmrEtlRunner: thanks to Michael, EmrEtlRunner is more efficient, more flexible and more robust than it otherwise would have been - and ready sooner. Many thanks Michael!&lt;/p&gt;

&lt;h2 id='using_emretlrunner'&gt;Using EmrEtlRunner&lt;/h2&gt;

&lt;p&gt;EmrEtlRunner is a Ruby application which you can setup on your server to regularly take your raw SnowPlow logs (as stored in CloudFront access logs) and apply the Hive-based ETL process to them using &lt;a href='http://aws.amazon.com/elasticmapreduce/'&gt;Amazon Elastic MapReduce&lt;/a&gt;. This ETL process populates a Hive-format events table which you can then use with the HiveQL recipes in our &lt;a href='http://snowplowanalytics.com/analytics/index.html'&gt;Analyst&amp;#8217;s Cookbook&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;For detailed instructions on installing, running and scheduling EmrEtlRunner on your server, please see the &lt;a href='https://github.com/snowplow/snowplow/wiki/Deploying-EmrEtlRunner'&gt;Deploying EmrEtlRunner&lt;/a&gt; guide on the SnowPlow Analytics wiki.&lt;/p&gt;
&lt;!--more--&gt;
&lt;h2 id='the_codebase'&gt;The codebase&lt;/h2&gt;

&lt;p&gt;If you want to take a look at the code, you can find it in the main repository here: &lt;a href='https://github.com/snowplow/snowplow/tree/master/3-etl/emr-etl-runner'&gt;&lt;code&gt;3-etl/emr-etl-runner/&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id='getting_help'&gt;Getting help&lt;/h2&gt;

&lt;p&gt;If you have any problems getting EmrEtlRunner working, please &lt;a href='https://github.com/snowplow/snowplow/issues'&gt;raise an issue&lt;/a&gt; or get in touch with us via &lt;a href='https://github.com/snowplow/snowplow/wiki/Talk-to-us'&gt;the usual channels&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id='in_the_pipeline'&gt;In the pipeline&lt;/h2&gt;

&lt;p&gt;At SnowPlow we want to support multiple different storage and analytics options for SnowPlow events, alongside our current Hive-based approach. Our first priority is supporting &lt;a href='http://www.infobright.org/'&gt;Infobright Community Edition&lt;/a&gt; (ICE) for event storage and querying; extending the current ETL process to load SnowPlow events into ICE will be the focus of our next few releases, so please stay tuned!&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/10/24/web-analytics-with-tableau-and-snowplow</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/10/24/web-analytics-with-tableau-and-snowplow"/>
    <title>Performing web analytics on SnowPlow data using Tableau - a video demo</title>
    <updated>2012-10-24T00:00:00+01:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;People who see SnowPlow for the first time often ask us to &lt;i&gt;&quot;show SnowPlow in action&quot;&lt;/i&gt;. It is one thing to tell someone that having access to their customer- and event-level data will open up whole new analysis possibilities, but it is another thing to demonstrate those possibilities.&lt;/p&gt;

&lt;p&gt;Demonstrating SnowPlow is tricky because currently, SnowPlow only gives you access to data: we have no snazzy front-end UI to show off. The good news is that there are a lot of smart people developing fast, powerful and easy-to-use reporting tools. And because SnowPlow gives you access to underlying customer- and event-level data, it is easy to analyse SnowPlow data in nearly all of these tools. One such tool is &lt;a href=&quot;http://www.tableausoftware.com/&quot;&gt;Tableau&lt;/a&gt; - we like Tableau as it is fast and intuitive, making it easy for us to perform train-of-thought analyses on SnowPlow data. (We will explain more on how to connect Tableau to SnowPlow data in a future blog post.)&lt;/p&gt;

&lt;p&gt;In the following series of videos, we start to show how SnowPlow lets you use &lt;a href=&quot;http://www.tableausoftware.com/&quot;&gt;Tableau&lt;/a&gt; for exploring your web analytics data. In the first video, we introduce Tableau and talk through the Tableau worksheet created with SnowPlow data for an online retailer:&lt;/p&gt;

&lt;video width=&quot;648&quot; height=&quot;563&quot; controls&gt;

	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-1.mp4&quot;  type=&quot;video/mp4&quot; /&gt;
	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-1.webm&quot; type=&quot;video/webm&quot; /&gt;
	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-1.ogv&quot;  type=&quot;video/ogg&quot; /&gt;
	&lt;object width=&quot;648&quot; height=&quot;563&quot; type=&quot;application/x-shockwave-flash&quot; data=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-1.swf&quot;&gt;
		&lt;param name=&quot;movie&quot; value=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-1.swf&quot; /&gt;
		&lt;param name=&quot;flashvars&quot; value=&quot;controlbar=over&amp;amp;image=http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-1-thumb.jpg&amp;amp;file=http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-1.mp4&quot; /&gt;
		&lt;img src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-1-thumb.jpg&quot; width=&quot;648&quot; height=&quot;563&quot; alt=&quot;Web analytics with Tableau and SnowPlow introductory video&quot;
		     title=&quot;No video playback capabilities, please download the video below&quot; /&gt;
	&lt;/object&gt; 
&lt;/video&gt;

&lt;p class=&quot;note&quot;&gt;&lt;i&gt;Having trouble viewing the video above? You may download the videos in your format of choice:&lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-1.mp4&quot;&gt;&quot;MP4&quot;&lt;/a&gt;, &lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-1.ogv&quot;&gt;&quot;Ogg&quot;&lt;/a&gt; or &lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-1.webm&quot;&gt;WebM&lt;/a&gt; formats.&lt;/i&gt;&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;In the second video, we show how to perform an analysis of the drivers of growth of traffic on a website. The video serves to highlight how effective Tableau is at performing train-of-thought analysis:&lt;/p&gt;

&lt;video width=&quot;648&quot; height=&quot;563&quot; controls&gt;

	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-2.mp4&quot;  type=&quot;video/mp4&quot; /&gt;
	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-2.webm&quot; type=&quot;video/webm&quot; /&gt;
	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-2.ogv&quot;  type=&quot;video/ogg&quot; /&gt;
	&lt;object width=&quot;648&quot; height=&quot;563&quot; type=&quot;application/x-shockwave-flash&quot; data=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-2.swf&quot;&gt;
		&lt;param name=&quot;movie&quot; value=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-2.swf&quot; /&gt;
		&lt;param name=&quot;flashvars&quot; value=&quot;controlbar=over&amp;amp;image=http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-2-thumb.jpg&amp;amp;file=http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-2.mp4&quot; /&gt;
		&lt;img src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-2-thumb.jpg&quot; width=&quot;648&quot; height=&quot;563&quot; alt=&quot;Web analytics with Tableau and SnowPlow introductory video&quot;
		     title=&quot;No video playback capabilities, please download the video below&quot; /&gt;
	&lt;/object&gt; 
&lt;/video&gt;

&lt;p class=&quot;note&quot;&gt;&lt;i&gt;Having trouble viewing the video above? You may download the videos in your format of choice:&lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-2.mp4&quot;&gt;&quot;MP4&quot;&lt;/a&gt;, &lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-2.ogv&quot;&gt;&quot;Ogg&quot;&lt;/a&gt; or &lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-2.webm&quot;&gt;WebM&lt;/a&gt; formats.&lt;/i&gt;&lt;/p&gt;

&lt;p&gt;In the third video, we show how to perform an analysis comparing the relative performance of different products in an online retailer's catalogue. This is an example of &lt;strong&gt;catalogue analytics&lt;/strong&gt;, a very important branch of analytics - where we analyse how different products on a retailer's site perform relative to one another, or how different media items (e.g. articles / videos) on a media site perform. Surprisingly, catalogue analytics is not supported by traditional web analytics packages like Google Analytics:&lt;/p&gt;

&lt;video width=&quot;648&quot; height=&quot;563&quot; controls&gt;

	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-3.mp4&quot;  type=&quot;video/mp4&quot; /&gt;
	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-3.webm&quot; type=&quot;video/webm&quot; /&gt;
	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-3.ogv&quot;  type=&quot;video/ogg&quot; /&gt;
	&lt;object width=&quot;648&quot; height=&quot;563&quot; type=&quot;application/x-shockwave-flash&quot; data=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-3.swf&quot;&gt;
		&lt;param name=&quot;movie&quot; value=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-3.swf&quot; /&gt;
		&lt;param name=&quot;flashvars&quot; value=&quot;controlbar=over&amp;amp;image=http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-3-thumb.jpg&amp;amp;file=http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-3.mp4&quot; /&gt;
		&lt;img src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-3-thumb.jpg&quot; width=&quot;648&quot; height=&quot;563&quot; alt=&quot;Web analytics with Tableau and SnowPlow introductory video&quot;
		     title=&quot;No video playback capabilities, please download the video below&quot; /&gt;
	&lt;/object&gt;
&lt;/video&gt;

&lt;p class=&quot;note&quot;&gt;&lt;i&gt;Having trouble viewing the video above? You may download the videos in your format of choice:&lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-3.mp4&quot;&gt;&quot;MP4&quot;&lt;/a&gt;, &lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-3.ogv&quot;&gt;&quot;Ogg&quot;&lt;/a&gt; or &lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-3.webm&quot;&gt;WebM&lt;/a&gt; formats.&lt;/i&gt;&lt;/p&gt;

&lt;p&gt;In the fourth video, we analyse improvements in conversion rates over time for the retailer. This is a core measure to track in order to understand how improvements to the website and marketing strategy drive increased conversion rates. Again, this is something not supported by Google Analytics out of the box. We show how easy it is with SnowPlow and Tableau to identify trends in conversion rates over time:&lt;/p&gt;

&lt;video width=&quot;648&quot; height=&quot;563&quot; controls&gt;

	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-4.mp4&quot;  type=&quot;video/mp4&quot; /&gt;
	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-4.webm&quot; type=&quot;video/webm&quot; /&gt;
	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-4.ogv&quot;  type=&quot;video/ogg&quot; /&gt;
	&lt;object width=&quot;648&quot; height=&quot;563&quot; type=&quot;application/x-shockwave-flash&quot; data=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-4.swf&quot;&gt;
		&lt;param name=&quot;movie&quot; value=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-4.swf&quot; /&gt;
		&lt;param name=&quot;flashvars&quot; value=&quot;controlbar=over&amp;amp;image=http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-4-thumb.jpg&amp;amp;file=http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-4.mp4&quot; /&gt;
		&lt;img src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-4-thumb.jpg&quot; width=&quot;648&quot; height=&quot;563&quot; alt=&quot;Web analytics with Tableau and SnowPlow introductory video&quot;
		     title=&quot;No video playback capabilities, please download the video below&quot; /&gt;
	&lt;/object&gt;
&lt;/video&gt;

&lt;p class=&quot;note&quot;&gt;&lt;i&gt;Having trouble viewing the video above? You may download the videos in your format of choice:&lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-4.mp4&quot;&gt;&quot;MP4&quot;&lt;/a&gt;, &lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-4.ogv&quot;&gt;&quot;Ogg&quot;&lt;/a&gt; or &lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-4.webm&quot;&gt;WebM&lt;/a&gt; formats.&lt;/i&gt;&lt;/p&gt;

&lt;p&gt;In the fifth video, we show how to visualise patterns of individual user visits over time. This is an interesting starting point to begin to unpick the patterns that make up successful user engagement:&lt;/p&gt;

&lt;video width=&quot;648&quot; height=&quot;563&quot; controls&gt;

	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-5.mp4&quot;  type=&quot;video/mp4&quot; /&gt;
	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-5.webm&quot; type=&quot;video/webm&quot; /&gt;
	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-5.ogv&quot;  type=&quot;video/ogg&quot; /&gt;
	&lt;object width=&quot;648&quot; height=&quot;563&quot; type=&quot;application/x-shockwave-flash&quot; data=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-5.swf&quot;&gt;
		&lt;param name=&quot;movie&quot; value=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-5.swf&quot; /&gt;
		&lt;param name=&quot;flashvars&quot; value=&quot;controlbar=over&amp;amp;image=http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-5-thumb.jpg&amp;amp;file=http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-5.mp4&quot; /&gt;
		&lt;img src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-5-thumb.jpg&quot; width=&quot;648&quot; height=&quot;563&quot; alt=&quot;Web analytics with Tableau and SnowPlow introductory video&quot;
		     title=&quot;No video playback capabilities, please download the video below&quot; /&gt;
	&lt;/object&gt;
&lt;/video&gt;

&lt;p class=&quot;note&quot;&gt;&lt;i&gt;Having trouble viewing the video above? You may download the videos in your format of choice:&lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-5.mp4&quot;&gt;&quot;MP4&quot;&lt;/a&gt;, &lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-5.ogv&quot;&gt;&quot;Ogg&quot;&lt;/a&gt; or &lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-5.webm&quot;&gt;WebM&lt;/a&gt; formats.&lt;/i&gt;&lt;/p&gt;

&lt;p&gt;In the sixth video, we show how to visualise the range of product pages visited by each user. This can help us to understand how successful the retailer is at driving users interested in one product to consider buying other products (cross-selling), and onwards to developing recommendation algorithms (users who liked &lt;i&gt;this&lt;/i&gt; product also liked &lt;i&gt;this&lt;/i&gt; product):&lt;/p&gt;

&lt;video width=&quot;648&quot; height=&quot;563&quot; controls&gt;

	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-6.mp4&quot;  type=&quot;video/mp4&quot; /&gt;
	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-6.webm&quot; type=&quot;video/webm&quot; /&gt;
	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-6.ogv&quot;  type=&quot;video/ogg&quot; /&gt;
	&lt;object width=&quot;648&quot; height=&quot;563&quot; type=&quot;application/x-shockwave-flash&quot; data=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-6.swf&quot;&gt;
		&lt;param name=&quot;movie&quot; value=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-6.swf&quot; /&gt;
		&lt;param name=&quot;flashvars&quot; value=&quot;controlbar=over&amp;amp;image=http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-6-thumb.jpg&amp;amp;file=http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-6.mp4&quot; /&gt;
		&lt;img src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-6-thumb.jpg&quot; width=&quot;648&quot; height=&quot;563&quot; alt=&quot;Web analytics with Tableau and SnowPlow introductory video&quot;
		     title=&quot;No video playback capabilities, please download the video below&quot; /&gt;
	&lt;/object&gt;
&lt;/video&gt;

&lt;p class=&quot;note&quot;&gt;&lt;i&gt;Having trouble viewing the video above? You may download the videos in your format of choice:&lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-6.mp4&quot;&gt;&quot;MP4&quot;&lt;/a&gt;, &lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-6.ogv&quot;&gt;&quot;Ogg&quot;&lt;/a&gt; or &lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-6.webm&quot;&gt;WebM&lt;/a&gt; formats.&lt;/i&gt;&lt;/p&gt;

&lt;p&gt;In the final video in the series, we perform an example cohort analysis, with a view to understanding how 'sticky' the online retailer site is, and how its stickiness has improved over time. In this example, we use &lt;i&gt;stickiness&lt;/i&gt; to refer to how good the website is at driving repeat visits:&lt;/p&gt;

&lt;video width=&quot;648&quot; height=&quot;563&quot; controls&gt;

	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-7.mp4&quot;  type=&quot;video/mp4&quot; /&gt;
	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-7.webm&quot; type=&quot;video/webm&quot; /&gt;
	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-7.ogv&quot;  type=&quot;video/ogg&quot; /&gt;
	&lt;object width=&quot;648&quot; height=&quot;563&quot; type=&quot;application/x-shockwave-flash&quot; data=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-7.swf&quot;&gt;
		&lt;param name=&quot;movie&quot; value=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-7.swf&quot; /&gt;
		&lt;param name=&quot;flashvars&quot; value=&quot;controlbar=over&amp;amp;image=http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-7-thumb.jpg&amp;amp;file=http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-7.mp4&quot; /&gt;
		&lt;img src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-7-thumb.jpg&quot; width=&quot;648&quot; height=&quot;563&quot; alt=&quot;Web analytics with Tableau and SnowPlow introductory video&quot;
		     title=&quot;No video playback capabilities, please download the video below&quot; /&gt;
	&lt;/object&gt;
&lt;/video&gt;

&lt;p class=&quot;note&quot;&gt;&lt;i&gt;Having trouble viewing the video above? You may download the videos in your format of choice:&lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-7.mp4&quot;&gt;&quot;MP4&quot;&lt;/a&gt;, &lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-7.ogv&quot;&gt;&quot;Ogg&quot;&lt;/a&gt; or &lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-7.webm&quot;&gt;WebM&lt;/a&gt; formats.&lt;/i&gt;&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/10/21/infobright-ruby-loader-released</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/10/21/infobright-ruby-loader-released"/>
    <title>Infobright Ruby Loader Released</title>
    <updated>2012-10-21T00:00:00+01:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We&amp;#8217;re pleased to start the week with the release of a new Ruby gem, our &lt;a href='https://github.com/snowplow/infobright-ruby-loader'&gt;Infobright Ruby Loader&lt;/a&gt; (IRL).&lt;/p&gt;

&lt;p&gt;At SnowPlow we&amp;#8217;re committed to supporting multiple different storage and analytics options for SnowPlow events, alongside our current Hive-based approach. One of the alternative data stores we are working with is &lt;a href='http://www.infobright.org/'&gt;Infobright&lt;/a&gt;, a columnar database which is available in open source and commercial versions.&lt;/p&gt;

&lt;p&gt;For all but the largest SnowPlow users, columnar databases such as Infobright should be an attractive alternative to doing all of your analysis in Hive. The main advantages of columnar databases are as follows:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Scale to terabytes (although not petabytes, unlike Hive)&lt;/li&gt;

&lt;li&gt;Fixed cost (dedicated RAM-heavy analytics server), versus pay-as-you-go querying on Amazon EMR&lt;/li&gt;

&lt;li&gt;Significantly faster query times – typically seconds, not minutes&lt;/li&gt;

&lt;li&gt;Plug in to many analytics front ends e.g. Tableau, Qlikview, R&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;So, open source columnar databases like Infobright Community Edition (ICE) are a good fit for SnowPlow analytics. Unfortunately, when we started to load SnowPlow event logs into ICE, we realised that there wasn&amp;#8217;t a good data-loading solution for Infobright in Ruby, our ETL language of choice. So, we built one :-)&lt;/p&gt;

&lt;p&gt;Our freshly minted &lt;a href='https://github.com/snowplow/infobright-ruby-loader'&gt;Infobright Ruby Loader&lt;/a&gt; (IRL) can be used in two different ways:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;As a command-line tool&lt;/strong&gt; - for manual loading of data into Infobright at the command-line. No Ruby expertise required&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;As part of another application&lt;/strong&gt; - because it&amp;#8217;s a Ruby gem with a Ruby API, IRL can be integrated into larger Ruby ETL processes&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We will be using IRL at SnowPlow as part of our larger ETL process to load SnowPlow events into ICE for analysis - we hope to roll this out within the next few weeks.&lt;/p&gt;

&lt;p&gt;In the meantime, we hope that IRL is useful to people in the Infobright community who need to run data loads at the command-line; IRL was inspired by &lt;a href='http://www.infobright.org/Blog/Entry/unscripted/'&gt;ParaFlex&lt;/a&gt;, an excellent Bash script from the Infobright team to perform parallel loading of Infobright, and can be used as a direct alternative to ParaFlex.&lt;/p&gt;

&lt;p&gt;To find out more about our Infobright Ruby Loader, please check out the detailed &lt;a href='https://github.com/snowplow/infobright-ruby-loader/blob/master/README.md'&gt;README&lt;/a&gt; in the GitHub repository. And please direct any questions through the &lt;a href='https://github.com/snowplow/snowplow/wiki/Talk-to-us'&gt;usual channels&lt;/a&gt;!&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/10/12/how-the-role-of-hive-is-changing-at-snowplow</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/10/12/how-the-role-of-hive-is-changing-at-snowplow"/>
    <title>How we use Hive at SnowPlow, and how the role of Hive is changing. (Slides from our presentation to Hive London.)</title>
    <updated>2012-10-12T00:00:00+01:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;Last night I gave a presentation to the clever folks at Hive London covering three things:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;How big data technologies like Apache Hive are transforming web analytics&lt;/li&gt;

&lt;li&gt;Howe we&amp;#8217;ve used Hive in SnowPlow development&lt;/li&gt;

&lt;li&gt;How the role of Hive has changed at SnowPlow over time, including a comparison of Hive against other technologies.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The slides from the presentation are below. As always, any questions / comments, please post them below.&lt;/p&gt;
&lt;iframe marginwidth='0' frameborder='0' marginheight='0' style='border:1px solid #CCC;border-width:1px 1px 0;margin-bottom:5px' width='427' height='356' scrolling='no' src='http://www.slideshare.net/slideshow/embed_code/14696456'&gt;  &lt;/iframe&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/10/11/snowplow-0.4.10-released</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/10/11/snowplow-0.4.10-released"/>
    <title>SnowPlow 0.4.10 released</title>
    <updated>2012-10-11T00:00:00+01:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We have just released version &lt;strong&gt;0.4.10&lt;/strong&gt; of SnowPlow - people using 0.4.8 can jump straight to this version. This version updates:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;snowplow.js to version 0.7.0&lt;/li&gt;

&lt;li&gt;the Hive deserializer to version 0.4.9&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Big thanks to community members &lt;a href='https://github.com/mtibben'&gt;Michael Tibben&lt;/a&gt; from &lt;a href='http://99designs.com'&gt;99designs&lt;/a&gt; and &lt;a href='https://github.com/ramn'&gt;Simon Andersson&lt;/a&gt; from &lt;a href='http://www.qwaya.com'&gt;Qwaya&lt;/a&gt; for their most-helpful contributions to this release!&lt;/p&gt;

&lt;h2 id='main_changes'&gt;Main changes&lt;/h2&gt;

&lt;p&gt;The main changes are as follows:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The querystring parameter for site ID which the JavaScript tracker sends to your collector is renamed from &lt;code&gt;said&lt;/code&gt; to &lt;code&gt;aid&lt;/code&gt;&lt;/li&gt;

&lt;li&gt;The Hive-based ETL process now extracts the ecommerce tracking fields and the site ID field and adds them into your processed events table&lt;/li&gt;

&lt;li&gt;We fixed a bug in the Hive deserializer where a partially-processed row was returned even if a fatal error was found in the row (now, a null row is returned instead)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The rest of the changes were all enhancements to the Hive deserializer&amp;#8217;s Specs2 test suite - these improvements should help to accelerate work on the deserializer (we have lots of cool new stuff we want to add to the deserializer!). &lt;!--more--&gt;&lt;/p&gt;

&lt;h2 id='new_event_table_fields'&gt;New event table fields&lt;/h2&gt;

&lt;p&gt;The new fields in the event table all relate directly to additional tracking functionality which was added to the JavaScript tracker in &lt;a href='/blog/2012/09/06/snowplow-0.4.7-released/'&gt;SnowPlow 0.4.7&lt;/a&gt;. Specifically:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The &lt;code&gt;setSiteId()&lt;/code&gt; functionality is now extracted to the &lt;code&gt;app_id&lt;/code&gt; field (short for application ID)&lt;/li&gt;

&lt;li&gt;The ecommerce tracking functionality is now extracted to a set of &lt;code&gt;tr_&lt;/code&gt; and &lt;code&gt;ti_&lt;/code&gt; fields&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For details on the new fields, please review our latest &lt;a href='/analytics/snowplow-table-structure.html'&gt;Hive events table definition&lt;/a&gt; - there is now a column indicating in which version a given field was added.&lt;/p&gt;

&lt;h2 id='how_to_get_the_new_version'&gt;How to get the new version&lt;/h2&gt;

&lt;p&gt;As usual, the new version of the Hive deserializer is available from the GitHub repository&amp;#8217;s &lt;a href='https://github.com/snowplow/snowplow/downloads'&gt;Downloads&lt;/a&gt; section as &lt;strong&gt;snowplow-log-deserializers-0.4.9.jar&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The updated snowplow.js is &lt;a href='https://raw.github.com/snowplow/snowplow/master/1-trackers/javascript-tracker/js/snowplow.js'&gt;available in our GitHub repository&lt;/a&gt; for you to minify and upload, or alternatively you can use the one on our CDN:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;https://d1fc8wv8zag5ca.cloudfront.net/0.7.0/sp.js&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you have any problems with either of these components, please &lt;a href='https://github.com/snowplow/snowplow/issues'&gt;raise an issue&lt;/a&gt;!&lt;/p&gt;

&lt;h2 id='a_note_on_backwards_compatibility_for_the_events_table'&gt;A note on backwards compatibility for the events table&lt;/h2&gt;

&lt;p&gt;We will continue to add extra fields to the SnowPlow events table as we add extra capabilities to the ETL process - for example, we are working on functionality to extract geo-location information from IP addresses via MaxMind.&lt;/p&gt;

&lt;p&gt;Starting with our new &lt;code&gt;app_id&lt;/code&gt; field, we will be adding all such new fields to the &lt;strong&gt;end&lt;/strong&gt; of our Hive events table definition. This will mean that you will &lt;strong&gt;not&lt;/strong&gt; have to re-run the ETL process across all your historic raw logs, provided you do &lt;strong&gt;not&lt;/strong&gt; need the data found in the new fields. This is because a Hive query across both the old event table format and the new table format works as long as you don&amp;#8217;t explicitly query a new field.&lt;/p&gt;

&lt;p&gt;In other words, Hive is futureproofed against new fields being added to the end of your underlying data files, and we&amp;#8217;ll take advantage of this to improve backwards compatibility for our events table!&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/10/11/attlib-0.0.1-released</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/10/11/attlib-0.0.1-released"/>
    <title>Attlib - an open source library for extracting search marketing attribution data from referrer URLs</title>
    <updated>2012-10-11T00:00:00+01:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;&lt;strong&gt;Update 17-Dec-12&lt;/strong&gt;: We have renamed Attlib to &lt;a href='https://github.com/snowplow/referer-parser'&gt;referer-parser&lt;/a&gt;, to make it clearer what Attlib does: parse referer URLs. The repository has been updated accordingly. Some of the example code below is out-of-date now: we recommend checking out the &lt;a href='https://github.com/snowplow/referer-parser'&gt;repository&lt;/a&gt; for more information.&lt;/p&gt;

&lt;p&gt;Last night we published &lt;a href='https://github.com/snowplow/referer-parser'&gt;Attlib&lt;/a&gt;, an open source Ruby library for extracting search marketing attribution data from referer (sic) URLs. In this post we talk through:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href='#what_attlib_does'&gt;What Attlib does, and how to use it&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='#install'&gt;Installing Attlib&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='#search_engine_yaml'&gt;The search_engine.yml file&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='#snowplow_stack'&gt;Attlib as part of the SnowPlow stack&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='#other_languages'&gt;Attlib in other languages&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='#snowplow_components_as_standalone_projects'&gt;Making components of SnowPlow available as standalone open source projects&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;a name='what_attlib_does' /&gt;
&lt;h3 id='what_attlib_does_and_how_to_use_it'&gt;What Attlib does, and how to use it&lt;/h3&gt;

&lt;p&gt;Attlib is straightforward Ruby library for extracting seach marketing attribution data from referrer URLs. You give it a referer URL to parse: it then lets you now whether the URL is from a search engine. If it is, it will tell you which search engine it is, and what keywords were typed. (If those keywords are included in the query string - this is no longer the case for users logged in to Google, as documented &lt;a href='http://googlewebmastercentral.blogspot.co.uk/2011/10/accessing-search-query-data-for-your.html'&gt;here&lt;/a&gt;.)&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='ruby'&gt;&lt;span class='nb'&gt;require&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;attlib&amp;#39;&lt;/span&gt;

&lt;span class='n'&gt;r&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='no'&gt;Referrer&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;new&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;http://images.google.ca/imgres?q=hermetic+tarot&amp;amp;hl=en&amp;amp;biw=1189&amp;amp;bih=521&amp;amp;tbm=isch&amp;amp;tbnid=BuQ_IyUbc25usM:&amp;amp;imgrefurl=http://www.psychicbazaar.com/tarot-cards/15-the-hermetic-tarot.html&amp;amp;imgurl=http://mdm.pbzstatic.com/tarot/the-hermetic-tarot/card-4.png&amp;amp;w=1064&amp;amp;h=1551&amp;amp;ei=ue9AUMe7Osn9iwLZ-4H4Dw&amp;amp;zoom=1&amp;amp;iact=hc&amp;amp;vpx=107&amp;amp;vpy=48&amp;amp;dur=2477&amp;amp;hovh=271&amp;amp;hovw=186&amp;amp;tx=133&amp;amp;ty=157&amp;amp;sig=115588264602219115047&amp;amp;page=4&amp;amp;tbnh=162&amp;amp;tbnw=120&amp;amp;start=57&amp;amp;ndsp=19&amp;amp;ved=1t:429,r:12,s:57,i:291&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt;

&lt;span class='n'&gt;r&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;is_search_engine?&lt;/span&gt; &lt;span class='c1'&gt;# True&lt;/span&gt;
&lt;span class='n'&gt;r&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;search_engine&lt;/span&gt; &lt;span class='c1'&gt;# &amp;#39;Google Images&amp;#39;&lt;/span&gt;
&lt;span class='n'&gt;r&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;keywords&lt;/span&gt; 	&lt;span class='c1'&gt;# &amp;#39;hermetic tarot&amp;#39;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;&lt;a name='install' /&gt;
&lt;h3 id='installing_attlib'&gt;Installing Attlib&lt;/h3&gt;

&lt;p&gt;Attlib is available via a Ruby Gem. To install, simply run the following at the command line:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo gem install attlib&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The sourcecode is available on &lt;a href='https://github.com/snowplow/referer-parser'&gt;Github&lt;/a&gt;&lt;/p&gt;
&lt;a name='search_engine_yaml' /&gt;
&lt;h3 id='the_search_enginesyml_file'&gt;The search_engines.yml file&lt;/h3&gt;

&lt;p&gt;Extracting search engine names and keywords from a referer URL is pretty straightforward. What is more complicated is keeping track of the myriad search engines that are out there, operating in different countries, the myriad domains they operate on, and the different query parameters that each of them uses to store the keywords.&lt;/p&gt;

&lt;p&gt;Because the space is constantly evolving, none of this information (about search engines, parameters and domains) has been hard coded into Attlib. All of it is available in the &lt;a href='https://github.com/snowplow/referer-parser/blob/master/search.yml'&gt;search_engines.yml&lt;/a&gt; file, in the &lt;a href='https://github.com/snowplow/attlib/tree/master'&gt;data&lt;/a&gt; in the repo. &lt;!--more--&gt;&lt;/p&gt;

&lt;p&gt;The structure of the YAML file should be straightforward to understand. Each search engine is a top level item. For each search engine, two lists are given: one is a list of parameters used in that search engine&amp;#8217;s query string to identify the keywords entered. The other is the list of domains on which that search engine operates. An extract is shown below:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='yaml'&gt;&lt;span class='l-Scalar-Plain'&gt;Babylon&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt;
  &lt;span class='l-Scalar-Plain'&gt;parameters&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; 
    &lt;span class='p-Indicator'&gt;-&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;q&lt;/span&gt;
  &lt;span class='l-Scalar-Plain'&gt;domains&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; 
   &lt;span class='p-Indicator'&gt;-&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;search.babylon.com&lt;/span&gt;
   &lt;span class='p-Indicator'&gt;-&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;searchassist.babylon.com&lt;/span&gt;

&lt;span class='l-Scalar-Plain'&gt;Baidu&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt;
  &lt;span class='l-Scalar-Plain'&gt;parameters&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; 
    &lt;span class='p-Indicator'&gt;-&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;wd&lt;/span&gt;
    &lt;span class='p-Indicator'&gt;-&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;word&lt;/span&gt;
    &lt;span class='p-Indicator'&gt;-&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;kw&lt;/span&gt;
    &lt;span class='p-Indicator'&gt;-&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;k&lt;/span&gt;
  &lt;span class='l-Scalar-Plain'&gt;domains&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt;
    &lt;span class='p-Indicator'&gt;-&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;www.baidu.com&lt;/span&gt;
    &lt;span class='p-Indicator'&gt;-&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;www1.baidu.com&lt;/span&gt;
    &lt;span class='p-Indicator'&gt;-&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;zhidao.baidu.com&lt;/span&gt;
    &lt;span class='p-Indicator'&gt;-&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;tieba.baidu.com&lt;/span&gt;
    &lt;span class='p-Indicator'&gt;-&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;news.baidu.com&lt;/span&gt;
    &lt;span class='p-Indicator'&gt;-&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;web.gougou.com&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;Keeping this file up to date is a big job: one of our hopes releasing Attlib as an open source, standalone library, is that the community contributes to the file. We are enormously grateful to our friends at &lt;a href='http://piwik.org/'&gt;Piwik&lt;/a&gt; as our initial version of the file is based on the Piwik equivalent &lt;a href='https://github.com/piwik/piwik/blob/master/core/DataFiles/SearchEngines.php'&gt;SearchEngines.php&lt;/a&gt;, for the hard work they put into this version.&lt;/p&gt;
&lt;a name='snowplow_stack' /&gt;
&lt;h3 id='attlib_as_part_of_the_snowplow_stack'&gt;Attlib as part of the SnowPlow stack&lt;/h3&gt;

&lt;p&gt;Our intention is to port &lt;a href='https://github.com/snowplow/referer-parser'&gt;Attlib&lt;/a&gt; into Scala and integrate it into the SnowPlow stack: specifically the ETL phase. Both Ruby and Scala versions of Attlib will run based on the same &lt;a href='https://github.com/snowplow/referer-parser/blob/master/search.yml'&gt;search_engines.yml&lt;/a&gt; file.&lt;/p&gt;
&lt;a name='other_languages' /&gt;
&lt;h3 id='attlib_in_other_languages'&gt;Attlib in other languages&lt;/h3&gt;

&lt;p&gt;As well as contributing to the search &lt;a href='https://github.com/snowplow/referer-parser/blob/master/search.yml'&gt;search_engines.yml&lt;/a&gt; file, we also hope that community members will develop versions of Attlib in other languages e.g. Python.&lt;/p&gt;
&lt;a name='snowplow_components_as_standalone_projects' /&gt;
&lt;h3 id='making_components_of_snowplow_available_as_standalone_open_source_projects'&gt;Making components of SnowPlow available as standalone open source projects&lt;/h3&gt;

&lt;p&gt;Attlib is the first component in the SnowPlow stack that we have released as a standalone library. There are many more in the pipeline. (More on this in future blog posts :-) ). For us, this is a key part of the SnowPlow strategy:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Keeping the SnowPlow architecture as loosely coupled as possible. We believe this makes SnowPlow robust, scalable and extendable&lt;/li&gt;

&lt;li&gt;Grow the userbase of people using and contributing to each component. Processing web analytics data is a big job: there are many individual components involved, and each of them needs to evolve with the changing marketplace. Attlib is concerned today with extracting useful data from search engine referrers: but it is likely that as time goes on, we&amp;#8217;ll want to extend it to capture data from other types of referrers e.g. social networks or affiliate sites. The bigger the community of people on top of those developments, the better for everyone in the web analytics community. Releasing each component as a standalone open source library should help grow that community.&lt;/li&gt;
&lt;/ol&gt;
&lt;hr /&gt;
&lt;p&gt;Any questions about Attlib, or anything else in this post? Then &lt;a href='/contact/index.html'&gt;get in touch&lt;/a&gt; with the SnowPlow team.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/09/24/what-does-snowplow-let-you-do</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/09/24/what-does-snowplow-let-you-do"/>
    <title>Why set your data free?</title>
    <updated>2012-09-24T00:00:00+01:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;At Saturday&amp;#8217;s &lt;a href='http://ukdaa.co.uk/'&gt;Measure Camp&lt;/a&gt;, I had the chance to introduce SnowPlow to a large number of some incredibly thoughtful and insightful people in the web analytics industry.&lt;/p&gt;

&lt;p&gt;With each person, I started by explaining that SnowPlow gave them direct access to their customer-level and event-level data. The response I got in nearly all cases was: &lt;strong&gt;what does having direct access to my web analytics data enable me to do, that I can&amp;#8217;t do with Google Analytics / Omniture?&lt;/strong&gt; It&amp;#8217;s such a good question I thought I should publish an answer below:&lt;/p&gt;

&lt;h3 id='1_integrate_web_analytics_data_with_other_data_sources'&gt;1. Integrate web analytics data with other data sources&lt;/h3&gt;

&lt;p&gt;Integrating your web analytics data with other data sets enables you to answer a wide range of valuable business questions:&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;&lt;strong&gt;Data source&lt;/strong&gt;&lt;/th&gt;&lt;th&gt;&lt;strong&gt;Example business questions&lt;/strong&gt;&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Marketing spend data e.g. AdWords, ad server data&lt;/td&gt;&lt;td style='text-align: left;'&gt;What is the return on my ad spend? How should I optimize my return on ad spend&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Customer data e.g. CRM, loyalty&lt;/td&gt;&lt;td style='text-align: left;'&gt;How does the online behaviour of my differnet customer segments vary by segment? Do online promotions drive offline sales? (Or vice versa?)&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Product / media catalogue data&lt;/td&gt;&lt;td style='text-align: left;'&gt;What are my most profitable product lines? Do different types of products attract different customer segments? What are the products that drive the most visits?&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;SnowPlow makes integrating web analytics data with other data sources easier in a two ways:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;All your SnowPlow data is directly accessible in Apache Hive or Infobright. (So no expensive export process is required, prior to linking the data sets.)&lt;/li&gt;

&lt;li&gt;Custom variables and event tracking give you plenty of opportunity to join e.g. customer IDs or campaigns names to enable &lt;code&gt;JOIN&lt;/code&gt;s across data set&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For more details on how to perform &lt;code&gt;JOIN&lt;/code&gt;s between SnowPlow data and other sources, see refer to the guide to &lt;a href='/analytics/customer-analytics/joining-customer-data.html'&gt;joining SnowPlow engagement data with other sources of customer data&lt;/a&gt;&lt;/p&gt;

&lt;h3 id='2_slice_and_dice_your_data_by_any_combination_of_dimensions__metrics_you_want'&gt;2. Slice and dice your data by any combination of dimensions / metrics you want&lt;/h3&gt;

&lt;p&gt;Google Analytics in particular only lets users create reports about of set combinations of dimensions and metrics. Examples of combinations that are &lt;strong&gt;not supported&lt;/strong&gt; include:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Number of unique visitors by product page&lt;/li&gt;

&lt;li&gt;Different sources of traffic by product page (and how this changes over time)&lt;/li&gt;

&lt;li&gt;Engagement levels (e.g. number of visits, number of page views, conversion rates) by traffic source&lt;/li&gt;

&lt;li&gt;Improvements to conversion rates over time&lt;/li&gt;
&lt;/ol&gt;
&lt;!--more--&gt;
&lt;p&gt;In contrast, because SnowPlow gives you access to the underlying data, it is possible to use BI tools like &lt;a href='http://www.tableausoftware.com/'&gt;Tableau&lt;/a&gt; and &lt;a href='http://www.microsoft.com/en-us/bi/powerpivot.aspx'&gt;PowerPivot&lt;/a&gt; to quickly slice and dice web analytics data by any dimensions / metrics you want. We&amp;#8217;ll be posting examples of how to do this in the next few days.&lt;/p&gt;

&lt;h3 id='3_use_machine_learning_tools_on_your_web_analytics_data'&gt;3. Use machine learning tools on your web analytics data&lt;/h3&gt;

&lt;p&gt;Machine learning tools, and &lt;a href='http://mahout.apache.org/'&gt;Mahout&lt;/a&gt; in particular, have created some new and exciting opportunities to:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Develop product and content recommendation engines, based on user web behaviour. (E.g. users who viewed these content items, also viewed&amp;#8230;)&lt;/li&gt;

&lt;li&gt;Segment your audience by online behaviour&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;SnowPlow makes it easy to extract the core input data you would need to feed a machine learning algorithm in a single query. (E.g. a matrix mapping users to products by page views / add to baskets / purchases etc.) We will be exploring ways to integrate SnowPlow with &lt;a href='http://mahout.apache.org/'&gt;Mahout&lt;/a&gt; in a future blog post.&lt;/p&gt;

&lt;h3 id='4_view_data_for_individual_users_over_their_entire_lives'&gt;4. View data for individual users over their entire lives&lt;/h3&gt;

&lt;p&gt;Whereas reports on Google Analytics tend to be about visits, page views or transactions, SnowPlow lets you slice data by users over multiple visits, opening up a wide range of possibilities:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Develop accurate models of customer lifetime value&lt;/li&gt;

&lt;li&gt;Develop more rigorous approaches to attribution modelling, by capturing in granular detail which channels touched a user at different points in their lifecycle&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id='5_interested_in_any__all_of_the_above'&gt;5. Interested in any / all of the above?&lt;/h3&gt;

&lt;p&gt;Then &lt;a href='/product/get-started.html'&gt;get started&lt;/a&gt; with SnowPlow, or &lt;a href='/contact/index.html'&gt;get in touch&lt;/a&gt; to find out more!&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/09/14/snowplow-0.4.8-released</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/09/14/snowplow-0.4.8-released"/>
    <title>SnowPlow 0.4.8 released</title>
    <updated>2012-09-14T00:00:00+01:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We have just released SnowPlow version &lt;strong&gt;0.4.8&lt;/strong&gt;, with a set of enhancements to the existing Hive deserializer:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The Hive deserializer now supports Amazon&amp;#8217;s new CloudFront log file format (launched 12 September 2012) as well as the older format&lt;/li&gt;

&lt;li&gt;The Hive deserializer now supports a tracking pixel called simply &lt;code&gt;i&lt;/code&gt; (saving some characters versus &lt;code&gt;ice.png&lt;/code&gt;) (&lt;a href='https://github.com/snowplow/snowplow/issues/35'&gt;issue #35&lt;/a&gt;)&lt;/li&gt;

&lt;li&gt;The Hive deserializer now works if the CloudFront distribution has Forward Query String = yes (&lt;a href='https://github.com/snowplow/snowplow/pull/39'&gt;issue #39&lt;/a&gt;)&lt;/li&gt;

&lt;li&gt;The Hive deserializer no longer dies if the calling page&amp;#8217;s querystring is malformed&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Many thanks to community member &lt;a href='https://github.com/mtibben'&gt;Michael Tibben&lt;/a&gt; from &lt;a href='http://99designs.com'&gt;99designs&lt;/a&gt; in Melbourne for contributing the Forward Query String = yes fix!&lt;/p&gt;

&lt;h2 id='new_cloudfront_log_file_format'&gt;New CloudFront log file format&lt;/h2&gt;

&lt;p&gt;On 12th September 2012, Amazon &lt;a href='http://aws.amazon.com/about-aws/whats-new/2012/09/04/cloudfront-support-for-cookies-and-price-classes/'&gt;rolled out a new CloudFront log file format&lt;/a&gt;, adding three additional fields onto the end of each line:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;cs(Cookie)&lt;/strong&gt;, the cookie header in the request (if any). Logging of this field is optional.&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;x-edge-result-type&lt;/strong&gt;, the result type of each HTTP(s) request (for example, cache hit/miss/error).&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;x-edge-request-id&lt;/strong&gt;, an encrypted string that uniquely identifies a request to help AWS troubleshoot/debug any issues.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As always, please consult the Amazon CloudFront &lt;a href='http://docs.amazonwebservices.com/AmazonCloudFront/latest/DeveloperGuide/AccessLogs.html#LogFileFormat'&gt;Developer Guide&lt;/a&gt; for more information on these fields. &lt;!--more--&gt;&lt;/p&gt;

&lt;p&gt;As part of this new &lt;strong&gt;0.4.8&lt;/strong&gt; SnowPlow release, the Hive deserializer now supports the new CloudFront format as well as the old format: if you deploy the latest version of the deserializer, you should be able to process both old-format and new-format CloudFront logs without issue.&lt;/p&gt;

&lt;h2 id='support_for__as_the_tracking_pixel'&gt;Support for &lt;code&gt;i&lt;/code&gt; as the tracking pixel&lt;/h2&gt;

&lt;p&gt;Currently the SnowPlow JavaScript tracker fires a GET request to a tracking pixel called &lt;code&gt;ice.png&lt;/code&gt;. This works fine, but it makes more sense to call the pixel &lt;code&gt;i&lt;/code&gt;, for two reasons:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;We free up 5 extra characters to use for sending data&lt;/li&gt;

&lt;li&gt;A transparent GIF is smaller to send than a transparent PNG&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Thanks to &lt;a href='https://github.com/shermozle/'&gt;Simon Rumble&lt;/a&gt; (author of &lt;a href='https://github.com/shermozle/SnowCannon'&gt;SnowCannon&lt;/a&gt;) for pointing this out! In due course we will update the JavaScript tracker and CloudFront collector to implement this change (see issues &lt;a href='https://github.com/snowplow/snowplow/issues/29'&gt;#29&lt;/a&gt; and &lt;a href='https://github.com/snowplow/snowplow/issues/25'&gt;#25&lt;/a&gt;), but to start off we have added support for &lt;code&gt;i&lt;/code&gt; to the new version of the Hive deserializer.&lt;/p&gt;

&lt;p&gt;This is a small change, but highlights a wider point for SnowPlow development: in general, whenever we have a &amp;#8220;breaking change&amp;#8221; coming upstream, we will try to prepare for this change downstream first, to prevent any disruption to your use of SnowPlow.&lt;/p&gt;

&lt;h2 id='support_for_forward_query_string__yes'&gt;Support for Forward Query String = yes&lt;/h2&gt;

&lt;p&gt;Thanks to &lt;a href='https://github.com/mtibben'&gt;Michael Tibben&lt;/a&gt; from &lt;a href='http://99designs.com'&gt;99designs&lt;/a&gt; for spotting that the Hive deserializer does not work if your CloudFront distribution has Forward Query String set to Yes; Michael not only raised the issue but also provided a fix, many thanks Michael!&lt;/p&gt;

&lt;p&gt;Most SnowPlow users will have Forward Query String in their CloudFront distribution set to No, so this issue will not arise for them; however this fix will be invaluable for anyone who does have it set to Yes. If you want to read more about this, please check out &lt;a href='https://github.com/snowplow/snowplow/pull/39'&gt;issue #39&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We&amp;#8217;re aware that our guide for setting up the CloudFront distribution is a bit out of date (which is how this issue can arise) - we will be refreshing the tracking pixel guide soon (&lt;a href='https://github.com/snowplow/snowplow/issues/25'&gt;issue #25&lt;/a&gt;)! Many thanks for your patience.&lt;/p&gt;

&lt;h2 id='more_robust_querystring_handling'&gt;More robust querystring handling&lt;/h2&gt;

&lt;p&gt;A small change - we have made the code for extracting marketing attribution more robust. Specifically, the Hive deserializer no longer dies (i.e. throws a non-recoverable &lt;code&gt;SerDeException&lt;/code&gt;) if the calling page&amp;#8217;s URL has a malformed querystring.&lt;/p&gt;

&lt;p&gt;An example of a malformed querystring would be something like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http://www.psychicbazaar.com/2-tarot-cards?n=48?utmsource=GoogleSearch&amp;amp;...&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note the two &lt;code&gt;?&lt;/code&gt; questionmarks (the second one should be an &lt;code&gt;&amp;amp;&lt;/code&gt; ampersand). In the case of a malformed querystring like this, the five marketing attribution fields in the Hive output format for this row will all be set to null.&lt;/p&gt;

&lt;h2 id='deploying_the_new_version'&gt;Deploying the new version&lt;/h2&gt;

&lt;p&gt;The new version of the Hive deserializer is available from the GitHub repository&amp;#8217;s &lt;a href='https://github.com/snowplow/snowplow/downloads'&gt;Downloads&lt;/a&gt; section as &lt;strong&gt;snowplow-log-deserializers-0.4.8.jar&lt;/strong&gt;. If you have any problems running it, please &lt;a href='https://github.com/snowplow/snowplow/issues'&gt;raise an issue&lt;/a&gt;!&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/09/06/snowplow-0.4.7-released</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/09/06/snowplow-0.4.7-released"/>
    <title>SnowPlow 0.4.7 released</title>
    <updated>2012-09-06T00:00:00+01:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We have just released SnowPlow version &lt;strong&gt;0.4.7&lt;/strong&gt;. This release bumps the SnowPlow JavaScript tracker to version &lt;strong&gt;0.6&lt;/strong&gt;, with two significant new features:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The ability to set a site ID for your tracking - useful for multi-site publishers&lt;/li&gt;

&lt;li&gt;The ability to log ecommerce transactions - useful for merchants wanting to track orders&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;A huge thanks to community member &lt;a href='https://github.com/ramn'&gt;Simon Andersson&lt;/a&gt; from &lt;a href='http://www.qwaya.com'&gt;Qwaya&lt;/a&gt; for contributing the ecommerce tracking functionality - thank you Simon!&lt;/p&gt;

&lt;p&gt;We&amp;#8217;ll take a look at both of these new features in turn:&lt;/p&gt;

&lt;h2 id='site_id'&gt;Site ID&lt;/h2&gt;

&lt;p&gt;The SnowPlow JavaScript tracker now lets you set a site identifier before you start logging events. The new method for this is called &lt;code&gt;setSiteId()&lt;/code&gt; - it takes one argument, the identifier you have assigned to this site. For example:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='javascript'&gt;&lt;span class='nx'&gt;_snaq&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='nx'&gt;push&lt;/span&gt;&lt;span class='p'&gt;([&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;setAccount&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;d3rkrsqld9gmqf&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;]);&lt;/span&gt;
&lt;span class='nx'&gt;_snaq&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='nx'&gt;push&lt;/span&gt;&lt;span class='p'&gt;([&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;setSiteId&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;CFe23a&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;]);&lt;/span&gt;
&lt;span class='nx'&gt;_snaq&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='nx'&gt;push&lt;/span&gt;&lt;span class='p'&gt;([&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;trackPageView&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;]);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;The querystring passed to your SnowPlow collector will now include the following parameter:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;...&amp;amp;said=CFe23a&amp;amp;...&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;where &lt;code&gt;said&lt;/code&gt; stands for &lt;em&gt;Site or App ID&lt;/em&gt; - because we plan on using the same parameter for mobile and desktop app tracking as well. &lt;!--more--&gt;&lt;/p&gt;

&lt;p&gt;This new feature should be helpful for anyone running multiple sites (or perhaps clients) against the same SnowPlow collector - it means that you can easily partition your SnowPlow events by site, whilst still being able to run cross-site analyses should you so wish.&lt;/p&gt;

&lt;p&gt;Note that we haven&amp;#8217;t yet added extracting &lt;code&gt;said&lt;/code&gt; to our ETL process, but we have an &lt;a href='https://github.com/snowplow/snowplow/issues/33'&gt;open ticket for this&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id='ecommerce_transactions'&gt;Ecommerce transactions&lt;/h2&gt;

&lt;p&gt;To date, we have been analysing e-commerce transactions using SnowPlow by:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Logging every &lt;em&gt;product add to basket&lt;/em&gt; event&lt;/li&gt;

&lt;li&gt;Logging every &lt;em&gt;product remove from basket&lt;/em&gt; event&lt;/li&gt;

&lt;li&gt;Netting these events off to determine the final contents of the order&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This approach works, but it adds complexity in the analysis step. Happily community member Simon Andersson has contributed an alternative solution: dedicated SnowPlow e-commerce transaction tracking, similar to the functionality found in the Google Analytics JavaScript API.&lt;/p&gt;

&lt;p&gt;The idea is that you add the new tracking code to your shop&amp;#8217;s checkout confirmation page, so that the completed order can be sent to SnowPlow. A complete example of the new tracking code looks like this:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='javascript'&gt;&lt;span class='kd'&gt;var&lt;/span&gt; &lt;span class='nx'&gt;orderId&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;order-123&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;;&lt;/span&gt;

&lt;span class='c1'&gt;// addTrans sets up the transaction, should be called first.&lt;/span&gt;
&lt;span class='nx'&gt;_snaq&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='nx'&gt;push&lt;/span&gt;&lt;span class='p'&gt;([&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;addTrans&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
  &lt;span class='nx'&gt;orderId&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;                &lt;span class='c1'&gt;// order ID - required&lt;/span&gt;
  &lt;span class='s1'&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;                     &lt;span class='c1'&gt;// affiliation or store name&lt;/span&gt;
  &lt;span class='s1'&gt;&amp;#39;8000&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;                 &lt;span class='c1'&gt;// total - required&lt;/span&gt;
  &lt;span class='s1'&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;                     &lt;span class='c1'&gt;// tax&lt;/span&gt;
  &lt;span class='s1'&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;                     &lt;span class='c1'&gt;// shipping&lt;/span&gt;
  &lt;span class='s1'&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;                     &lt;span class='c1'&gt;// city&lt;/span&gt;
  &lt;span class='s1'&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;                     &lt;span class='c1'&gt;// state or province&lt;/span&gt;
  &lt;span class='s1'&gt;&amp;#39;&amp;#39;&lt;/span&gt;                      &lt;span class='c1'&gt;// country&lt;/span&gt;
  &lt;span class='p'&gt;]);&lt;/span&gt;

&lt;span class='c1'&gt;// addItem is called for each item in the shopping cart.&lt;/span&gt;
&lt;span class='nx'&gt;_snaq&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='nx'&gt;push&lt;/span&gt;&lt;span class='p'&gt;([&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;addItem&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
  &lt;span class='nx'&gt;orderId&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;                &lt;span class='c1'&gt;// order ID - required&lt;/span&gt;
  &lt;span class='s1'&gt;&amp;#39;1001&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;                 &lt;span class='c1'&gt;// SKU - required&lt;/span&gt;
  &lt;span class='s1'&gt;&amp;#39;Blue t-shirt&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;         &lt;span class='c1'&gt;// product name&lt;/span&gt;
  &lt;span class='s1'&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;                     &lt;span class='c1'&gt;// category&lt;/span&gt;
  &lt;span class='s1'&gt;&amp;#39;2000&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;                 &lt;span class='c1'&gt;// unit price - required&lt;/span&gt;
  &lt;span class='s1'&gt;&amp;#39;2&amp;#39;&lt;/span&gt;                     &lt;span class='c1'&gt;// quantity - required&lt;/span&gt;
  &lt;span class='p'&gt;]);&lt;/span&gt;
&lt;span class='nx'&gt;_snaq&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='nx'&gt;push&lt;/span&gt;&lt;span class='p'&gt;([&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;addItem&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
  &lt;span class='nx'&gt;orderId&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;                &lt;span class='c1'&gt;// order ID - required&lt;/span&gt;
  &lt;span class='s1'&gt;&amp;#39;1002&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;                 &lt;span class='c1'&gt;// SKU - required&lt;/span&gt;
  &lt;span class='s1'&gt;&amp;#39;Red shoes&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;            &lt;span class='c1'&gt;// product name&lt;/span&gt;
  &lt;span class='s1'&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;                     &lt;span class='c1'&gt;// category&lt;/span&gt;
  &lt;span class='s1'&gt;&amp;#39;4000&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;                 &lt;span class='c1'&gt;// unit price - required&lt;/span&gt;
  &lt;span class='s1'&gt;&amp;#39;1&amp;#39;&lt;/span&gt;                     &lt;span class='c1'&gt;// quantity - required&lt;/span&gt;
  &lt;span class='p'&gt;]);&lt;/span&gt;

&lt;span class='c1'&gt;// trackTrans sends the transaction to SnowPlow tracking servers.&lt;/span&gt;
&lt;span class='c1'&gt;// Must be called last to commit the transaction.&lt;/span&gt;
&lt;span class='nx'&gt;_snaq&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='nx'&gt;push&lt;/span&gt;&lt;span class='p'&gt;([&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;trackTrans&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;]);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;The above example creates an order (aka &amp;#8220;transaction&amp;#8221;) with ID &lt;code&gt;order-123&lt;/code&gt; and then adds two line items (two blue t-shirts and one pair of red shoes) as line items to the order. The final &lt;code&gt;trackTrans&lt;/code&gt; call sends this complete order to SnowPlow as three separate events - one each for the order and its line items.&lt;/p&gt;

&lt;p&gt;This new functionality should be useful for anybody who wants to track orders transacted in a online shopping cart such as Magento, PrestaShop or Spree.&lt;/p&gt;

&lt;p&gt;Note that we haven&amp;#8217;t yet added extracting these e-commerce orders to our ETL process, but we have an &lt;a href='https://github.com/snowplow/snowplow/issues/34'&gt;open ticket for this&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id='upgrading'&gt;Upgrading&lt;/h2&gt;

&lt;p&gt;We have made the minified JavaScript tracker version 0.6 available on this URL:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http://d1fc8wv8zag5ca.cloudfront.net/0.6/sp.js&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There are no breaking changes with the previous version 0.5, so you can upgrade your existing SnowPlow JavaScript tracker without issue.&lt;/p&gt;

&lt;p&gt;Note that we have now added versioning to the JavaScript tracker&amp;#8217;s URL. This is because we have &amp;#8220;breaking changes&amp;#8221; to the JavaScript tracker in the pipeline (see e.g. issues &lt;a href='https://github.com/snowplow/snowplow/issues/29'&gt;#29&lt;/a&gt; and &lt;a href='https://github.com/snowplow/snowplow/issues/32'&gt;#32&lt;/a&gt;).&lt;/p&gt;

&lt;h2 id='thanks'&gt;Thanks&lt;/h2&gt;

&lt;p&gt;A final note to say thanks again to &lt;a href='https://github.com/ramn'&gt;Simon Andersson&lt;/a&gt; for contributing the ecommerce tracking functionality! Community contributors like Simon A and Simon R(umble) are helping us to quickly make the SnowPlow vision a reality.&lt;/p&gt;

&lt;p&gt;And of course, we welcome contributions across the five SnowPlow sub-systems. If you would like help implementing a new tracker, trying a different ETL approach or loading SnowPlow events into an alternative database, please &lt;a href='mailto:contribute@snowplowanalytics.com'&gt;get in touch&lt;/a&gt;!&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/08/21/amazon-glacier-launch</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/08/21/amazon-glacier-launch"/>
    <title>Amazon announces Glacier - lowers the cost of running SnowPlow</title>
    <updated>2012-08-21T00:00:00+01:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;Today Amazon announced the launch of &lt;a href='http://aws.amazon.com/glacier/'&gt;Amazon Glacier&lt;/a&gt;, which is a low-cost data archiving service designed for rarely accessed data.&lt;/p&gt;

&lt;p&gt;As Werner Vogels described it in his &lt;a href='http://www.allthingsdistributed.com/2012/08/amazon-glacier.html'&gt;blog post&lt;/a&gt; this morning:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Amazon Glacier provides the same high durability guarantee as Amazon S3 but relaxes the access times to a few hours. This is the right service for customers who have archival data that requires highly reliable storage but for which immediate access is not needed.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;At first sight, Amazon Glacier looks to be a fantastic fit for archiving the raw event logs generated by the SnowPlow collector (whether the CloudFront collector or alternatives such as &lt;a href='https://github.com/shermozle/SnowCannon'&gt;SnowCannon&lt;/a&gt;). Once the nightly SnowPlow ETL has been run on your raw event logs, you shouldn&amp;#8217;t need to access those raw logs frequently. However, we would always recommend retaining them, as there may well be a reason to revisit them in the future. We never recommend throwing away atomic source data!&lt;/p&gt;

&lt;p&gt;This is where Amazon Glacier comes in - at the proposed pricing levels for Glacier, you could archive 2 terabytes of raw SnowPlow data for around $20 a month; this would be significantly cheaper than storing your raw logs in Amazon S3, which is the current SnowPlow approach.&lt;/p&gt;

&lt;p&gt;Moreover, Werner has indicated that:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;In the coming months, Amazon S3 will introduce an option that will allow customers to seamlessly move data between Amazon S3 and Amazon Glacier based on data lifecycle policies.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Once Amazon has launched this feature, we&amp;#8217;ll get this automatic S3-&amp;gt;Glacier archiving process working internally, and then release a howto for SnowPlow users so you can do the same, and start running your SnowPlow over Amazon Glacier!&lt;/p&gt;

&lt;p&gt;Exciting times for everybody who likes storing atomic event data cheaply and safely - stay tuned!&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/08/20/snowplow-0.4.6-released</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/08/20/snowplow-0.4.6-released"/>
    <title>SnowPlow 0.4.6 released</title>
    <updated>2012-08-20T00:00:00+01:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;Over the weekend we released SnowPlow version &lt;strong&gt;0.4.6&lt;/strong&gt;. This was a minor release that added a new capability into the SnowPlow JavaScript tracker.&lt;/p&gt;

&lt;p&gt;Specifically, with the JavaScript you can now specify your own collector URL, rather than simply pass in an account ID which resolves to a CloudFront bucket.&lt;/p&gt;

&lt;p&gt;You can use this feature in your JavaScript invocation code like so:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='javascript'&gt;&lt;span class='c'&gt;&amp;lt;!--&lt;/span&gt; &lt;span class='nx'&gt;SnowPlow&lt;/span&gt; &lt;span class='nx'&gt;starts&lt;/span&gt; &lt;span class='nx'&gt;plowing&lt;/span&gt; &lt;span class='o'&gt;--&amp;gt;&lt;/span&gt;
&lt;span class='o'&gt;&amp;lt;&lt;/span&gt;&lt;span class='nx'&gt;script&lt;/span&gt; &lt;span class='nx'&gt;type&lt;/span&gt;&lt;span class='o'&gt;=&lt;/span&gt;&lt;span class='s2'&gt;&amp;quot;text/javascript&amp;quot;&lt;/span&gt;&lt;span class='o'&gt;&amp;gt;&lt;/span&gt;
&lt;span class='kd'&gt;var&lt;/span&gt; &lt;span class='nx'&gt;_snaq&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='nx'&gt;_snaq&lt;/span&gt; &lt;span class='o'&gt;||&lt;/span&gt; &lt;span class='p'&gt;[];&lt;/span&gt;

&lt;span class='nx'&gt;_snaq&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='nx'&gt;push&lt;/span&gt;&lt;span class='p'&gt;([&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;setCollectorUrl&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;collector.mydomain.com&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;]);&lt;/span&gt;
&lt;span class='nx'&gt;_snaq&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='nx'&gt;push&lt;/span&gt;&lt;span class='p'&gt;([&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;trackPageView&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;]);&lt;/span&gt;

&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='kd'&gt;function&lt;/span&gt;&lt;span class='p'&gt;()&lt;/span&gt; &lt;span class='p'&gt;{&lt;/span&gt;
&lt;span class='p'&gt;...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;Where &lt;code&gt;collector.mydomain.com&lt;/code&gt; is the URL to your own collector.&lt;/p&gt;

&lt;p&gt;We added this capability to SnowPlow in support of Simon Rumble&amp;#8217;s excellent &lt;a href='https://github.com/shermozle/SnowCannon'&gt;SnowCannon&lt;/a&gt; prototype node.js collector for SnowPlow. Going forwards you can of course use this custom URL to send your SnowPlow events to any kind of collector on a domain you control.&lt;/p&gt;

&lt;p&gt;Anyway I hope you like the feature and let us know how you get on with it!&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/08/14/updated-hive-serde-released</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/08/14/updated-hive-serde-released"/>
    <title>Updated Hive SerDe released</title>
    <updated>2012-08-14T00:00:00+01:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;One of the key elements in the SnowPlow technology stack is the Hive SerDe. This is what makes it possible for Elastic MapReduce to read the Cloudfront log files generated by the SnowPlow javascript trackings tags, extarct the relevant fields and make these available in Hive as a nice, clean query table. (The structure of the Hive table is documented &lt;a href='https://github.com/snowplow/snowplow/wiki/Hive-data-structure'&gt;here&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;A number of improvements have been made in the new versions. However, the most significant is that the 5 utm_marketing fields have been added, so that campaign attributes are now available for analytics.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/08/13/introducing-snow-cannon-a-node-js-collector-for-snowplow</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/08/13/introducing-snow-cannon-a-node-js-collector-for-snowplow"/>
    <title>SnowCannon - a node.js collector for SnowPlow</title>
    <updated>2012-08-13T00:00:00+01:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We are hugely excited to introduce &lt;a href='https://github.com/shermozle/SnowCannon'&gt;SnowCannon&lt;/a&gt;, a Node.js collector for SnowPlow, authored by &lt;a href='http://twitter.com/shermozle'&gt;@shermozle&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;SnowCannon is an alternative collector to the default cloudfront collector included with SnowPlow. It offers a number of significant advantages over the Cloudfront connector:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;It allows the use of 3rd party cookies. In particular, this makes it possible to track usage across multiple domains&lt;/li&gt;

&lt;li&gt;It enables real-time analytics. (This is not possible with the Cloudfront-enabled collector, where there&amp;#8217;s a 20-30 minute delay between the javascript tracking event and the associated log being written to S3.)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To learn more about SnowCannon, visit the &lt;a href='https://github.com/shermozle/SnowCannon'&gt;Github repo&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;SnowCannon is the first user-contributed module for SnowPlow, and we are delighted to see community members working to build out the SnowPlow platform. There are other contributions in the works, including a SnowPlow IOS client, that we hope to be announcing shortly.&lt;/p&gt;

&lt;p&gt;To encourage users to extend SnowPlow, we&amp;#8217;ve architected SnowPlow in a module way, to enable developers to swap out elements in the SnowPlow stack with their own elements or complimenet those already in the stack with parallel implementations. Learn more about the SnowPlow architecture &lt;a href='/product/technical-architecture.html'&gt;here&lt;/a&gt;.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/08/02/snowplow-setup-documentation-overhauled</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/08/02/snowplow-setup-documentation-overhauled"/>
    <title>The setup guide has been overhauled</title>
    <updated>2012-08-02T00:00:00+01:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;Following a lot of invaluable feedback from users setting up SnowPlow for the first time, we&amp;#8217;ve updated the SnowPlow setup documentation.&lt;/p&gt;

&lt;p&gt;The documentation can be found &lt;a href='https://github.com/snowplow/snowplow/wiki/SnowPlow-setup-guide'&gt;here&lt;/a&gt;. Any further feedback would be much appreciated - we want to make it as painless as possible for SnowPlow newbies to get up and running&amp;#8230;&lt;/p&gt;</content>
  </entry>
  
 
</feed>