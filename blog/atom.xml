<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom">
 
  <title>The SnowPlow Analytics Blog</title>
  <link href="http://snowplowanalytics.com/"/>
  <link type="application/atom+xml" rel="self" href="http://snowplowanalytics.com/blog/atom.xml"/>
  <updated>2012-11-06T17:12:45+00:00</updated>
  <id>http://snowplowanalytics.com/</id>
  <author>
    <name>The SnowPlow Analytics Team</name>
    <email>contact@snowplowanalytics.com</email>
  </author>

  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/11/06/snowplow-0.5.2-released-and-introducing-sluice</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/11/06/snowplow-0.5.2-released-and-introducing-sluice"/>
    <title>SnowPlow 0.5.2 released, and introducing the Sluice Ruby gem</title>
    <updated>2012-11-06T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;Another week, another release: SnowPlow &lt;strong&gt;0.5.2&lt;/strong&gt;! This is a small release, consisting just of a small set of bug fixes and improvements to EmrEtlRunner - although we&amp;#8217;ll also use this post to introduce our new Ruby gem, called Sluice.&lt;/p&gt;

&lt;p&gt;Many thanks to community member &lt;a href='https://github.com/testower'&gt;Tom Erik St√∏wer&lt;/a&gt; for his testing of EmrEtlRunner over the weekend, which helped us to identify and fix these bugs:&lt;/p&gt;

&lt;h2 id='bugs_fixed'&gt;Bugs fixed&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href='https://github.com/snowplow/snowplow/issues/71'&gt;Issue 71&lt;/a&gt;&lt;/strong&gt;: the template &lt;code&gt;config.yml&lt;/code&gt; (in the GitHub repo and in the wiki) was specifying an out-of-date version for the Hive deserializer. We have updated this to specify version &lt;strong&gt;0.5.0&lt;/strong&gt; of the serde, like so:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;...
:snowplow:
  :serde_version: 0.4.9
...&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href='https://github.com/snowplow/snowplow/issues/72'&gt;Issue 72&lt;/a&gt;&lt;/strong&gt;: Tom&amp;#8217;s testing also identified a bug in EmrEtlRunner&amp;#8217;s log archiving, which only occurs if the Processing Bucket contains sub-folders. This has now been fixed too.&lt;/p&gt;
&lt;!--more--&gt;
&lt;h2 id='a_new_feature_skip'&gt;A new feature: &amp;#8211;skip&lt;/h2&gt;

&lt;p&gt;A new release which only contains bug fixes is a boring release, so we have also implemented a new &lt;code&gt;--skip&lt;/code&gt; option for EmrEtlRunner (&lt;a href='https://github.com/snowplow/snowplow/issues/58'&gt;issue #58&lt;/a&gt;). You can use this when you call EmrEtlRunner like so:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bundle exec snowplow-emr-etl-runner &amp;lt;...&amp;gt; --skip staging OR --skip emr&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This option skips the work steps &lt;strong&gt;up to and including&lt;/strong&gt; the specified step. To give an example: &lt;code&gt;--skip emr&lt;/code&gt; skips both moving the raw logs to the Staging Bucket &lt;strong&gt;and&lt;/strong&gt; running the ETL process on Amazon EMR, i.e. EmrEtlRunner will &lt;strong&gt;only&lt;/strong&gt;* perform the final archiving step.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;--skip&lt;/code&gt; is useful if you encounter a problem midway through your ETL process: you can fix the problem and then skip the steps which ran okay, rather than re-processing from the start. We find it especially helpful when we&amp;#8217;re testing new versions of EmrEtlRunner.&lt;/p&gt;

&lt;h2 id='and_introducing_sluice'&gt;And introducing Sluice&lt;/h2&gt;

&lt;p&gt;At SnowPlow Analytics we are committed to making our software as modular and loosely-coupled as possible. Where we have functionality which could be more widely used, we aim to extract it into standalone modules for developers to use even if they are not implementing SnowPlow.&lt;/p&gt;

&lt;p&gt;We have followed this approach with the parallel file-copy code for Amazon S3 added to EmrEtlRunner by community member &lt;a href='https://github.com/mtibben'&gt;Michael Tibben&lt;/a&gt; from &lt;a href='http://99designs.com'&gt;99designs&lt;/a&gt;: we have moved this code out of EmrEtlRunner into a new Ruby gem, called Sluice. Sluice now has its own &lt;a href='https://github.com/snowplow/sluice'&gt;GitHub repository&lt;/a&gt;, and has been published on &lt;a href='http://rubygems.org/gems/sluice'&gt;RubyGems.org&lt;/a&gt;. It&amp;#8217;s called Sluice because, like &lt;a href='https://github.com/cwensel'&gt;Chris Wensel&lt;/a&gt; (Cascading), we believe in flowing-water metaphors for ETL tools :-)&lt;/p&gt;

&lt;p&gt;Sluice is used by our EmrEtlRunner, and is also a dependency for the StorageLoader Ruby application which we are currently developing.&lt;/p&gt;

&lt;p&gt;We hope to build out Sluice as a general-purpose Ruby toolkit for cloud-friendly ETL over the coming months - and would love contributors! Our view is that, in a world of cloud services like Amazon S3, Google BigQuery and Elastic MapReduce, it makes most sense to take a programmatic approach to ETL, rather than contort the historic, application-based approach of &lt;a href='http://www.talend.com'&gt;Talend&lt;/a&gt;, &lt;a href='http://www.pentaho.com/explore/pentaho-data-integration/'&gt;Pentaho DI&lt;/a&gt; et al. We see Sluice as part of that toolkit for programmatic ETL, alongside great tools such as &lt;a href='http://www.cascading.org'&gt;Cascading&lt;/a&gt;, Rob Slifka&amp;#8217;s &lt;a href='https://github.com/rslifka/elasticity'&gt;Elasticity&lt;/a&gt; and &lt;a href='http://palletops.com'&gt;Pallet&lt;/a&gt;.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/11/01/snowplow-0.5.1-released</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/11/01/snowplow-0.5.1-released"/>
    <title>SnowPlow 0.5.1 released, with lots of small improvements</title>
    <updated>2012-11-01T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We have just released SnowPlow &lt;strong&gt;0.5.1&lt;/strong&gt;! Rather than one large new feature, version 0.5.1 is an incremental release which contains lots of small fixes and improvements to the ETL and storage sub-systems. The two big themes of these updates are:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Improving the robustness of the ETL process&lt;/li&gt;

&lt;li&gt;Laying the foundations for loading SnowPlow events into &lt;a href='http://www.infobright.org/'&gt;Infobright Community Edition&lt;/a&gt; (ICE)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To take each of these themes in turn:&lt;/p&gt;

&lt;h2 id='1_a_more_robust_etl_process'&gt;1. A more robust ETL process&lt;/h2&gt;

&lt;p&gt;The Hive deserializer now has improved error handling - many thanks to community member &lt;a href='https://github.com/mtibben'&gt;Michael Tibben&lt;/a&gt; from &lt;a href='http://99designs.com'&gt;99designs&lt;/a&gt; for his help here!&lt;/p&gt;

&lt;p&gt;Firstly, the Hive deserializer is now setup to log warnings (rather than die) on non-critical data quality issues.&lt;/p&gt;

&lt;p&gt;Additionally, there is now an option (switched off by default) to continue processing even on unexpected row-level errors (such as an input file not matching the expected CloudFront format). We have added a configuration option to the EmrEtlRunner&amp;#8217;s &lt;a href='https://github.com/snowplow/snowplow/blob/master/3-etl/emr-etl-runner/config/config.yml'&gt;configuration file&lt;/a&gt; to support this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:etl:
  :continue_on_unexpected_error: false&lt;/code&gt;&lt;/pre&gt;
&lt;!--more--&gt;
&lt;p&gt;Switch this to &amp;#8216;true&amp;#8217; to continue processing on unexpected row-level errors.&lt;/p&gt;

&lt;h2 id='2_groundwork_for_infobright_compatibility'&gt;2. Groundwork for Infobright compatibility&lt;/h2&gt;

&lt;p&gt;We have added a table definition (and supporting scripts) for setting up a SnowPlow events table in Infobright - you can find these in the main repository under &lt;a href='https://github.com/snowplow/snowplow/tree/master/4-storage/infobright-storage'&gt;&lt;code&gt;snowplow/4-storage/infobright-storage&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Some early ETL design decisions meant that the SnowPlow event files being generated before 0.5.1 were not compatible with being loaded into Infobright (or similar relational databases like Postgres or MySQL). We have made some updates to the ETL process in 0.5.1 to fix this:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;In the Hive deserializer, we now convert tabs to 4 spaces to prevent a stray tab from breaking our load into Infobright&lt;/li&gt;

&lt;li&gt;Databases like Infobright don&amp;#8217;t support Hive&amp;#8217;s &lt;code&gt;ARRAY&amp;lt;STRING&amp;gt;&lt;/code&gt; syntax, so we have updated the Hive deserializer to also output individual booleans for the browser features, alongside the browser features array&lt;/li&gt;

&lt;li&gt;We have created a new HiveQL script which outputs SnowPlow event files in a format which can be easily loaded into Infobright - this is called &lt;a href='https://github.com/snowplow/snowplow/blob/master/3-etl/hive-etl/hiveql/non-hive-rolling-etl.q'&gt;&lt;code&gt;non-hive-rolling-etl.q&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;We have added a configuration option to the EmrEtlRunner&amp;#8217;s &lt;a href='https://github.com/snowplow/snowplow/blob/master/3-etl/emr-etl-runner/config/config.yml'&gt;configuration file&lt;/a&gt; so that you can choose whether to output Hive-format or non-Hive-format event files&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;On point 4: we believe that most people will want to load their SnowPlow event files into other database systems, such as Infobright (or eventually, Postgres, Google BigQuery, SkyDB etc). Therefore, the default setting for the &lt;a href='https://github.com/snowplow/snowplow/blob/master/3-etl/emr-etl-runner/config/config.yml'&gt;configuration option&lt;/a&gt; in the EmrEtlRunner is to output your SnowPlow event files in the non-Hive-format:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:etl:
  :storage_format: non-hive&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As the comment says, if you will &lt;strong&gt;only&lt;/strong&gt; be doing analysis in Hive, you could switch this setting to &amp;#8216;hive&amp;#8217; and benefit from the slightly-tweaked, Hive-friendly file format.&lt;/p&gt;

&lt;h2 id='getting_help'&gt;Getting help&lt;/h2&gt;

&lt;p&gt;If you have any problems getting version 0.5.1 working, please &lt;a href='https://github.com/snowplow/snowplow/issues'&gt;raise an issue&lt;/a&gt; or get in touch with us via &lt;a href='https://github.com/snowplow/snowplow/wiki/Talk-to-us'&gt;the usual channels&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id='in_the_pipeline'&gt;In the pipeline&lt;/h2&gt;

&lt;p&gt;At SnowPlow we want to support multiple different storage and analytics options for SnowPlow events, alongside our current Hive-based approach. This version 0.5.1 provides the building blocks for our Infobright support - for the next release, we are working on a Storage Loader component to download your event files from Amazon S3 and load them into your local Infobright instance. We&amp;#8217;ll keep you posted on our progress here!&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/10/31/snowplow-in-a-universal-analytics-world-what-the-new-version-of-google-analytics-means-for-companies-adopting-snowplow</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/10/31/snowplow-in-a-universal-analytics-world-what-the-new-version-of-google-analytics-means-for-companies-adopting-snowplow"/>
    <title>SnowPlow in a Universal Analytics world - what the new version of Google Analytics means for companies adopting SnowPlow</title>
    <updated>2012-10-31T00:00:00+00:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;Earlier this week, Google announced a series of significant advances in Google Analytics at the GA Summit, that are collectively referred to as &lt;a href='http://cutroni.com/blog/2012/10/29/universal-analytics-the-next-generation-of-google-analytics/'&gt;Universal Analytics&lt;/a&gt;. In this post, we look at:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href='#what'&gt;The actual features Google has announced&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2012/10/31/snowplow-proposition-in-a-universal-analytics-world-what-the-new-version-of-ga-means-for-snowplow-adoption#whysnowplow'&gt;How those advances change the case for companies considering adopting SnowPlow&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img alt='universal-analytics-image' src='/static/img/google-analytics-universal-analytics.png' /&gt;&lt;/p&gt;
&lt;a name='what' /&gt;
&lt;h2 id='1_what_changes_has_google_announced'&gt;1. What changes has Google announced?&lt;/h2&gt;

&lt;p&gt;The most significant change Google has announced is the new &lt;a href='https://developers.google.com/analytics/devguides/collection/protocol/v1/'&gt;Measurement Protocol&lt;/a&gt;, which enables businesses using GA to capture much more data. This will make it possible for Google to deliver a much broader range of reports, of higher business value, than was previously possible. To understand the changes, we start by considering what &lt;a href='#new-data-points'&gt;new data points&lt;/a&gt; businesses can &lt;em&gt;feed&lt;/em&gt; GA, before considering &lt;a href='/blog/2012/10/31/snowplow-proposition-in-a-universal-analytics-world-what-the-new-version-of-ga-means-for-snowplow-adoption#reporting-capabilities'&gt;what that means for GA&amp;#8217;s reporting capabilities&lt;/a&gt;.&lt;/p&gt;
&lt;a name='new-data-points' /&gt;
&lt;h3 id='11_custom_user_identifiers'&gt;1.1 Custom user identifiers&lt;/h3&gt;

&lt;p&gt;The first new data points that businesses can feed into Google Analytics is a user&amp;#8217;s &lt;code&gt;client_id&lt;/code&gt; (basically, a customer ID) as defined on the business&amp;#8217;s own systems.&lt;/p&gt;

&lt;p&gt;Previously, Google Analytics identified unique users using their own &lt;code&gt;cookie_id&lt;/code&gt;s. Google&amp;#8217;s &lt;code&gt;cookie_id&lt;/code&gt;s are an excellent starting point for identifying users, because so many users have Google accounts (thanks to their myriad mass-market services, including Gmail, YouTube, Google Play etc): consumers using these services on multiple devices identify themselves to Google when they login, meaning that Google can marry their &lt;code&gt;cookie_id&lt;/code&gt;s for these users on all the different devices they use. Our assumption is that Google already use this to reliably identify individual users across multiple platforms and devices.&lt;/p&gt;

&lt;p&gt;For businesses using GA, being able to augment Google&amp;#8217;s user identification with their own internal &lt;code&gt;client_id&lt;/code&gt;s is a significant step forwards for two reasons:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Many GA users (especially those with applications or properties where users login, or those with a loyalty card scheme) can identify their own users reliably on specific platforms, devices and physical stores. By adding this additional user identification data into GA, GA will be more accurate at identifying the same user in different places reliably, moving us further from a world in which we rely on persistent cookies dropped on browsers with unique identifiers, to one where users are more robustly identified via logins, payments and loyalty schemes. These approaches will still use cookies, but as part of a broader set of user identification business processes that actively involve the user in the identification process&lt;/li&gt;

&lt;li&gt;It should make it easier for GA users to join GA data with other customer data sets on those &lt;code&gt;client_id&lt;/code&gt;s. This is more of a nuanced point, as it was still possible previously to add customer IDs to GA as custom variables and use that to do joining&lt;/li&gt;
&lt;/ol&gt;
&lt;!--more--&gt;&lt;a name='customer-journey' /&gt;
&lt;h3 id='12_capturing_events_across_a_users_entire_customer_journey_not_just_the_web_not_just_digital_interactions'&gt;1.2 Capturing events across a user&amp;#8217;s entire customer journey (not just the web, not just digital interactions)&lt;/h3&gt;

&lt;p&gt;We have long argued that web analytics is just one customer data source - and that analysts performing customer analytics need to crunch data covering the customer&amp;#8217;s complete journey, including other digital channels and offline interactions. That means joining data sets from different digital products and offline data sets to generate a single customer view. To date, companies that have implemented &amp;#8220;single customer views&amp;#8221; have typically struggled incorporating web behaviour in those views.&lt;/p&gt;

&lt;p&gt;Google has taken a significant step towards enabling businesses to capture much more of their customer&amp;#8217;s journeys in Google Analytics itself. The &lt;a href='https://developers.google.com/analytics/devguides/collection/protocol/v1/'&gt;Measurement Protocol&lt;/a&gt; makes it possible to pass offline events into Google: so for example, when a customer buys an item in store, it would be possible to fire an event to Google Analytics recording that sale. If the customer was on a CRM programme (e.g. loyalty scheme), his / her &lt;code&gt;client_id&lt;/code&gt; could be passed in, and then Google Analytics would know that this is the same user who browsed the website on their mobile phone yesterday and viewed it from their office today, prior to coming in store to make the purchase.&lt;/p&gt;

&lt;p&gt;The Measurement Protocol can also be used to capture events on digital platforms that are not so well suited to traditional web analytics solution e.g. mobile applications, set-top box applications, videogames on consoles etc. It thus opens the door for Google Analytics to capture and report on event data from a range of devices, not just those that are web based.&lt;/p&gt;

&lt;p&gt;Taken together, this means it will be possible for Google Analytics to offer reports detailing customer behaviour across the complete customer journey. Building on this, it should also be possible for GA to enable analysts to calculate &lt;a href='/analytics/customer-analytics/customer-lifetime-value.html'&gt;customer lifetime value&lt;/a&gt; (if the value of different events was passed in with the events): this is one of the most important metrics in customer analytics, and one that has been conspicuous by its absence from web analytics outside of solutions like &lt;a href='/analytics/customer-analytics/customer-lifetime-value.html'&gt;SnowPlow&lt;/a&gt; until now. The Measurement Protocol potentially means a huge increase in the scope and value of reports that it should be possible to generate in Google Analytics.&lt;/p&gt;

&lt;h3 id='13_capturing_customeracquisition_cost_data'&gt;1.3 Capturing customer-acquisition cost data&lt;/h3&gt;

&lt;p&gt;One of the most common types of analytics performed on web data is working out the return on marketing investment for different customer-acquisition channels. To perform this analysis, we need to combine:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Data on what was spent on each channel - typically cost data from those channels themselves, e.g. display, PPC, affiliate, social etc&lt;/li&gt;

&lt;li&gt;Web analytics data on how many people visited the website in response to those ads and what fraction of them went on to become customers. By dividing the total spent on each channel (1) by the number of customers acquired from each channel (2), we can calculate the cost of acquiring each customer for that channel.&lt;/li&gt;

&lt;li&gt;Financial data on the revenue/profits generated by those customers, over their lifetimes. By comparing the average value of each customer acquired from each channel against the average cost of acquiring each of those customers, we can calculate the return on that acquisition cost&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Now that Google lets businesses reliably track their users over their entire lifecycles (on and offline), it becomes possible to calculate the user&amp;#8217;s lifetime value, as detailed &lt;a href='#customer-journey'&gt;above&lt;/a&gt;, delivering on point #3. Google has always enabled business to capture #2. Now, Google lets you send the cost data into Google Analytics (point #1) - so that the return of each campaign can be accurately calculated. (Previously, only spend data from AdWords could be imported into GA.) With this information, companies should be better placed to drive marketing spend decisions based on Google Analytics reports. Again though, the reality is more nuanced, because typically those spend decisions have to be made &lt;em&gt;before&lt;/em&gt; a customer&amp;#8217;s lifetime value (#3) can be accurately calculated, so companies really need to develop predictive models of how valuable customers are likely to be. Anything but the most basic models are likely to require tools outside of GA to develop, and then pulling that data out of GA to power those models.&lt;/p&gt;

&lt;h3 id='14_custom_dimensions_and_metrics'&gt;1.4 Custom dimensions and metrics&lt;/h3&gt;

&lt;p&gt;The &lt;a href='https://developers.google.com/analytics/devguides/collection/protocol/v1/'&gt;Measurement Protocol&lt;/a&gt; enables businesses to define and capture their own dimensions and metrics each time an event that is tracked. Those additional metrics and dimensions are then available to report in in GA.&lt;/p&gt;

&lt;p&gt;As well as enabling businesses to add custom dimension and metric values to individual event tracking calls, Google also lets businesses bulk upload multiple dimensions at a time into the GA, if a relationship between those custom dimensions and dimensions already in GA can be defined, and GA knows what values to ascribe events already in it to those new dimensions, based on that defined relationship. To give an example: you could upload the product names/SKUs associated with each web page, enabling reporting on page views by SKU. Or, you could upload a range of product metadata (e.g. book titles and authors) and associate that with an ISBN custom field.&lt;/p&gt;
&lt;a name='reporting-capabilities' /&gt;
&lt;h3 id='15_what_new_reporting_is_enabled_through_the_capture_of_all_these_additional_data_points'&gt;1.5 What new reporting is enabled through the capture of all these additional data points?&lt;/h3&gt;

&lt;p&gt;Taken together, the additional data that businesses can feed into Google Analytics gives Google enough to offer a much broader and more valuable range of reporting than was previously possible:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Customer analytics&lt;/strong&gt;. We have long argued that web analytics packages including GA are too focused on sessions, page views and conversions, and neglect the broader, more valuable customer analytics that underpin the most successful businesses in the world. With these new data points, GA has the raw data to produce useful customer reports including customer lifetime value, and analysis of user behaviours over their entire journeys. No longer will web analysts using GA be confined to viewing actions over an isolated session: now they can slice and dice metrics by users over their user journeys spanning multiple site visits.&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Event analytics&lt;/strong&gt; across platforms, on and offline. GA can now report on user&amp;#8217;s complete journey, not just what they do on websites, but also their behaviours on other digital platforms (esp. mobile) and offline.&lt;/li&gt;
&lt;/ol&gt;
&lt;a name='whysnowplow' /&gt;
&lt;h2 id='2_how_do_the_advances_in_ga_change_the_case_for_adopting_snowplow'&gt;2. How do the advances in GA change the case for adopting SnowPlow?&lt;/h2&gt;

&lt;p&gt;Prior to the latest announcement, the case for adopting SnowPlow alongside your GA implementation was as follows:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The reporting provided by Google Analytics is very limited, with little/no customer analytics, catalogue analytics and platform analytics supported.&lt;/li&gt;

&lt;li&gt;SnowPlow enables you to perform all these three types of analytics, by providing you with access to your raw customer-level and event-level clickstream data, so that you can use whatever analytics tool you like to crunch the data and perform that analysis&lt;/li&gt;

&lt;li&gt;SnowPlow makes it easier to join your web analytics data sets with other data sets (e.g. marketing data sets, CRM and offline data sets), by enabling businesses to load customer IDs into SnowPlow, and then perform the join on the raw data sets. This means that businesses running SnowPlow can analyse user behaviour across their entire customer journey (on and offline, across all digital and non-digital channels)&lt;/li&gt;

&lt;li&gt;SnowPlow makes it easy to warehouse your customer data for posterity: an asset which will doubtless grow in value over time.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Following the latest announcement, some of these arguments fall away:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Google has significantly strengthened its customer analytics capability. To what extent is not yet clear - we only know at this stage what extra data points Google Analytics will, hypothetically, let you collect - not what additional reporting UIs GA will provide to process that data&lt;/li&gt;

&lt;li&gt;The additional data points &lt;em&gt;should&lt;/em&gt; improve GA&amp;#8217;s platform and catalogue analytics capabilities; we will only be able to confirm this once we start working with the updated version of GA&lt;/li&gt;

&lt;li&gt;Therefore, the gap between what is possible with GA, and what is possible with SnowPlow, has shrunk&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Nevertheless, the case for implementing SnowPlow alongside GA is still compelling, for three main reasons. To take each of these in turn:&lt;/p&gt;

&lt;h3 id='21_analytics_capabilities'&gt;2.1 Analytics capabilities&lt;/h3&gt;

&lt;p&gt;There are several different considerations here:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Google Analytics still does not give you access to your customer-level and event-level data. Therefore, &lt;strong&gt;there will always be ways that you can crunch SnowPlow data that you cannot accomplish in GA&lt;/strong&gt;: drilling down to segments of one visitor is just the most obvious example&lt;/li&gt;

&lt;li&gt;There are a range of analytics techniques which are hard to imagine Google implementing at all, even with the new data sets that are available. To give just three examples:&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
&lt;li&gt;Using machine learning techniques (e.g. &lt;a href='http://mahout.apache.org/'&gt;Mahout&lt;/a&gt;) to &lt;strong&gt;segment audience by behaviour&lt;/strong&gt;&lt;/li&gt;

&lt;li&gt;Performing &lt;strong&gt;event analytics&lt;/strong&gt; / pathing in a way that takes into account the &lt;strong&gt;structure of the website&lt;/strong&gt;. This is described brilliantly by &lt;a href='http://semphonic.blogs.com/about.html'&gt;Gary Angel&lt;/a&gt; on the &lt;a href='http://semphonic.blogs.com/semangel/2011/01/statistical-analysis-functionalism-and-how-web-analytics-works.html'&gt;Semphonic blog&lt;/a&gt;. This methodology includes identifying those events that are predictive of customer lifetime value&lt;/li&gt;

&lt;li&gt;Building and testing models that &lt;strong&gt;predict customer lifetime value ahead of time&lt;/strong&gt;, so that you can quickly (and robustly) calculate the ROI on marketing campaigns, and adjust your spend accordingly&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
&lt;li&gt;There will always be barriers analysts run up against in trying to fit all of their data into Google&amp;#8217;s schema. For example, it&amp;#8217;s not obvious how Google&amp;#8217;s single &lt;code&gt;client_id&lt;/code&gt; will cope with different packages (CRM, email, CMS et al) each having their own internal set of user IDs&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id='22_creating_live_datadriven_products'&gt;2.2 Creating live, data-driven products&lt;/h3&gt;

&lt;p&gt;There are also important capabilities around using your event data and derived analyses in &lt;strong&gt;live, data-driven products&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Having access to the event stream and your own analyses allows you to make use of that data in data-driven products and systems, including &lt;strong&gt;product / content recommendation&lt;/strong&gt;, &lt;strong&gt;user personalisation engines&lt;/strong&gt; and &lt;strong&gt;internal search algorithms&lt;/strong&gt;&lt;/li&gt;

&lt;li&gt;Because SnowPlow is open-source software which can be installed on your own servers, it should be possible to co-locate SnowPlow with your own software (CMSes, ecommerce packages, custom apps etc) and thus tightly integrate these data-driven products into your offering&lt;/li&gt;

&lt;li&gt;Because GA doesn&amp;#8217;t provide the granular customer-level and event-level data, GA data cannot be used to prototype or drive these data-driven services&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id='23_data_ownership_and_technical_architecture'&gt;2.3 Data ownership and technical architecture&lt;/h3&gt;

&lt;p&gt;Finally, there are also a number of &lt;strong&gt;data ownership&lt;/strong&gt; and &lt;strong&gt;architectural issues&lt;/strong&gt; which we believe make a SnowPlow solution an important compliment, if not yet a full alternative, to a GA implementation. These relate to the fact that, with GA, businesses get more value out by feeding more and more data in: to realise all of the new potential above, they need to be feeding GA with data covering their &lt;em&gt;complete&lt;/em&gt; set of customer interactions. However:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;This is the &lt;strong&gt;most valuable data&lt;/strong&gt; your company owns. Does it make sense to leave the warehousing and storage of that data to a third-party who in many cases is providing the service for free? What guarantees does you have that that data will always be available, 3, 5 or 10 years down the line?&lt;/li&gt;

&lt;li&gt;Does it make sense to feed your detailed event- and customer-level data to Google Analytics, when GA does not share that data back with you at the same atomic level of detail. (GA rolls the data up to &lt;strong&gt;aggregates&lt;/strong&gt; which are less flexible to work with from an analytics perspective)&lt;/li&gt;

&lt;li&gt;What happens when &lt;strong&gt;innacurate data&lt;/strong&gt; is loaded into Google Analytics? Without the ablity to query and diligence the data directly, leave alone clean and reprocess data, there are very limited options available for a business that has innaccurate data in GA. This becomes a bigger issue as implementation become more complex (because data is being ingested across digital and offline platforms), and GA becomes the de facto tool for all customer analytics&lt;/li&gt;

&lt;li&gt;If you need to setup regular ETL processes to load the data from all of your third-party systems into GA, you could &lt;strong&gt;expend the same energy setting up SnowPlow instead&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id='closing_thoughts'&gt;Closing thoughts&lt;/h2&gt;

&lt;p&gt;We at SnowPlow Analytics are enormously excited by the progress Google are making with their Universal Analytics proposition, and especially the good work Google are doing educating the market into the value of customer-centric analytics. But to unleash the full power of that type of customer, platform and catalogue analytics, the serious analyst will still need access to the customer-level and event-level data: ideally on infrastructure that is totally under your own control. SnowPlow is still the best way of getting hold and storing that data.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/10/25/snowplow-0.5.0-released</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/10/25/snowplow-0.5.0-released"/>
    <title>SnowPlow 0.5.0 released, now with a Ruby gem to run SnowPlow's ETL process on Amazon EMR</title>
    <updated>2012-10-25T00:00:00+01:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We have just released SnowPlow &lt;strong&gt;0.5.0&lt;/strong&gt;, with an all-new component, the SnowPlow EmrEtlRunner. EmrEtlRunner is a Ruby application to run SnowPlow&amp;#8217;s Hive-based ETL (extract, transform, load) process on &lt;a href='http://aws.amazon.com/elasticmapreduce/'&gt;Amazon Elastic MapReduce&lt;/a&gt; with minimum fuss.&lt;/p&gt;

&lt;p&gt;We are hugely grateful to community member &lt;a href='https://github.com/mtibben'&gt;Michael Tibben&lt;/a&gt; from &lt;a href='http://99designs.com'&gt;99designs&lt;/a&gt; for his contributions to EmrEtlRunner: thanks to Michael, EmrEtlRunner is more efficient, more flexible and more robust than it otherwise would have been - and ready sooner. Many thanks Michael!&lt;/p&gt;

&lt;h2 id='using_emretlrunner'&gt;Using EmrEtlRunner&lt;/h2&gt;

&lt;p&gt;EmrEtlRunner is a Ruby application which you can setup on your server to regularly take your raw SnowPlow logs (as stored in CloudFront access logs) and apply the Hive-based ETL process to them using &lt;a href='http://aws.amazon.com/elasticmapreduce/'&gt;Amazon Elastic MapReduce&lt;/a&gt;. This ETL process populates a Hive-format events table which you can then use with the HiveQL recipes in our &lt;a href='http://snowplowanalytics.com/analytics/index.html'&gt;Analyst&amp;#8217;s Cookbook&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;For detailed instructions on installing, running and scheduling EmrEtlRunner on your server, please see the &lt;a href='https://github.com/snowplow/snowplow/wiki/Deploying-EmrEtlRunner'&gt;Deploying EmrEtlRunner&lt;/a&gt; guide on the SnowPlow Analytics wiki.&lt;/p&gt;
&lt;!--more--&gt;
&lt;h2 id='the_codebase'&gt;The codebase&lt;/h2&gt;

&lt;p&gt;If you want to take a look at the code, you can find it in the main repository here: &lt;a href='https://github.com/snowplow/snowplow/tree/master/3-etl/emr-etl-runner'&gt;&lt;code&gt;3-etl/emr-etl-runner/&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id='getting_help'&gt;Getting help&lt;/h2&gt;

&lt;p&gt;If you have any problems getting EmrEtlRunner working, please &lt;a href='https://github.com/snowplow/snowplow/issues'&gt;raise an issue&lt;/a&gt; or get in touch with us via &lt;a href='https://github.com/snowplow/snowplow/wiki/Talk-to-us'&gt;the usual channels&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id='in_the_pipeline'&gt;In the pipeline&lt;/h2&gt;

&lt;p&gt;At SnowPlow we want to support multiple different storage and analytics options for SnowPlow events, alongside our current Hive-based approach. Our first priority is supporting &lt;a href='http://www.infobright.org/'&gt;Infobright Community Edition&lt;/a&gt; (ICE) for event storage and querying; extending the current ETL process to load SnowPlow events into ICE will be the focus of our next few releases, so please stay tuned!&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/10/24/web-analytics-with-tableau-and-snowplow</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/10/24/web-analytics-with-tableau-and-snowplow"/>
    <title>Performing web analytics on SnowPlow data using Tableau - a video demo</title>
    <updated>2012-10-24T00:00:00+01:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;People who see SnowPlow for the first time often ask us to &lt;i&gt;&quot;show SnowPlow in action&quot;&lt;/i&gt;. It is one thing to tell someone that having access to their customer- and event-level data will open up whole new analysis possibilities, but it is another thing to demonstrate those possibilities.&lt;/p&gt;

&lt;p&gt;Demonstrating SnowPlow is tricky because currently, SnowPlow only gives you access to data: we have no snazzy front-end UI to show off. The good news is that there are a lot of smart people developing fast, powerful and easy-to-use reporting tools. And because SnowPlow gives you access to underlying customer- and event-level data, it is easy to analyse SnowPlow data in nearly all of these tools. One such tool is &lt;a href=&quot;http://www.tableausoftware.com/&quot;&gt;Tableau&lt;/a&gt; - we like Tableau as it is fast and intuitive, making it easy for us to perform train-of-thought analyses on SnowPlow data. (We will explain more on how to connect Tableau to SnowPlow data in a future blog post.)&lt;/p&gt;

&lt;p&gt;In the following series of videos, we start to show how SnowPlow lets you use &lt;a href=&quot;http://www.tableausoftware.com/&quot;&gt;Tableau&lt;/a&gt; for exploring your web analytics data. In the first video, we introduce Tableau and talk through the Tableau worksheet created with SnowPlow data for an online retailer:&lt;/p&gt;

&lt;video width=&quot;648&quot; height=&quot;563&quot; controls&gt;

	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-1.mp4&quot;  type=&quot;video/mp4&quot; /&gt;
	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-1.webm&quot; type=&quot;video/webm&quot; /&gt;
	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-1.ogv&quot;  type=&quot;video/ogg&quot; /&gt;
	&lt;object width=&quot;648&quot; height=&quot;563&quot; type=&quot;application/x-shockwave-flash&quot; data=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-1.swf&quot;&gt;
		&lt;param name=&quot;movie&quot; value=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-1.swf&quot; /&gt;
		&lt;param name=&quot;flashvars&quot; value=&quot;controlbar=over&amp;amp;image=http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-1-thumb.jpg&amp;amp;file=http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-1.mp4&quot; /&gt;
		&lt;img src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-1-thumb.jpg&quot; width=&quot;648&quot; height=&quot;563&quot; alt=&quot;Web analytics with Tableau and SnowPlow introductory video&quot;
		     title=&quot;No video playback capabilities, please download the video below&quot; /&gt;
	&lt;/object&gt; 
&lt;/video&gt;

&lt;p class=&quot;note&quot;&gt;&lt;i&gt;Having trouble viewing the video above? You may download the videos in your format of choice:&lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-1.mp4&quot;&gt;&quot;MP4&quot;&lt;/a&gt;, &lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-1.ogv&quot;&gt;&quot;Ogg&quot;&lt;/a&gt; or &lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-1.webm&quot;&gt;WebM&lt;/a&gt; formats.&lt;/i&gt;&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;In the second video, we show how to perform an analysis of the drivers of growth of traffic on a website. The video serves to highlight how effective Tableau is at performing train-of-thought analysis:&lt;/p&gt;

&lt;video width=&quot;648&quot; height=&quot;563&quot; controls&gt;

	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-2.mp4&quot;  type=&quot;video/mp4&quot; /&gt;
	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-2.webm&quot; type=&quot;video/webm&quot; /&gt;
	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-2.ogv&quot;  type=&quot;video/ogg&quot; /&gt;
	&lt;object width=&quot;648&quot; height=&quot;563&quot; type=&quot;application/x-shockwave-flash&quot; data=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-2.swf&quot;&gt;
		&lt;param name=&quot;movie&quot; value=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-2.swf&quot; /&gt;
		&lt;param name=&quot;flashvars&quot; value=&quot;controlbar=over&amp;amp;image=http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-2-thumb.jpg&amp;amp;file=http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-2.mp4&quot; /&gt;
		&lt;img src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-2-thumb.jpg&quot; width=&quot;648&quot; height=&quot;563&quot; alt=&quot;Web analytics with Tableau and SnowPlow introductory video&quot;
		     title=&quot;No video playback capabilities, please download the video below&quot; /&gt;
	&lt;/object&gt; 
&lt;/video&gt;

&lt;p class=&quot;note&quot;&gt;&lt;i&gt;Having trouble viewing the video above? You may download the videos in your format of choice:&lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-2.mp4&quot;&gt;&quot;MP4&quot;&lt;/a&gt;, &lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-2.ogv&quot;&gt;&quot;Ogg&quot;&lt;/a&gt; or &lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-2.webm&quot;&gt;WebM&lt;/a&gt; formats.&lt;/i&gt;&lt;/p&gt;

&lt;p&gt;In the third video, we show how to perform an analysis comparing the relative performance of different products in an online retailer's catalogue. This is an example of &lt;strong&gt;catalogue analytics&lt;/strong&gt;, a very important branch of analytics - where we analyse how different products on a retailer's site perform relative to one another, or how different media items (e.g. articles / videos) on a media site perform. Surprisingly, catalogue analytics is not supported by traditional web analytics packages like Google Analytics:&lt;/p&gt;

&lt;video width=&quot;648&quot; height=&quot;563&quot; controls&gt;

	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-3.mp4&quot;  type=&quot;video/mp4&quot; /&gt;
	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-3.webm&quot; type=&quot;video/webm&quot; /&gt;
	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-3.ogv&quot;  type=&quot;video/ogg&quot; /&gt;
	&lt;object width=&quot;648&quot; height=&quot;563&quot; type=&quot;application/x-shockwave-flash&quot; data=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-3.swf&quot;&gt;
		&lt;param name=&quot;movie&quot; value=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-3.swf&quot; /&gt;
		&lt;param name=&quot;flashvars&quot; value=&quot;controlbar=over&amp;amp;image=http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-3-thumb.jpg&amp;amp;file=http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-3.mp4&quot; /&gt;
		&lt;img src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-3-thumb.jpg&quot; width=&quot;648&quot; height=&quot;563&quot; alt=&quot;Web analytics with Tableau and SnowPlow introductory video&quot;
		     title=&quot;No video playback capabilities, please download the video below&quot; /&gt;
	&lt;/object&gt;
&lt;/video&gt;

&lt;p class=&quot;note&quot;&gt;&lt;i&gt;Having trouble viewing the video above? You may download the videos in your format of choice:&lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-3.mp4&quot;&gt;&quot;MP4&quot;&lt;/a&gt;, &lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-3.ogv&quot;&gt;&quot;Ogg&quot;&lt;/a&gt; or &lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-3.webm&quot;&gt;WebM&lt;/a&gt; formats.&lt;/i&gt;&lt;/p&gt;

&lt;p&gt;In the fourth video, we analyse improvements in conversion rates over time for the retailer. This is a core measure to track in order to understand how improvements to the website and marketing strategy drive increased conversion rates. Again, this is something not supported by Google Analytics out of the box. We show how easy it is with SnowPlow and Tableau to identify trends in conversion rates over time:&lt;/p&gt;

&lt;video width=&quot;648&quot; height=&quot;563&quot; controls&gt;

	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-4.mp4&quot;  type=&quot;video/mp4&quot; /&gt;
	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-4.webm&quot; type=&quot;video/webm&quot; /&gt;
	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-4.ogv&quot;  type=&quot;video/ogg&quot; /&gt;
	&lt;object width=&quot;648&quot; height=&quot;563&quot; type=&quot;application/x-shockwave-flash&quot; data=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-4.swf&quot;&gt;
		&lt;param name=&quot;movie&quot; value=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-4.swf&quot; /&gt;
		&lt;param name=&quot;flashvars&quot; value=&quot;controlbar=over&amp;amp;image=http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-4-thumb.jpg&amp;amp;file=http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-4.mp4&quot; /&gt;
		&lt;img src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-4-thumb.jpg&quot; width=&quot;648&quot; height=&quot;563&quot; alt=&quot;Web analytics with Tableau and SnowPlow introductory video&quot;
		     title=&quot;No video playback capabilities, please download the video below&quot; /&gt;
	&lt;/object&gt;
&lt;/video&gt;

&lt;p class=&quot;note&quot;&gt;&lt;i&gt;Having trouble viewing the video above? You may download the videos in your format of choice:&lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-4.mp4&quot;&gt;&quot;MP4&quot;&lt;/a&gt;, &lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-4.ogv&quot;&gt;&quot;Ogg&quot;&lt;/a&gt; or &lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-4.webm&quot;&gt;WebM&lt;/a&gt; formats.&lt;/i&gt;&lt;/p&gt;

&lt;p&gt;In the fifth video, we show how to visualise patterns of individual user visits over time. This is an interesting starting point to begin to unpick the patterns that make up successful user engagement:&lt;/p&gt;

&lt;video width=&quot;648&quot; height=&quot;563&quot; controls&gt;

	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-5.mp4&quot;  type=&quot;video/mp4&quot; /&gt;
	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-5.webm&quot; type=&quot;video/webm&quot; /&gt;
	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-5.ogv&quot;  type=&quot;video/ogg&quot; /&gt;
	&lt;object width=&quot;648&quot; height=&quot;563&quot; type=&quot;application/x-shockwave-flash&quot; data=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-5.swf&quot;&gt;
		&lt;param name=&quot;movie&quot; value=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-5.swf&quot; /&gt;
		&lt;param name=&quot;flashvars&quot; value=&quot;controlbar=over&amp;amp;image=http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-5-thumb.jpg&amp;amp;file=http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-5.mp4&quot; /&gt;
		&lt;img src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-5-thumb.jpg&quot; width=&quot;648&quot; height=&quot;563&quot; alt=&quot;Web analytics with Tableau and SnowPlow introductory video&quot;
		     title=&quot;No video playback capabilities, please download the video below&quot; /&gt;
	&lt;/object&gt;
&lt;/video&gt;

&lt;p class=&quot;note&quot;&gt;&lt;i&gt;Having trouble viewing the video above? You may download the videos in your format of choice:&lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-5.mp4&quot;&gt;&quot;MP4&quot;&lt;/a&gt;, &lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-5.ogv&quot;&gt;&quot;Ogg&quot;&lt;/a&gt; or &lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-5.webm&quot;&gt;WebM&lt;/a&gt; formats.&lt;/i&gt;&lt;/p&gt;

&lt;p&gt;In the sixth video, we show how to visualise the range of product pages visited by each user. This can help us to understand how successful the retailer is at driving users interested in one product to consider buying other products (cross-selling), and onwards to developing recommendation algorithms (users who liked &lt;i&gt;this&lt;/i&gt; product also liked &lt;i&gt;this&lt;/i&gt; product):&lt;/p&gt;

&lt;video width=&quot;648&quot; height=&quot;563&quot; controls&gt;

	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-6.mp4&quot;  type=&quot;video/mp4&quot; /&gt;
	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-6.webm&quot; type=&quot;video/webm&quot; /&gt;
	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-6.ogv&quot;  type=&quot;video/ogg&quot; /&gt;
	&lt;object width=&quot;648&quot; height=&quot;563&quot; type=&quot;application/x-shockwave-flash&quot; data=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-6.swf&quot;&gt;
		&lt;param name=&quot;movie&quot; value=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-6.swf&quot; /&gt;
		&lt;param name=&quot;flashvars&quot; value=&quot;controlbar=over&amp;amp;image=http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-6-thumb.jpg&amp;amp;file=http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-6.mp4&quot; /&gt;
		&lt;img src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-6-thumb.jpg&quot; width=&quot;648&quot; height=&quot;563&quot; alt=&quot;Web analytics with Tableau and SnowPlow introductory video&quot;
		     title=&quot;No video playback capabilities, please download the video below&quot; /&gt;
	&lt;/object&gt;
&lt;/video&gt;

&lt;p class=&quot;note&quot;&gt;&lt;i&gt;Having trouble viewing the video above? You may download the videos in your format of choice:&lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-6.mp4&quot;&gt;&quot;MP4&quot;&lt;/a&gt;, &lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-6.ogv&quot;&gt;&quot;Ogg&quot;&lt;/a&gt; or &lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-6.webm&quot;&gt;WebM&lt;/a&gt; formats.&lt;/i&gt;&lt;/p&gt;

&lt;p&gt;In the final video in the series, we perform an example cohort analysis, with a view to understanding how 'sticky' the online retailer site is, and how its stickiness has improved over time. In this example, we use &lt;i&gt;stickiness&lt;/i&gt; to refer to how good the website is at driving repeat visits:&lt;/p&gt;

&lt;video width=&quot;648&quot; height=&quot;563&quot; controls&gt;

	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-7.mp4&quot;  type=&quot;video/mp4&quot; /&gt;
	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-7.webm&quot; type=&quot;video/webm&quot; /&gt;
	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-7.ogv&quot;  type=&quot;video/ogg&quot; /&gt;
	&lt;object width=&quot;648&quot; height=&quot;563&quot; type=&quot;application/x-shockwave-flash&quot; data=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-7.swf&quot;&gt;
		&lt;param name=&quot;movie&quot; value=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-7.swf&quot; /&gt;
		&lt;param name=&quot;flashvars&quot; value=&quot;controlbar=over&amp;amp;image=http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-7-thumb.jpg&amp;amp;file=http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-7.mp4&quot; /&gt;
		&lt;img src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-7-thumb.jpg&quot; width=&quot;648&quot; height=&quot;563&quot; alt=&quot;Web analytics with Tableau and SnowPlow introductory video&quot;
		     title=&quot;No video playback capabilities, please download the video below&quot; /&gt;
	&lt;/object&gt;
&lt;/video&gt;

&lt;p class=&quot;note&quot;&gt;&lt;i&gt;Having trouble viewing the video above? You may download the videos in your format of choice:&lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-7.mp4&quot;&gt;&quot;MP4&quot;&lt;/a&gt;, &lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-7.ogv&quot;&gt;&quot;Ogg&quot;&lt;/a&gt; or &lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-7.webm&quot;&gt;WebM&lt;/a&gt; formats.&lt;/i&gt;&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/10/21/infobright-ruby-loader-released</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/10/21/infobright-ruby-loader-released"/>
    <title>Infobright Ruby Loader Released</title>
    <updated>2012-10-21T00:00:00+01:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We&amp;#8217;re pleased to start the week with the release of a new Ruby gem, our &lt;a href='https://github.com/snowplow/infobright-ruby-loader'&gt;Infobright Ruby Loader&lt;/a&gt; (IRL).&lt;/p&gt;

&lt;p&gt;At SnowPlow we&amp;#8217;re committed to supporting multiple different storage and analytics options for SnowPlow events, alongside our current Hive-based approach. One of the alternative data stores we are working with is &lt;a href='http://www.infobright.org/'&gt;Infobright&lt;/a&gt;, a columnar database which is available in open source and commercial versions.&lt;/p&gt;

&lt;p&gt;For all but the largest SnowPlow users, columnar databases such as Infobright should be an attractive alternative to doing all of your analysis in Hive. The main advantages of columnar databases are as follows:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Scale to terabytes (although not petabytes, unlike Hive)&lt;/li&gt;

&lt;li&gt;Fixed cost (dedicated RAM-heavy analytics server), versus pay-as-you-go querying on Amazon EMR&lt;/li&gt;

&lt;li&gt;Significantly faster query times ‚Äì typically seconds, not minutes&lt;/li&gt;

&lt;li&gt;Plug in to many analytics front ends e.g. Tableau, Qlikview, R&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;So, open source columnar databases like Infobright Community Edition (ICE) are a good fit for SnowPlow analytics. Unfortunately, when we started to load SnowPlow event logs into ICE, we realised that there wasn&amp;#8217;t a good data-loading solution for Infobright in Ruby, our ETL language of choice. So, we built one :-)&lt;/p&gt;

&lt;p&gt;Our freshly minted &lt;a href='https://github.com/snowplow/infobright-ruby-loader'&gt;Infobright Ruby Loader&lt;/a&gt; (IRL) can be used in two different ways:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;As a command-line tool&lt;/strong&gt; - for manual loading of data into Infobright at the command-line. No Ruby expertise required&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;As part of another application&lt;/strong&gt; - because it&amp;#8217;s a Ruby gem with a Ruby API, IRL can be integrated into larger Ruby ETL processes&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We will be using IRL at SnowPlow as part of our larger ETL process to load SnowPlow events into ICE for analysis - we hope to roll this out within the next few weeks.&lt;/p&gt;

&lt;p&gt;In the meantime, we hope that IRL is useful to people in the Infobright community who need to run data loads at the command-line; IRL was inspired by &lt;a href='http://www.infobright.org/Blog/Entry/unscripted/'&gt;ParaFlex&lt;/a&gt;, an excellent Bash script from the Infobright team to perform parallel loading of Infobright, and can be used as a direct alternative to ParaFlex.&lt;/p&gt;

&lt;p&gt;To find out more about our Infobright Ruby Loader, please check out the detailed &lt;a href='https://github.com/snowplow/infobright-ruby-loader/blob/master/README.md'&gt;README&lt;/a&gt; in the GitHub repository. And please direct any questions through the &lt;a href='https://github.com/snowplow/snowplow/wiki/Talk-to-us'&gt;usual channels&lt;/a&gt;!&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/10/12/how-the-role-of-hive-is-changing-at-snowplow</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/10/12/how-the-role-of-hive-is-changing-at-snowplow"/>
    <title>How we use Hive at SnowPlow, and how the role of Hive is changing. (Slides from our presentation to Hive London.)</title>
    <updated>2012-10-12T00:00:00+01:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;Last night I gave a presentation to the clever folks at Hive London covering three things:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;How big data technologies like Apache Hive are transforming web analytics&lt;/li&gt;

&lt;li&gt;Howe we&amp;#8217;ve used Hive in SnowPlow development&lt;/li&gt;

&lt;li&gt;How the role of Hive has changed at SnowPlow over time, including a comparison of Hive against other technologies.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The slides from the presentation are below. As always, any questions / comments, please post them below.&lt;/p&gt;
&lt;iframe width='427' frameborder='0' style='border:1px solid #CCC;border-width:1px 1px 0;margin-bottom:5px' marginwidth='0' marginheight='0' height='356' src='http://www.slideshare.net/slideshow/embed_code/14696456' scrolling='no'&gt;  &lt;/iframe&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/10/11/snowplow-0.4.10-released</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/10/11/snowplow-0.4.10-released"/>
    <title>SnowPlow 0.4.10 released</title>
    <updated>2012-10-11T00:00:00+01:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We have just released version &lt;strong&gt;0.4.10&lt;/strong&gt; of SnowPlow - people using 0.4.8 can jump straight to this version. This version updates:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;snowplow.js to version 0.7.0&lt;/li&gt;

&lt;li&gt;the Hive deserializer to version 0.4.9&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Big thanks to community members &lt;a href='https://github.com/mtibben'&gt;Michael Tibben&lt;/a&gt; from &lt;a href='http://99designs.com'&gt;99designs&lt;/a&gt; and &lt;a href='https://github.com/ramn'&gt;Simon Andersson&lt;/a&gt; from &lt;a href='http://www.qwaya.com'&gt;Qwaya&lt;/a&gt; for their most-helpful contributions to this release!&lt;/p&gt;

&lt;h2 id='main_changes'&gt;Main changes&lt;/h2&gt;

&lt;p&gt;The main changes are as follows:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The querystring parameter for site ID which the JavaScript tracker sends to your collector is renamed from &lt;code&gt;said&lt;/code&gt; to &lt;code&gt;aid&lt;/code&gt;&lt;/li&gt;

&lt;li&gt;The Hive-based ETL process now extracts the ecommerce tracking fields and the site ID field and adds them into your processed events table&lt;/li&gt;

&lt;li&gt;We fixed a bug in the Hive deserializer where a partially-processed row was returned even if a fatal error was found in the row (now, a null row is returned instead)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The rest of the changes were all enhancements to the Hive deserializer&amp;#8217;s Specs2 test suite - these improvements should help to accelerate work on the deserializer (we have lots of cool new stuff we want to add to the deserializer!). &lt;!--more--&gt;&lt;/p&gt;

&lt;h2 id='new_event_table_fields'&gt;New event table fields&lt;/h2&gt;

&lt;p&gt;The new fields in the event table all relate directly to additional tracking functionality which was added to the JavaScript tracker in &lt;a href='/blog/2012/09/06/snowplow-0.4.7-released/'&gt;SnowPlow 0.4.7&lt;/a&gt;. Specifically:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The &lt;code&gt;setSiteId()&lt;/code&gt; functionality is now extracted to the &lt;code&gt;app_id&lt;/code&gt; field (short for application ID)&lt;/li&gt;

&lt;li&gt;The ecommerce tracking functionality is now extracted to a set of &lt;code&gt;tr_&lt;/code&gt; and &lt;code&gt;ti_&lt;/code&gt; fields&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For details on the new fields, please review our latest &lt;a href='/analytics/snowplow-table-structure.html'&gt;Hive events table definition&lt;/a&gt; - there is now a column indicating in which version a given field was added.&lt;/p&gt;

&lt;h2 id='how_to_get_the_new_version'&gt;How to get the new version&lt;/h2&gt;

&lt;p&gt;As usual, the new version of the Hive deserializer is available from the GitHub repository&amp;#8217;s &lt;a href='https://github.com/snowplow/snowplow/downloads'&gt;Downloads&lt;/a&gt; section as &lt;strong&gt;snowplow-log-deserializers-0.4.9.jar&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The updated snowplow.js is &lt;a href='https://raw.github.com/snowplow/snowplow/master/1-trackers/javascript-tracker/js/snowplow.js'&gt;available in our GitHub repository&lt;/a&gt; for you to minify and upload, or alternatively you can use the one on our CDN:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;https://d1fc8wv8zag5ca.cloudfront.net/0.7.0/sp.js&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you have any problems with either of these components, please &lt;a href='https://github.com/snowplow/snowplow/issues'&gt;raise an issue&lt;/a&gt;!&lt;/p&gt;

&lt;h2 id='a_note_on_backwards_compatibility_for_the_events_table'&gt;A note on backwards compatibility for the events table&lt;/h2&gt;

&lt;p&gt;We will continue to add extra fields to the SnowPlow events table as we add extra capabilities to the ETL process - for example, we are working on functionality to extract geo-location information from IP addresses via MaxMind.&lt;/p&gt;

&lt;p&gt;Starting with our new &lt;code&gt;app_id&lt;/code&gt; field, we will be adding all such new fields to the &lt;strong&gt;end&lt;/strong&gt; of our Hive events table definition. This will mean that you will &lt;strong&gt;not&lt;/strong&gt; have to re-run the ETL process across all your historic raw logs, provided you do &lt;strong&gt;not&lt;/strong&gt; need the data found in the new fields. This is because a Hive query across both the old event table format and the new table format works as long as you don&amp;#8217;t explicitly query a new field.&lt;/p&gt;

&lt;p&gt;In other words, Hive is futureproofed against new fields being added to the end of your underlying data files, and we&amp;#8217;ll take advantage of this to improve backwards compatibility for our events table!&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/10/11/attlib-0.0.1-released</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/10/11/attlib-0.0.1-released"/>
    <title>Attlib - an open source library for extracting search marketing attribution data from referrer URLs</title>
    <updated>2012-10-11T00:00:00+01:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;Last night we published &lt;a href='https://github.com/snowplow/attlib'&gt;Attlib&lt;/a&gt;, an open source Ruby library for extracting search marketing attribution data from referrer URLs. In this post we talk through:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href='#what_attlib_does'&gt;What Attlib does, and how to use it&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='#install'&gt;Installing Attlib&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='#search_engine_yaml'&gt;The search_engine.yml file&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='#snowplow_stack'&gt;Attlib as part of the SnowPlow stack&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='#other_languages'&gt;Attlib in other languages&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='#snowplow_components_as_standalone_projects'&gt;Making components of SnowPlow available as standalone open source projects&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;a name='what_attlib_does' /&gt;
&lt;h3 id='what_attlib_does_and_how_to_use_it'&gt;What Attlib does, and how to use it&lt;/h3&gt;

&lt;p&gt;Attlib is straightforward Ruby library for extracting seach marketing attribution data from referrer URLs. You give it a referrer URL: it then lets you now whether the URL is from a search engine. If it is, it will tell you which search engine it is, and what keywords were typed. (If those keywords are included in the query string - this is no longer the case for users logged in to Google, as documented &lt;a href='http://googlewebmastercentral.blogspot.co.uk/2011/10/accessing-search-query-data-for-your.html'&gt;here&lt;/a&gt;.)&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='ruby'&gt;&lt;span class='nb'&gt;require&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;attlib&amp;#39;&lt;/span&gt;

&lt;span class='n'&gt;r&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='no'&gt;Referrer&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;new&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;http://images.google.ca/imgres?q=hermetic+tarot&amp;amp;hl=en&amp;amp;biw=1189&amp;amp;bih=521&amp;amp;tbm=isch&amp;amp;tbnid=BuQ_IyUbc25usM:&amp;amp;imgrefurl=http://www.psychicbazaar.com/tarot-cards/15-the-hermetic-tarot.html&amp;amp;imgurl=http://mdm.pbzstatic.com/tarot/the-hermetic-tarot/card-4.png&amp;amp;w=1064&amp;amp;h=1551&amp;amp;ei=ue9AUMe7Osn9iwLZ-4H4Dw&amp;amp;zoom=1&amp;amp;iact=hc&amp;amp;vpx=107&amp;amp;vpy=48&amp;amp;dur=2477&amp;amp;hovh=271&amp;amp;hovw=186&amp;amp;tx=133&amp;amp;ty=157&amp;amp;sig=115588264602219115047&amp;amp;page=4&amp;amp;tbnh=162&amp;amp;tbnw=120&amp;amp;start=57&amp;amp;ndsp=19&amp;amp;ved=1t:429,r:12,s:57,i:291&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt;

&lt;span class='n'&gt;r&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;is_search_engine?&lt;/span&gt; &lt;span class='c1'&gt;# True&lt;/span&gt;
&lt;span class='n'&gt;r&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;search_engine&lt;/span&gt; &lt;span class='c1'&gt;# &amp;#39;Google Images&amp;#39;&lt;/span&gt;
&lt;span class='n'&gt;r&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;keywords&lt;/span&gt; 	&lt;span class='c1'&gt;# &amp;#39;hermetic tarot&amp;#39;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;&lt;a name='install' /&gt;
&lt;h3 id='installing_attlib'&gt;Installing Attlib&lt;/h3&gt;

&lt;p&gt;Attlib is available via a Ruby Gem. To install, simply run the following at the command line:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo gem install attlib&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The sourcecode is available on &lt;a href='https://github.com/snowplow/attlib'&gt;Github&lt;/a&gt;&lt;/p&gt;
&lt;a name='search_engine_yaml' /&gt;
&lt;h3 id='the_search_enginesyml_file'&gt;The search_engines.yml file&lt;/h3&gt;

&lt;p&gt;Extracting search engine names and keywords from a referrer URL is pretty straightforward. What is more complicated is keeping track of the myriad search engines that are out there, operating in different countries, the myriad domains they operate on, and the different query parameters that each of them uses to store the keywords.&lt;/p&gt;

&lt;p&gt;Because the space is constantly evolving, none of this information (about search engines, parameters and domains) has been hard coded into Attlib. All of it is available in the &lt;a href='https://github.com/snowplow/attlib/blob/master/data/search_engines.yml'&gt;search_engines.yml&lt;/a&gt; file, in the &lt;a href='https://github.com/snowplow/attlib/tree/master/data'&gt;data&lt;/a&gt; in the repo. &lt;!--more--&gt;&lt;/p&gt;

&lt;p&gt;The structure of the YAML file should be straightforward to understand. Each search engine is a top level item. For each search engine, two lists are given: one is a list of parameters used in that search engine&amp;#8217;s query string to identify the keywords entered. The other is the list of domains on which that search engine operates. An extract is shown below:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='yaml'&gt;&lt;span class='l-Scalar-Plain'&gt;Babylon&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt;
  &lt;span class='l-Scalar-Plain'&gt;parameters&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; 
    &lt;span class='p-Indicator'&gt;-&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;q&lt;/span&gt;
  &lt;span class='l-Scalar-Plain'&gt;domains&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; 
   &lt;span class='p-Indicator'&gt;-&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;search.babylon.com&lt;/span&gt;
   &lt;span class='p-Indicator'&gt;-&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;searchassist.babylon.com&lt;/span&gt;

&lt;span class='l-Scalar-Plain'&gt;Baidu&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt;
  &lt;span class='l-Scalar-Plain'&gt;parameters&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; 
    &lt;span class='p-Indicator'&gt;-&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;wd&lt;/span&gt;
    &lt;span class='p-Indicator'&gt;-&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;word&lt;/span&gt;
    &lt;span class='p-Indicator'&gt;-&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;kw&lt;/span&gt;
    &lt;span class='p-Indicator'&gt;-&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;k&lt;/span&gt;
  &lt;span class='l-Scalar-Plain'&gt;domains&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt;
    &lt;span class='p-Indicator'&gt;-&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;www.baidu.com&lt;/span&gt;
    &lt;span class='p-Indicator'&gt;-&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;www1.baidu.com&lt;/span&gt;
    &lt;span class='p-Indicator'&gt;-&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;zhidao.baidu.com&lt;/span&gt;
    &lt;span class='p-Indicator'&gt;-&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;tieba.baidu.com&lt;/span&gt;
    &lt;span class='p-Indicator'&gt;-&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;news.baidu.com&lt;/span&gt;
    &lt;span class='p-Indicator'&gt;-&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;web.gougou.com&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;Keeping this file up to date is a big job: one of our hopes releasing Attlib as an open source, standalone library, is that the community contributes to the file. We are enormously grateful to our friends at &lt;a href='http://piwik.org/'&gt;Piwik&lt;/a&gt; as our initial version of the file is based on the Piwik equivalent &lt;a href='https://github.com/piwik/piwik/blob/master/core/DataFiles/SearchEngines.php'&gt;SearchEngines.php&lt;/a&gt;, for the hard work they put into this version.&lt;/p&gt;
&lt;a name='snowplow_stack' /&gt;
&lt;h3 id='attlib_as_part_of_the_snowplow_stack'&gt;Attlib as part of the SnowPlow stack&lt;/h3&gt;

&lt;p&gt;Our intention is to port &lt;a href='https://github.com/snowplow/attlib'&gt;Attlib&lt;/a&gt; into Scala and integrate it into the SnowPlow stack: specifically the ETL phase. Both Ruby and Scala versions of Attlib will run based on the same &lt;a href='https://github.com/snowplow/attlib/blob/master/data/search_engines.yml'&gt;search_engines.yml&lt;/a&gt; file.&lt;/p&gt;
&lt;a name='other_languages' /&gt;
&lt;h3 id='attlib_in_other_languages'&gt;Attlib in other languages&lt;/h3&gt;

&lt;p&gt;As well as contributing to the search &lt;a href='https://github.com/snowplow/attlib/blob/master/data/search_engines.yml'&gt;search_engines.yml&lt;/a&gt; file, we also hope that community members will develop versions of Attlib in other languages e.g. Python.&lt;/p&gt;
&lt;a name='snowplow_components_as_standalone_projects' /&gt;
&lt;h3 id='making_components_of_snowplow_available_as_standalone_open_source_projects'&gt;Making components of SnowPlow available as standalone open source projects&lt;/h3&gt;

&lt;p&gt;Attlib is the first component in the SnowPlow stack that we have released as a standalone library. There are many more in the pipeline. (More on this in future blog posts :-) ). For us, this is a key part of the SnowPlow strategy:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Keeping the SnowPlow architecture as loosely coupled as possible. We believe this makes SnowPlow robust, scalable and extendable&lt;/li&gt;

&lt;li&gt;Grow the userbase of people using and contributing to each component. Processing web analytics data is a big job: there are many individual components involved, and each of them needs to evolve with the changing marketplace. Attlib is concerned today with extracting useful data from search engine referrers: but it is likely that as time goes on, we&amp;#8217;ll want to extend it to capture data from other types of referrers e.g. social networks or affiliate sites. The bigger the community of people on top of those developments, the better for everyone in the web analytics community. Releasing each component as a standalone open source library should help grow that community.&lt;/li&gt;
&lt;/ol&gt;
&lt;hr /&gt;
&lt;p&gt;Any questions about Attlib, or anything else in this post? Then &lt;a href='/contact/index.html'&gt;get in touch&lt;/a&gt; with the SnowPlow team.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/09/24/what-does-snowplow-let-you-do</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/09/24/what-does-snowplow-let-you-do"/>
    <title>Why set your data free?</title>
    <updated>2012-09-24T00:00:00+01:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;At Saturday&amp;#8217;s &lt;a href='http://ukdaa.co.uk/'&gt;Measure Camp&lt;/a&gt;, I had the chance to introduce SnowPlow to a large number of some incredibly thoughtful and insightful people in the web analytics industry.&lt;/p&gt;

&lt;p&gt;With each person, I started by explaining that SnowPlow gave them direct access to their customer-level and event-level data. The response I got in nearly all cases was: &lt;strong&gt;what does having direct access to my web analytics data enable me to do, that I can&amp;#8217;t do with Google Analytics / Omniture?&lt;/strong&gt; It&amp;#8217;s such a good question I thought I should publish an answer below:&lt;/p&gt;

&lt;h3 id='1_integrate_web_analytics_data_with_other_data_sources'&gt;1. Integrate web analytics data with other data sources&lt;/h3&gt;

&lt;p&gt;Integrating your web analytics data with other data sets enables you to answer a wide range of valuable business questions:&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;&lt;strong&gt;Data source&lt;/strong&gt;&lt;/th&gt;&lt;th&gt;&lt;strong&gt;Example business questions&lt;/strong&gt;&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Marketing spend data e.g. AdWords, ad server data&lt;/td&gt;&lt;td style='text-align: left;'&gt;What is the return on my ad spend? How should I optimize my return on ad spend&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Customer data e.g. CRM, loyalty&lt;/td&gt;&lt;td style='text-align: left;'&gt;How does the online behaviour of my differnet customer segments vary by segment? Do online promotions drive offline sales? (Or vice versa?)&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Product / media catalogue data&lt;/td&gt;&lt;td style='text-align: left;'&gt;What are my most profitable product lines? Do different types of products attract different customer segments? What are the products that drive the most visits?&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;SnowPlow makes integrating web analytics data with other data sources easier in a two ways:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;All your SnowPlow data is directly accessible in Apache Hive or Infobright. (So no expensive export process is required, prior to linking the data sets.)&lt;/li&gt;

&lt;li&gt;Custom variables and event tracking give you plenty of opportunity to join e.g. customer IDs or campaigns names to enable &lt;code&gt;JOIN&lt;/code&gt;s across data set&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For more details on how to perform &lt;code&gt;JOIN&lt;/code&gt;s between SnowPlow data and other sources, see refer to the guide to &lt;a href='/analytics/customer-analytics/joining-customer-data.html'&gt;joining SnowPlow engagement data with other sources of customer data&lt;/a&gt;&lt;/p&gt;

&lt;h3 id='2_slice_and_dice_your_data_by_any_combination_of_dimensions__metrics_you_want'&gt;2. Slice and dice your data by any combination of dimensions / metrics you want&lt;/h3&gt;

&lt;p&gt;Google Analytics in particular only lets users create reports about of set combinations of dimensions and metrics. Examples of combinations that are &lt;strong&gt;not supported&lt;/strong&gt; include:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Number of unique visitors by product page&lt;/li&gt;

&lt;li&gt;Different sources of traffic by product page (and how this changes over time)&lt;/li&gt;

&lt;li&gt;Engagement levels (e.g. number of visits, number of page views, conversion rates) by traffic source&lt;/li&gt;

&lt;li&gt;Improvements to conversion rates over time&lt;/li&gt;
&lt;/ol&gt;
&lt;!--more--&gt;
&lt;p&gt;In contrast, because SnowPlow gives you access to the underlying data, it is possible to use BI tools like &lt;a href='http://www.tableausoftware.com/'&gt;Tableau&lt;/a&gt; and &lt;a href='http://www.microsoft.com/en-us/bi/powerpivot.aspx'&gt;PowerPivot&lt;/a&gt; to quickly slice and dice web analytics data by any dimensions / metrics you want. We&amp;#8217;ll be posting examples of how to do this in the next few days.&lt;/p&gt;

&lt;h3 id='3_use_machine_learning_tools_on_your_web_analytics_data'&gt;3. Use machine learning tools on your web analytics data&lt;/h3&gt;

&lt;p&gt;Machine learning tools, and &lt;a href='http://mahout.apache.org/'&gt;Mahout&lt;/a&gt; in particular, have created some new and exciting opportunities to:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Develop product and content recommendation engines, based on user web behaviour. (E.g. users who viewed these content items, also viewed&amp;#8230;)&lt;/li&gt;

&lt;li&gt;Segment your audience by online behaviour&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;SnowPlow makes it easy to extract the core input data you would need to feed a machine learning algorithm in a single query. (E.g. a matrix mapping users to products by page views / add to baskets / purchases etc.) We will be exploring ways to integrate SnowPlow with &lt;a href='http://mahout.apache.org/'&gt;Mahout&lt;/a&gt; in a future blog post.&lt;/p&gt;

&lt;h3 id='4_view_data_for_individual_users_over_their_entire_lives'&gt;4. View data for individual users over their entire lives&lt;/h3&gt;

&lt;p&gt;Whereas reports on Google Analytics tend to be about visits, page views or transactions, SnowPlow lets you slice data by users over multiple visits, opening up a wide range of possibilities:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Develop accurate models of customer lifetime value&lt;/li&gt;

&lt;li&gt;Develop more rigorous approaches to attribution modelling, by capturing in granular detail which channels touched a user at different points in their lifecycle&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id='5_interested_in_any__all_of_the_above'&gt;5. Interested in any / all of the above?&lt;/h3&gt;

&lt;p&gt;Then &lt;a href='/product/get-started.html'&gt;get started&lt;/a&gt; with SnowPlow, or &lt;a href='/contact/index.html'&gt;get in touch&lt;/a&gt; to find out more!&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/09/14/snowplow-0.4.8-released</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/09/14/snowplow-0.4.8-released"/>
    <title>SnowPlow 0.4.8 released</title>
    <updated>2012-09-14T00:00:00+01:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We have just released SnowPlow version &lt;strong&gt;0.4.8&lt;/strong&gt;, with a set of enhancements to the existing Hive deserializer:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The Hive deserializer now supports Amazon&amp;#8217;s new CloudFront log file format (launched 12 September 2012) as well as the older format&lt;/li&gt;

&lt;li&gt;The Hive deserializer now supports a tracking pixel called simply &lt;code&gt;i&lt;/code&gt; (saving some characters versus &lt;code&gt;ice.png&lt;/code&gt;) (&lt;a href='https://github.com/snowplow/snowplow/issues/35'&gt;issue #35&lt;/a&gt;)&lt;/li&gt;

&lt;li&gt;The Hive deserializer now works if the CloudFront distribution has Forward Query String = yes (&lt;a href='https://github.com/snowplow/snowplow/pull/39'&gt;issue #39&lt;/a&gt;)&lt;/li&gt;

&lt;li&gt;The Hive deserializer no longer dies if the calling page&amp;#8217;s querystring is malformed&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Many thanks to community member &lt;a href='https://github.com/mtibben'&gt;Michael Tibben&lt;/a&gt; from &lt;a href='http://99designs.com'&gt;99designs&lt;/a&gt; in Melbourne for contributing the Forward Query String = yes fix!&lt;/p&gt;

&lt;h2 id='new_cloudfront_log_file_format'&gt;New CloudFront log file format&lt;/h2&gt;

&lt;p&gt;On 12th September 2012, Amazon &lt;a href='http://aws.amazon.com/about-aws/whats-new/2012/09/04/cloudfront-support-for-cookies-and-price-classes/'&gt;rolled out a new CloudFront log file format&lt;/a&gt;, adding three additional fields onto the end of each line:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;cs(Cookie)&lt;/strong&gt;, the cookie header in the request (if any). Logging of this field is optional.&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;x-edge-result-type&lt;/strong&gt;, the result type of each HTTP(s) request (for example, cache hit/miss/error).&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;x-edge-request-id&lt;/strong&gt;, an encrypted string that uniquely identifies a request to help AWS troubleshoot/debug any issues.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As always, please consult the Amazon CloudFront &lt;a href='http://docs.amazonwebservices.com/AmazonCloudFront/latest/DeveloperGuide/AccessLogs.html#LogFileFormat'&gt;Developer Guide&lt;/a&gt; for more information on these fields. &lt;!--more--&gt;&lt;/p&gt;

&lt;p&gt;As part of this new &lt;strong&gt;0.4.8&lt;/strong&gt; SnowPlow release, the Hive deserializer now supports the new CloudFront format as well as the old format: if you deploy the latest version of the deserializer, you should be able to process both old-format and new-format CloudFront logs without issue.&lt;/p&gt;

&lt;h2 id='support_for__as_the_tracking_pixel'&gt;Support for &lt;code&gt;i&lt;/code&gt; as the tracking pixel&lt;/h2&gt;

&lt;p&gt;Currently the SnowPlow JavaScript tracker fires a GET request to a tracking pixel called &lt;code&gt;ice.png&lt;/code&gt;. This works fine, but it makes more sense to call the pixel &lt;code&gt;i&lt;/code&gt;, for two reasons:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;We free up 5 extra characters to use for sending data&lt;/li&gt;

&lt;li&gt;A transparent GIF is smaller to send than a transparent PNG&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Thanks to &lt;a href='https://github.com/shermozle/'&gt;Simon Rumble&lt;/a&gt; (author of &lt;a href='https://github.com/shermozle/SnowCannon'&gt;SnowCannon&lt;/a&gt;) for pointing this out! In due course we will update the JavaScript tracker and CloudFront collector to implement this change (see issues &lt;a href='https://github.com/snowplow/snowplow/issues/29'&gt;#29&lt;/a&gt; and &lt;a href='https://github.com/snowplow/snowplow/issues/25'&gt;#25&lt;/a&gt;), but to start off we have added support for &lt;code&gt;i&lt;/code&gt; to the new version of the Hive deserializer.&lt;/p&gt;

&lt;p&gt;This is a small change, but highlights a wider point for SnowPlow development: in general, whenever we have a &amp;#8220;breaking change&amp;#8221; coming upstream, we will try to prepare for this change downstream first, to prevent any disruption to your use of SnowPlow.&lt;/p&gt;

&lt;h2 id='support_for_forward_query_string__yes'&gt;Support for Forward Query String = yes&lt;/h2&gt;

&lt;p&gt;Thanks to &lt;a href='https://github.com/mtibben'&gt;Michael Tibben&lt;/a&gt; from &lt;a href='http://99designs.com'&gt;99designs&lt;/a&gt; for spotting that the Hive deserializer does not work if your CloudFront distribution has Forward Query String set to Yes; Michael not only raised the issue but also provided a fix, many thanks Michael!&lt;/p&gt;

&lt;p&gt;Most SnowPlow users will have Forward Query String in their CloudFront distribution set to No, so this issue will not arise for them; however this fix will be invaluable for anyone who does have it set to Yes. If you want to read more about this, please check out &lt;a href='https://github.com/snowplow/snowplow/pull/39'&gt;issue #39&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We&amp;#8217;re aware that our guide for setting up the CloudFront distribution is a bit out of date (which is how this issue can arise) - we will be refreshing the tracking pixel guide soon (&lt;a href='https://github.com/snowplow/snowplow/issues/25'&gt;issue #25&lt;/a&gt;)! Many thanks for your patience.&lt;/p&gt;

&lt;h2 id='more_robust_querystring_handling'&gt;More robust querystring handling&lt;/h2&gt;

&lt;p&gt;A small change - we have made the code for extracting marketing attribution more robust. Specifically, the Hive deserializer no longer dies (i.e. throws a non-recoverable &lt;code&gt;SerDeException&lt;/code&gt;) if the calling page&amp;#8217;s URL has a malformed querystring.&lt;/p&gt;

&lt;p&gt;An example of a malformed querystring would be something like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http://www.psychicbazaar.com/2-tarot-cards?n=48?utmsource=GoogleSearch&amp;amp;...&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note the two &lt;code&gt;?&lt;/code&gt; questionmarks (the second one should be an &lt;code&gt;&amp;amp;&lt;/code&gt; ampersand). In the case of a malformed querystring like this, the five marketing attribution fields in the Hive output format for this row will all be set to null.&lt;/p&gt;

&lt;h2 id='deploying_the_new_version'&gt;Deploying the new version&lt;/h2&gt;

&lt;p&gt;The new version of the Hive deserializer is available from the GitHub repository&amp;#8217;s &lt;a href='https://github.com/snowplow/snowplow/downloads'&gt;Downloads&lt;/a&gt; section as &lt;strong&gt;snowplow-log-deserializers-0.4.8.jar&lt;/strong&gt;. If you have any problems running it, please &lt;a href='https://github.com/snowplow/snowplow/issues'&gt;raise an issue&lt;/a&gt;!&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/09/06/snowplow-0.4.7-released</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/09/06/snowplow-0.4.7-released"/>
    <title>SnowPlow 0.4.7 released</title>
    <updated>2012-09-06T00:00:00+01:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We have just released SnowPlow version &lt;strong&gt;0.4.7&lt;/strong&gt;. This release bumps the SnowPlow JavaScript tracker to version &lt;strong&gt;0.6&lt;/strong&gt;, with two significant new features:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The ability to set a site ID for your tracking - useful for multi-site publishers&lt;/li&gt;

&lt;li&gt;The ability to log ecommerce transactions - useful for merchants wanting to track orders&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;A huge thanks to community member &lt;a href='https://github.com/ramn'&gt;Simon Andersson&lt;/a&gt; from &lt;a href='http://www.qwaya.com'&gt;Qwaya&lt;/a&gt; for contributing the ecommerce tracking functionality - thank you Simon!&lt;/p&gt;

&lt;p&gt;We&amp;#8217;ll take a look at both of these new features in turn:&lt;/p&gt;

&lt;h2 id='site_id'&gt;Site ID&lt;/h2&gt;

&lt;p&gt;The SnowPlow JavaScript tracker now lets you set a site identifier before you start logging events. The new method for this is called &lt;code&gt;setSiteId()&lt;/code&gt; - it takes one argument, the identifier you have assigned to this site. For example:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='javascript'&gt;&lt;span class='nx'&gt;_snaq&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='nx'&gt;push&lt;/span&gt;&lt;span class='p'&gt;([&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;setAccount&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;d3rkrsqld9gmqf&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;]);&lt;/span&gt;
&lt;span class='nx'&gt;_snaq&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='nx'&gt;push&lt;/span&gt;&lt;span class='p'&gt;([&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;setSiteId&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;CFe23a&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;]);&lt;/span&gt;
&lt;span class='nx'&gt;_snaq&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='nx'&gt;push&lt;/span&gt;&lt;span class='p'&gt;([&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;trackPageView&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;]);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;The querystring passed to your SnowPlow collector will now include the following parameter:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;...&amp;amp;said=CFe23a&amp;amp;...&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;where &lt;code&gt;said&lt;/code&gt; stands for &lt;em&gt;Site or App ID&lt;/em&gt; - because we plan on using the same parameter for mobile and desktop app tracking as well. &lt;!--more--&gt;&lt;/p&gt;

&lt;p&gt;This new feature should be helpful for anyone running multiple sites (or perhaps clients) against the same SnowPlow collector - it means that you can easily partition your SnowPlow events by site, whilst still being able to run cross-site analyses should you so wish.&lt;/p&gt;

&lt;p&gt;Note that we haven&amp;#8217;t yet added extracting &lt;code&gt;said&lt;/code&gt; to our ETL process, but we have an &lt;a href='https://github.com/snowplow/snowplow/issues/33'&gt;open ticket for this&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id='ecommerce_transactions'&gt;Ecommerce transactions&lt;/h2&gt;

&lt;p&gt;To date, we have been analysing e-commerce transactions using SnowPlow by:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Logging every &lt;em&gt;product add to basket&lt;/em&gt; event&lt;/li&gt;

&lt;li&gt;Logging every &lt;em&gt;product remove from basket&lt;/em&gt; event&lt;/li&gt;

&lt;li&gt;Netting these events off to determine the final contents of the order&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This approach works, but it adds complexity in the analysis step. Happily community member Simon Andersson has contributed an alternative solution: dedicated SnowPlow e-commerce transaction tracking, similar to the functionality found in the Google Analytics JavaScript API.&lt;/p&gt;

&lt;p&gt;The idea is that you add the new tracking code to your shop&amp;#8217;s checkout confirmation page, so that the completed order can be sent to SnowPlow. A complete example of the new tracking code looks like this:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='javascript'&gt;&lt;span class='kd'&gt;var&lt;/span&gt; &lt;span class='nx'&gt;orderId&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;order-123&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;;&lt;/span&gt;

&lt;span class='c1'&gt;// addTrans sets up the transaction, should be called first.&lt;/span&gt;
&lt;span class='nx'&gt;_snaq&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='nx'&gt;push&lt;/span&gt;&lt;span class='p'&gt;([&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;addTrans&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
  &lt;span class='nx'&gt;orderId&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;                &lt;span class='c1'&gt;// order ID - required&lt;/span&gt;
  &lt;span class='s1'&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;                     &lt;span class='c1'&gt;// affiliation or store name&lt;/span&gt;
  &lt;span class='s1'&gt;&amp;#39;8000&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;                 &lt;span class='c1'&gt;// total - required&lt;/span&gt;
  &lt;span class='s1'&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;                     &lt;span class='c1'&gt;// tax&lt;/span&gt;
  &lt;span class='s1'&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;                     &lt;span class='c1'&gt;// shipping&lt;/span&gt;
  &lt;span class='s1'&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;                     &lt;span class='c1'&gt;// city&lt;/span&gt;
  &lt;span class='s1'&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;                     &lt;span class='c1'&gt;// state or province&lt;/span&gt;
  &lt;span class='s1'&gt;&amp;#39;&amp;#39;&lt;/span&gt;                      &lt;span class='c1'&gt;// country&lt;/span&gt;
  &lt;span class='p'&gt;]);&lt;/span&gt;

&lt;span class='c1'&gt;// addItem is called for each item in the shopping cart.&lt;/span&gt;
&lt;span class='nx'&gt;_snaq&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='nx'&gt;push&lt;/span&gt;&lt;span class='p'&gt;([&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;addItem&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
  &lt;span class='nx'&gt;orderId&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;                &lt;span class='c1'&gt;// order ID - required&lt;/span&gt;
  &lt;span class='s1'&gt;&amp;#39;1001&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;                 &lt;span class='c1'&gt;// SKU - required&lt;/span&gt;
  &lt;span class='s1'&gt;&amp;#39;Blue t-shirt&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;         &lt;span class='c1'&gt;// product name&lt;/span&gt;
  &lt;span class='s1'&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;                     &lt;span class='c1'&gt;// category&lt;/span&gt;
  &lt;span class='s1'&gt;&amp;#39;2000&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;                 &lt;span class='c1'&gt;// unit price - required&lt;/span&gt;
  &lt;span class='s1'&gt;&amp;#39;2&amp;#39;&lt;/span&gt;                     &lt;span class='c1'&gt;// quantity - required&lt;/span&gt;
  &lt;span class='p'&gt;]);&lt;/span&gt;
&lt;span class='nx'&gt;_snaq&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='nx'&gt;push&lt;/span&gt;&lt;span class='p'&gt;([&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;addItem&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
  &lt;span class='nx'&gt;orderId&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;                &lt;span class='c1'&gt;// order ID - required&lt;/span&gt;
  &lt;span class='s1'&gt;&amp;#39;1002&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;                 &lt;span class='c1'&gt;// SKU - required&lt;/span&gt;
  &lt;span class='s1'&gt;&amp;#39;Red shoes&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;            &lt;span class='c1'&gt;// product name&lt;/span&gt;
  &lt;span class='s1'&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;                     &lt;span class='c1'&gt;// category&lt;/span&gt;
  &lt;span class='s1'&gt;&amp;#39;4000&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;                 &lt;span class='c1'&gt;// unit price - required&lt;/span&gt;
  &lt;span class='s1'&gt;&amp;#39;1&amp;#39;&lt;/span&gt;                     &lt;span class='c1'&gt;// quantity - required&lt;/span&gt;
  &lt;span class='p'&gt;]);&lt;/span&gt;

&lt;span class='c1'&gt;// trackTrans sends the transaction to SnowPlow tracking servers.&lt;/span&gt;
&lt;span class='c1'&gt;// Must be called last to commit the transaction.&lt;/span&gt;
&lt;span class='nx'&gt;_snaq&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='nx'&gt;push&lt;/span&gt;&lt;span class='p'&gt;([&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;trackTrans&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;]);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;The above example creates an order (aka &amp;#8220;transaction&amp;#8221;) with ID &lt;code&gt;order-123&lt;/code&gt; and then adds two line items (two blue t-shirts and one pair of red shoes) as line items to the order. The final &lt;code&gt;trackTrans&lt;/code&gt; call sends this complete order to SnowPlow as three separate events - one each for the order and its line items.&lt;/p&gt;

&lt;p&gt;This new functionality should be useful for anybody who wants to track orders transacted in a online shopping cart such as Magento, PrestaShop or Spree.&lt;/p&gt;

&lt;p&gt;Note that we haven&amp;#8217;t yet added extracting these e-commerce orders to our ETL process, but we have an &lt;a href='https://github.com/snowplow/snowplow/issues/34'&gt;open ticket for this&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id='upgrading'&gt;Upgrading&lt;/h2&gt;

&lt;p&gt;We have made the minified JavaScript tracker version 0.6 available on this URL:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http://d1fc8wv8zag5ca.cloudfront.net/0.6/sp.js&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There are no breaking changes with the previous version 0.5, so you can upgrade your existing SnowPlow JavaScript tracker without issue.&lt;/p&gt;

&lt;p&gt;Note that we have now added versioning to the JavaScript tracker&amp;#8217;s URL. This is because we have &amp;#8220;breaking changes&amp;#8221; to the JavaScript tracker in the pipeline (see e.g. issues &lt;a href='https://github.com/snowplow/snowplow/issues/29'&gt;#29&lt;/a&gt; and &lt;a href='https://github.com/snowplow/snowplow/issues/32'&gt;#32&lt;/a&gt;).&lt;/p&gt;

&lt;h2 id='thanks'&gt;Thanks&lt;/h2&gt;

&lt;p&gt;A final note to say thanks again to &lt;a href='https://github.com/ramn'&gt;Simon Andersson&lt;/a&gt; for contributing the ecommerce tracking functionality! Community contributors like Simon A and Simon R(umble) are helping us to quickly make the SnowPlow vision a reality.&lt;/p&gt;

&lt;p&gt;And of course, we welcome contributions across the five SnowPlow sub-systems. If you would like help implementing a new tracker, trying a different ETL approach or loading SnowPlow events into an alternative database, please &lt;a href='mailto:contribute@snowplowanalytics.com'&gt;get in touch&lt;/a&gt;!&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/08/21/amazon-glacier-launch</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/08/21/amazon-glacier-launch"/>
    <title>Amazon announces Glacier - lowers the cost of running SnowPlow</title>
    <updated>2012-08-21T00:00:00+01:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;Today Amazon announced the launch of &lt;a href='http://aws.amazon.com/glacier/'&gt;Amazon Glacier&lt;/a&gt;, which is a low-cost data archiving service designed for rarely accessed data.&lt;/p&gt;

&lt;p&gt;As Werner Vogels described it in his &lt;a href='http://www.allthingsdistributed.com/2012/08/amazon-glacier.html'&gt;blog post&lt;/a&gt; this morning:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Amazon Glacier provides the same high durability guarantee as Amazon S3 but relaxes the access times to a few hours. This is the right service for customers who have archival data that requires highly reliable storage but for which immediate access is not needed.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;At first sight, Amazon Glacier looks to be a fantastic fit for archiving the raw event logs generated by the SnowPlow collector (whether the CloudFront collector or alternatives such as &lt;a href='https://github.com/shermozle/SnowCannon'&gt;SnowCannon&lt;/a&gt;). Once the nightly SnowPlow ETL has been run on your raw event logs, you shouldn&amp;#8217;t need to access those raw logs frequently. However, we would always recommend retaining them, as there may well be a reason to revisit them in the future. We never recommend throwing away atomic source data!&lt;/p&gt;

&lt;p&gt;This is where Amazon Glacier comes in - at the proposed pricing levels for Glacier, you could archive 2 terabytes of raw SnowPlow data for around $20 a month; this would be significantly cheaper than storing your raw logs in Amazon S3, which is the current SnowPlow approach.&lt;/p&gt;

&lt;p&gt;Moreover, Werner has indicated that:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;In the coming months, Amazon S3 will introduce an option that will allow customers to seamlessly move data between Amazon S3 and Amazon Glacier based on data lifecycle policies.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Once Amazon has launched this feature, we&amp;#8217;ll get this automatic S3-&amp;gt;Glacier archiving process working internally, and then release a howto for SnowPlow users so you can do the same, and start running your SnowPlow over Amazon Glacier!&lt;/p&gt;

&lt;p&gt;Exciting times for everybody who likes storing atomic event data cheaply and safely - stay tuned!&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/08/20/snowplow-0.4.6-released</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/08/20/snowplow-0.4.6-released"/>
    <title>SnowPlow 0.4.6 released</title>
    <updated>2012-08-20T00:00:00+01:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;Over the weekend we released SnowPlow version &lt;strong&gt;0.4.6&lt;/strong&gt;. This was a minor release that added a new capability into the SnowPlow JavaScript tracker.&lt;/p&gt;

&lt;p&gt;Specifically, with the JavaScript you can now specify your own collector URL, rather than simply pass in an account ID which resolves to a CloudFront bucket.&lt;/p&gt;

&lt;p&gt;You can use this feature in your JavaScript invocation code like so:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='javascript'&gt;&lt;span class='c'&gt;&amp;lt;!--&lt;/span&gt; &lt;span class='nx'&gt;SnowPlow&lt;/span&gt; &lt;span class='nx'&gt;starts&lt;/span&gt; &lt;span class='nx'&gt;plowing&lt;/span&gt; &lt;span class='o'&gt;--&amp;gt;&lt;/span&gt;
&lt;span class='o'&gt;&amp;lt;&lt;/span&gt;&lt;span class='nx'&gt;script&lt;/span&gt; &lt;span class='nx'&gt;type&lt;/span&gt;&lt;span class='o'&gt;=&lt;/span&gt;&lt;span class='s2'&gt;&amp;quot;text/javascript&amp;quot;&lt;/span&gt;&lt;span class='o'&gt;&amp;gt;&lt;/span&gt;
&lt;span class='kd'&gt;var&lt;/span&gt; &lt;span class='nx'&gt;_snaq&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='nx'&gt;_snaq&lt;/span&gt; &lt;span class='o'&gt;||&lt;/span&gt; &lt;span class='p'&gt;[];&lt;/span&gt;

&lt;span class='nx'&gt;_snaq&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='nx'&gt;push&lt;/span&gt;&lt;span class='p'&gt;([&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;setCollectorUrl&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;collector.mydomain.com&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;]);&lt;/span&gt;
&lt;span class='nx'&gt;_snaq&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='nx'&gt;push&lt;/span&gt;&lt;span class='p'&gt;([&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;trackPageView&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;]);&lt;/span&gt;

&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='kd'&gt;function&lt;/span&gt;&lt;span class='p'&gt;()&lt;/span&gt; &lt;span class='p'&gt;{&lt;/span&gt;
&lt;span class='p'&gt;...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;Where &lt;code&gt;collector.mydomain.com&lt;/code&gt; is the URL to your own collector.&lt;/p&gt;

&lt;p&gt;We added this capability to SnowPlow in support of Simon Rumble&amp;#8217;s excellent &lt;a href='https://github.com/shermozle/SnowCannon'&gt;SnowCannon&lt;/a&gt; prototype node.js collector for SnowPlow. Going forwards you can of course use this custom URL to send your SnowPlow events to any kind of collector on a domain you control.&lt;/p&gt;

&lt;p&gt;Anyway I hope you like the feature and let us know how you get on with it!&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/08/14/updated-hive-serde-released</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/08/14/updated-hive-serde-released"/>
    <title>Updated Hive SerDe released</title>
    <updated>2012-08-14T00:00:00+01:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;One of the key elements in the SnowPlow technology stack is the Hive SerDe. This is what makes it possible for Elastic MapReduce to read the Cloudfront log files generated by the SnowPlow javascript trackings tags, extarct the relevant fields and make these available in Hive as a nice, clean query table. (The structure of the Hive table is documented &lt;a href='https://github.com/snowplow/snowplow/wiki/Hive-data-structure'&gt;here&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;A number of improvements have been made in the new versions. However, the most significant is that the 5 utm_marketing fields have been added, so that campaign attributes are now available for analytics.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/08/13/introducing-snow-cannon-a-node-js-collector-for-snowplow</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/08/13/introducing-snow-cannon-a-node-js-collector-for-snowplow"/>
    <title>SnowCannon - a node.js collector for SnowPlow</title>
    <updated>2012-08-13T00:00:00+01:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We are hugely excited to introduce &lt;a href='https://github.com/shermozle/SnowCannon'&gt;SnowCannon&lt;/a&gt;, a Node.js collector for SnowPlow, authored by &lt;a href='http://twitter.com/shermozle'&gt;@shermozle&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;SnowCannon is an alternative collector to the default cloudfront collector included with SnowPlow. It offers a number of significant advantages over the Cloudfront connector:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;It allows the use of 3rd party cookies. In particular, this makes it possible to track usage across multiple domains&lt;/li&gt;

&lt;li&gt;It enables real-time analytics. (This is not possible with the Cloudfront-enabled collector, where there&amp;#8217;s a 20-30 minute delay between the javascript tracking event and the associated log being written to S3.)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To learn more about SnowCannon, visit the &lt;a href='https://github.com/shermozle/SnowCannon'&gt;Github repo&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;SnowCannon is the first user-contributed module for SnowPlow, and we are delighted to see community members working to build out the SnowPlow platform. There are other contributions in the works, including a SnowPlow IOS client, that we hope to be announcing shortly.&lt;/p&gt;

&lt;p&gt;To encourage users to extend SnowPlow, we&amp;#8217;ve architected SnowPlow in a module way, to enable developers to swap out elements in the SnowPlow stack with their own elements or complimenet those already in the stack with parallel implementations. Learn more about the SnowPlow architecture &lt;a href='/product/technical-architecture.html'&gt;here&lt;/a&gt;.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/08/02/snowplow-setup-documentation-overhauled</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/08/02/snowplow-setup-documentation-overhauled"/>
    <title>The setup guide has been overhauled</title>
    <updated>2012-08-02T00:00:00+01:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;Following a lot of invaluable feedback from users setting up SnowPlow for the first time, we&amp;#8217;ve updated the SnowPlow setup documentation.&lt;/p&gt;

&lt;p&gt;The documentation can be found &lt;a href='https://github.com/snowplow/snowplow/wiki/SnowPlow-setup-guide'&gt;here&lt;/a&gt;. Any further feedback would be much appreciated - we want to make it as painless as possible for SnowPlow newbies to get up and running&amp;#8230;&lt;/p&gt;</content>
  </entry>
  
 
</feed>