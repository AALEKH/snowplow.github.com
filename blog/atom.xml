<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom">
 
  <title>The Snowplow Analytics Blog</title>
  <link href="http://snowplowanalytics.com/"/>
  <link type="application/atom+xml" rel="self" href="http://snowplowanalytics.com/blog/atom.xml"/>
  <updated>2013-06-05T14:33:56+01:00</updated>
  <id>http://snowplowanalytics.com/</id>
  <author>
    <name>The Snowplow Analytics Team</name>
    <email>contact@snowplowanalytics.com</email>
  </author>

  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/06/03/snowplow-0.8.6-released-with-performance-improvements</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/06/03/snowplow-0.8.6-released-with-performance-improvements"/>
    <title>Snowplow 0.8.6 released with performance improvements</title>
    <updated>2013-06-03T00:00:00+01:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We are very pleased to announce the release of Snowplow &lt;strong&gt;0.8.6&lt;/strong&gt;, with two significant performance-related improvements to the Hadoop ETL. These improvements are:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The Hadoop ETL process is now much faster at processing raw Snowplow log files generated by the CloudFront Collector, because we have &lt;a href='http://snowplowanalytics.com/blog/2013/05/30/dealing-with-hadoops-small-files-problem/'&gt;tackled the Hadoop &amp;#8220;small files problem&amp;#8221;&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;You can now configure your ETL process on Elastic MapReduce to use Task instances alongside your Master and Core instances; optionally these task instances can be spot (bid-based) instances rather than on-demand&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In this post, then, we will cover:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href='/blog/2013/06/03/snowplow-0.8.6-released-with-performance-improvements#speed-up'&gt;The ETL process speed-up&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/06/03/snowplow-0.8.6-released-with-performance-improvements#task-instances'&gt;Using task instances in your ETL process&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/06/03/snowplow-0.8.6-released-with-performance-improvements#upgrading'&gt;Upgrading&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/06/03/snowplow-0.8.6-released-with-performance-improvements#help'&gt;Getting help&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;!--more--&gt;&lt;h2&gt;&lt;a name='speed-up'&gt;1. The ETL process speed-up&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;We are very pleased in this release to finally address &lt;a href='http://snowplowanalytics.com/blog/2013/05/30/dealing-with-hadoops-small-files-problem/'&gt;Hadoop&amp;#8217;s &amp;#8220;small files problem&amp;#8221;&lt;/a&gt; for Snowplow users relying on our CloudFront Collector. As some of you may know, the CloudFront Collector can generate large numbers of very small files - and this is something that can really impede Hadoop&amp;#8217;s performance.&lt;/p&gt;

&lt;p&gt;With this fix in place, ETL processing speeds will be significantly faster if you previously were processing thousands of smaller CloudFront files. In particularly severe cases, we have seen speed-ups of &lt;strong&gt;1,867%&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;For more information on Hadoop&amp;#8217;s small files problem, how badly it was slowing down our ETL process and what we did to fix it, do check out our companion blog post, &lt;a href='http://snowplowanalytics.com/blog/2013/05/30/dealing-with-hadoops-small-files-problem/'&gt;Dealing with Hadoop&amp;#8217;s small files problem&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a name='task-instances'&gt;2. Using task instances in your ETL process&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;With this release you can now add Task instances to your ETL process, alongside your existing Master instance and Core instance(s). The additional configuration options in EmrEtlRunner&amp;#8217;s &lt;code&gt;config.yml&lt;/code&gt; look like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:jobflow:
    ...
    :task_instance_count: 0 # Increase to use spot instances
    :task_instance_type: m1.small
    :task_instance_bid: 0.015 # In USD. Adjust bid, or leave blank for non-spot-priced (i.e. on-demand) task instances&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note the &lt;code&gt;:task_instance_bid:&lt;/code&gt; variable - this lets you bid an upper bound (in US Dollars) that you are willing to pay for Task instances to be added to your job. Leave this blank if you would prefer on-demand Task instances at the standard EMR prices.&lt;/p&gt;

&lt;p&gt;The best introduction to configuring the various instance groups for your job (Master, Core and Task) is in the post &lt;a href='http://aws.typepad.com/aws/2011/08/run-amazon-elastic-mapreduce-on-ec2-spot-instances.html'&gt;Run Amazon Elastic MapReduce on EC2 Spot Instances&lt;/a&gt; on the Amazon Web Services Blog. In the language of this blog post, the Snowpow ETL process is typically a &lt;strong&gt;Data-Critical Workload&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&amp;#8220;If the overall cost is more important than the time to completion and you don&amp;#8217;t want to lose any partial work, run the Master and Core instance groups on On-Demand instances, making sure that you run enough Core instance groups to hold all of your data in HDFS. Add Spot Instances as needed to reduce the overall processing speed and the total cost.&amp;#8221;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;We recommend you experiment with different Task instance configurations (including different bids) to find the best cost-time balance for you.&lt;/p&gt;
&lt;h2&gt;&lt;a name='upgrading'&gt;3. Upgrading&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;There are &lt;strong&gt;two components&lt;/strong&gt; to upgrade in this release:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The Scalding ETL, to version 0.3.2&lt;/li&gt;

&lt;li&gt;EmrEtlRunner, to version 0.3.0&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Let&amp;#8217;s take these in turn:&lt;/p&gt;

&lt;h3 id='hadoop_etl'&gt;Hadoop ETL&lt;/h3&gt;

&lt;p&gt;If you are using EmrEtlRunner, you need to update your configuration file, &lt;code&gt;config.yml&lt;/code&gt;, to the latest version of the Hadoop ETL:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:snowplow:
  :hadoop_etl_version: 0.3.2 # Version of the Hadoop ETL&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id='emretlrunner'&gt;EmrEtlRunner&lt;/h3&gt;

&lt;p&gt;You need to upgrade your EmrEtlRunner installation to the latest code (0.8.6 release) on GitHub:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ git clone git://github.com/snowplow/snowplow.git
$ git checkout 0.8.6&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next, you need to update the format of your &lt;code&gt;config.yml&lt;/code&gt;, specifically the &lt;code&gt;:jobflow:&lt;/code&gt; section. The new format looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:jobflow:
    :master_instance_type: m1.small
    :core_instance_count: 2
    :core_instance_type: m1.small
    :task_instance_count: 0 # Increase to use spot instances
    :task_instance_type: m1.small
    :task_instance_bid: 0.015 # In USD. Adjust bid, or leave blank for non-spot-priced (i.e. on-demand) task instances&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Note that almost all of these variables are either new or renamed.&lt;/strong&gt; For recommended settings for the new &lt;code&gt;task_instance&lt;/code&gt; settings, please see &lt;a href='/blog/2013/06/03/snowplow-0.8.6-released-with-performance-improvements#task-instances'&gt;Using task instances in your ETL process&lt;/a&gt; above.&lt;/p&gt;
&lt;h2&gt;&lt;a name='help'&gt;4. Getting help&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;As always, if you do run into any issues or don&amp;#8217;t understand any of the above changes, please &lt;a href='https://github.com/snowplow/snowplow/issues'&gt;raise an issue&lt;/a&gt; or get in touch with us via &lt;a href='https://github.com/snowplow/snowplow/wiki/Talk-to-us'&gt;the usual channels&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;You can see the full list of issues delivered in Snowplow 0.8.6 on &lt;a href='https://github.com/snowplow/snowplow/issues?milestone=22&amp;amp;page=1&amp;amp;state=closed'&gt;GitHub&lt;/a&gt;.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/05/30/dealing-with-hadoops-small-files-problem</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/05/30/dealing-with-hadoops-small-files-problem"/>
    <title>Dealing with Hadoop's small files problem</title>
    <updated>2013-05-30T00:00:00+01:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;Hadoop has a serious Small File Problem. It&amp;#8217;s widely known that Hadoop struggles to run MapReduce jobs that involve thousands of small files: Hadoop much prefers to crunch through tens or hundreds of files sized at or around the magic 128 megabytes. The technical reasons for this are well explained in this &lt;a href='http://blog.cloudera.com/blog/2009/02/the-small-files-problem/'&gt;Cloudera blog post&lt;/a&gt; - what is less well understood is how badly small files can slow down your Hadoop job, and what to do about it.&lt;/p&gt;
&lt;img src='/static/img/blog/2013/05/plowing-small-files.jpg' /&gt;
&lt;p&gt;In this blog post we will discuss the small files problem in terms of our experiences with it at Snowplow. &lt;strong&gt;And we will argue that dealing with the small files problem - if you have it - is the single most important optimisation you can perform on your MapReduce process.&lt;/strong&gt;&lt;/p&gt;
&lt;!--more--&gt;
&lt;h2 id='background'&gt;Background&lt;/h2&gt;

&lt;p&gt;To give some necessary background on our architecture: Snowplow event trackers send user events to a pixel hosted on CloudFront, which logs those raw events to Amazon S3. Amazon&amp;#8217;s CloudFront logging generates many small log files in S3: a relatively low-traffic e-commerce site using Snowplow generated 26,372 CloudFront log files over a six month period, containing 83,110 events - that&amp;#8217;s just 3.2 events per log file.&lt;/p&gt;

&lt;p&gt;Once the events have been collected in S3, &lt;a href='https://github.com/snowplow/snowplow/tree/master/3-enrich/hadoop-etl'&gt;Snowplow&amp;#8217;s Hadoop job&lt;/a&gt; (written in &lt;a href='https://github.com/twitter/scalding/wiki'&gt;Scalding&lt;/a&gt;) processes them, validating them and then enriching them with referer, geo-location and similar data; these enriched events are then written back to S3.&lt;/p&gt;

&lt;p&gt;So you can see how our Enrichment process ran pretty directly into Hadoop&amp;#8217;s small files problem. But quantifying the impact of small files on our job&amp;#8217;s performance was impossible until we had a solution in place&amp;#8230;&lt;/p&gt;

&lt;h2 id='quantifying_the_small_file_problem'&gt;Quantifying the small file problem&lt;/h2&gt;

&lt;p&gt;This week we implemented a solution to aggregate our tiny CloudFront logs into more sensibly sized input files - this enhancement will be released as part of &lt;a href='https://github.com/snowplow/snowplow/issues?milestone=22&amp;amp;page=1&amp;amp;state=open'&gt;Snowplow 0.8.6&lt;/a&gt; shortly.&lt;/p&gt;

&lt;p&gt;In testing this code and running before- and after- performance tests, we realised just how badly the small file problem was slowing down our Enrichment process. This screenshot shows you what we found:&lt;/p&gt;
&lt;img src='/static/img/blog/2013/05/small-files-before-after.png' /&gt;
&lt;p&gt;That&amp;#8217;s right - aggregating with the small files first reduced total processing time from 2 hours 57 minutes to just 9 minutes - of which 3 minutes was the aggregation, and 4 minutes was running our actual Enrichment process. That&amp;#8217;s a speedup of &lt;strong&gt;1,867%&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;To make the comparison as helpful as possible, here is the exact specification of the before- and after- test. We have also added a second after- run with a more realistic cluster size.&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Metric&lt;/th&gt;&lt;th&gt;Before (with small files)&lt;/th&gt;&lt;th&gt;After (with small files aggregated)&lt;/th&gt;&lt;th&gt;After (with smaller cluster)&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;strong&gt;Source log files&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;26,372&lt;/td&gt;&lt;td style='text-align: left;'&gt;26,372&lt;/td&gt;&lt;td style='text-align: left;'&gt;26,372&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;strong&gt;Files read by job&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;Source log files&lt;/td&gt;&lt;td style='text-align: left;'&gt;Aggregated log files&lt;/td&gt;&lt;td style='text-align: left;'&gt;Aggregated log files&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;strong&gt;Location of files&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;Amazon S3&lt;/td&gt;&lt;td style='text-align: left;'&gt;HDFS on Core instances&lt;/td&gt;&lt;td style='text-align: left;'&gt;HDFS on Core instances&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;strong&gt;File compression&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;Gzip&lt;/td&gt;&lt;td style='text-align: left;'&gt;LZO&lt;/td&gt;&lt;td style='text-align: left;'&gt;LZO&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;strong&gt;part- files out&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;23,618&lt;/td&gt;&lt;td style='text-align: left;'&gt;141&lt;/td&gt;&lt;td style='text-align: left;'&gt;141&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;strong&gt;Events out&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;83,110&lt;/td&gt;&lt;td style='text-align: left;'&gt;83,110&lt;/td&gt;&lt;td style='text-align: left;'&gt;83,110&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;strong&gt;Cluster&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;1 x m1.large, 18 x m1.medium&lt;/td&gt;&lt;td style='text-align: left;'&gt;1 x m1.large, 18 x m1.medium&lt;/td&gt;&lt;td style='text-align: left;'&gt;1 x m1.small, 1 x m1.small&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;strong&gt;Execution time&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;&lt;strong&gt;177 minutes&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;&lt;strong&gt;9 minutes&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;&lt;strong&gt;39 minutes&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;strong&gt;Aggregate step time&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;-&lt;/td&gt;&lt;td style='text-align: left;'&gt;3 minutes&lt;/td&gt;&lt;td style='text-align: left;'&gt;11 minutes&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;strong&gt;ETL step time&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;166 minutes&lt;/td&gt;&lt;td style='text-align: left;'&gt;4 minutes&lt;/td&gt;&lt;td style='text-align: left;'&gt;25 minutes&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;strong&gt;Norm. instance hours&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;120&lt;/td&gt;&lt;td style='text-align: left;'&gt;40&lt;/td&gt;&lt;td style='text-align: left;'&gt;2&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Health warning:&lt;/strong&gt; this is one single benchmark, measuring the performance of the &lt;a href='https://github.com/snowplow/snowplow/tree/master/3-enrich/hadoop-etl'&gt;Snowplow Hadoop job&lt;/a&gt; using a single data set. We encourage you to run your own benchmarks.&lt;/p&gt;

&lt;p&gt;This is an astonishing speed-up, which shows how badly the small files problem was impacting our Hadoop job. And aggregating the small files had another beneficial effect: the much smaller number of &lt;code&gt;part-&lt;/code&gt; output files meant much faster loading of events into Redshift.&lt;/p&gt;

&lt;p&gt;So how did we fix the small files problem for Snowplow? In the next section we will discuss possible solutions for you to consider, and in the last section we will go into some more detail on the solution we chose.&lt;/p&gt;

&lt;h2 id='options_for_dealing_with_small_files_on_hadoop'&gt;Options for dealing with small files on Hadoop&lt;/h2&gt;

&lt;p&gt;As we did our background research into solutions to the small files problem, three main schools of thought emerged:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Change your &amp;#8220;feeder&amp;#8221; software&lt;/strong&gt; so it doesn&amp;#8217;t produce small files (or perhaps files at all). In other words, if small files are the problem, change your upstream code to stop generating them&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Run an offline aggregation process&lt;/strong&gt; which aggregates your small files and re-uploads the aggregated files ready for processing&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Add an additional Hadoop step&lt;/strong&gt; to the start of your jobflow which aggregates the small files&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For us, option 1 was out of the question, as we have no control over how CloudFront writes its log files.&lt;/p&gt;

&lt;p&gt;Option 2 was interesting - and we have had Snowplow users such as 99designs successfully adopt this approach; if you are interested in exploring this further, &lt;a href='https://github.com/larsyencken'&gt;Lars Yencken&lt;/a&gt; from 99designs has shared a &lt;a href='https://gist.github.com/larsyencken/4076413'&gt;CloudFront log aggregation script in Python&lt;/a&gt; as a gist. However, overall option 2 seemed to us to introduce more complexity - with a new long-running process to run, and potentially fragility - with a manifest file now to maintain. We had super-interesting discussions with the Snowplow community about this in &lt;a href='https://groups.google.com/forum/#!topic/snowplow-user/xdhegsztJlA'&gt;this Google Groups thread&lt;/a&gt; and &lt;a href='https://github.com/snowplow/snowplow/issues/82'&gt;this GitHub issue&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In the end, though, we opted for option 3, for a few reasons:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;We are starting a Hadoop cluster anyway to run our ETL job, so this reduces the number of additional moving parts required&lt;/li&gt;

&lt;li&gt;We hoped for performance improvements in moving the small files to the cluster&amp;#8217;s local HDFS filesystem during the aggregation&lt;/li&gt;

&lt;li&gt;We hoped that accessing Amazon S3 from a cluster rather than a single machine would mean more parallel connections to S3&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With that decided, we then looked for options to aggregate and compact small files on Hadoop, identifying three possible solutions:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href='https://github.com/edwardcapriolo/filecrush'&gt;&lt;strong&gt;filecrush&lt;/strong&gt;&lt;/a&gt; - a highly configurable tool by &lt;a href='https://github.com/edwardcapriolo'&gt;Edward Capriolo&lt;/a&gt; to &amp;#8220;crush&amp;#8221; small files on HDFS. It supports a rich set of configuration arguments and is available as a jarfile (&lt;a href='http://www.jointhegrid.com/hadoop_filecrush/'&gt;download it here&lt;/a&gt;) ready to run on your cluster. It&amp;#8217;s a sophisticated tool - for example, by default it won&amp;#8217;t bother crushing a file which is within 75% of the HDFS block size already. Unfortunately, it does not work yet with Amazon&amp;#8217;s s3:// paths, only hdfs:// paths - and our &lt;a href='https://github.com/edwardcapriolo/filecrush/pull/2'&gt;pull request&lt;/a&gt; to add this functionality is incomplete&lt;/li&gt;

&lt;li&gt;&lt;a href='https://github.com/nathanmarz/dfs-datastores/blob/develop/dfs-datastores/src/main/java/com/backtype/hadoop/Consolidator.java'&gt;&lt;strong&gt;Consolidator&lt;/strong&gt;&lt;/a&gt; - a Hadoop file consolidation tool from the &lt;a href='https://github.com/nathanmarz/dfs-datastores'&gt;dfs-datastores&lt;/a&gt; library, written by &lt;a href='https://github.com/nathanmarz'&gt;Nathan Marz&lt;/a&gt;. There is scant documentation for this - we could only find one paragraph, &lt;a href='https://groups.google.com/forum/?fromgroups#!topic/cascalog-user/ovYSq2vTyYE'&gt;in this email thread&lt;/a&gt;. It has fewer capabilities than filecrush, and could do with a CLI-like wrapper to invoke it (we started writing just such a wrapper but then we found filecrush)&lt;/li&gt;

&lt;li&gt;&lt;a href='http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/UsingEMR_s3distcp.html'&gt;&lt;strong&gt;S3DistCp&lt;/strong&gt;&lt;/a&gt; - created by Amazon as an S3-friendly adaptation of Hadoop&amp;#8217;s &lt;a href='http://hadoop.apache.org/docs/stable/distcp.html'&gt;DistCp&lt;/a&gt; utility for HDFS. Don&amp;#8217;t be fooled by the name - if you are running on Elastic MapReduce, then this can deal with your small files problem using its &lt;code&gt;--groupBy&lt;/code&gt; option for aggregating files (which the original DistCp seems to lack)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;After trying to work with filecrush and Consolidator, ultimately we went with S3DistCp for Snowplow. In the next section, we will look at exactly how we set it up.&lt;/p&gt;

&lt;h2 id='running_s3distcp'&gt;Running S3DistCp&lt;/h2&gt;

&lt;p&gt;Once we had chosen S3DistCp, we had to update our ETL process to include it in our jobflow. Luckily, the &lt;a href='http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/UsingEMR_s3distcp.html'&gt;S3DistCp documentation&lt;/a&gt; has an example on aggregating CloudFront files:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./elastic-mapreduce --jobflow j-3GY8JC4179IOK --jar \
/home/hadoop/lib/emr-s3distcp-1.0.jar \
--args &amp;#39;--src,s3://myawsbucket/cf,\
--dest,hdfs:///local,\
--groupBy,.*XABCD12345678.([0-9]+-[0-9]+-[0-9]+-[0-9]+).*,\
--targetSize,128,\
--outputCodec,lzo,--deleteOnSuccess&amp;#39;&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that as well as aggregating the small files into 128 megabyte files, this step also changes the encoding to &lt;a href='http://en.wikipedia.org/wiki/Lempel%E2%80%93Ziv%E2%80%93Oberhumer'&gt;LZO&lt;/a&gt;. As the Amazon documentation explains it:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&amp;#8220;Data compressed using LZO can be split into multiple maps as it is decompressed, so you don&amp;#8217;t have to wait until the compression is complete, as you do with Gzip. This provides better performance when you analyze the data using Amazon EMR.&amp;#8221;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;We only needed to make a few changes to this example code for our own ETL:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;We can&amp;#8217;t predict the prefix on the CloudFront log files - and it certainly won&amp;#8217;t be &lt;code&gt;XABCD12345678&lt;/code&gt;, so it made more sense to drop this&lt;/li&gt;

&lt;li&gt;Grouping files to the hour is overkill - we can roll up to the day and S3DistCp will happily split files if they are larger than &lt;code&gt;targetSize&lt;/code&gt;&lt;/li&gt;

&lt;li&gt;&lt;code&gt;--deleteOnSuccess&lt;/code&gt; is dangerous - we don&amp;#8217;t want to delete our source data and leave the only copy on a transient Hadoop cluster&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Given the above, our updated &lt;code&gt;--groupBy&lt;/code&gt; regular expression was:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;quot;.*\.([0-9]+-[0-9]+-[0-9]+)-[0-9]+\..*&amp;quot;&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, all we needed to do was add the call to S3DistCp into our jobflow before our main ETL step. We use the excellent &lt;a href='https://github.com/rslifka/elasticity'&gt;Elasticity&lt;/a&gt; Ruby library by &lt;a href='https://github.com/rslifka'&gt;Rob Slifka&lt;/a&gt; to execute our jobs, so calling S3DistCp was a matter of adding the extra step to our jobflow in Ruby:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='ruby'&gt;&lt;span class='n'&gt;hadoop_input&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='s2'&gt;&amp;quot;hdfs:///local/snowplow-logs&amp;quot;&lt;/span&gt;

&lt;span class='c1'&gt;# Create the Hadoop MR step for the file crushing&lt;/span&gt;
&lt;span class='n'&gt;filecrush_step&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='ss'&gt;Elasticity&lt;/span&gt;&lt;span class='p'&gt;:&lt;/span&gt;&lt;span class='ss'&gt;:CustomJarStep&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;new&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;config&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='ss'&gt;:s3distcp_asset&lt;/span&gt;&lt;span class='o'&gt;]&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='c1'&gt;# s3distcp_asset is &amp;quot;/home/hadoop/lib/emr-s3distcp-1.0.jar&amp;quot;&lt;/span&gt;

&lt;span class='n'&gt;filecrush_step&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;arguments&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='o'&gt;[&lt;/span&gt;
  &lt;span class='s2'&gt;&amp;quot;--src&amp;quot;&lt;/span&gt;               &lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;config&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='ss'&gt;:s3&lt;/span&gt;&lt;span class='o'&gt;][&lt;/span&gt;&lt;span class='ss'&gt;:buckets&lt;/span&gt;&lt;span class='o'&gt;][&lt;/span&gt;&lt;span class='ss'&gt;:processing&lt;/span&gt;&lt;span class='o'&gt;]&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
  &lt;span class='s2'&gt;&amp;quot;--dest&amp;quot;&lt;/span&gt;              &lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;hadoop_input&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
  &lt;span class='s2'&gt;&amp;quot;--groupBy&amp;quot;&lt;/span&gt;           &lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='s2'&gt;&amp;quot;.*&lt;/span&gt;&lt;span class='se'&gt;\\&lt;/span&gt;&lt;span class='s2'&gt;.([0-9]+-[0-9]+-[0-9]+)-[0-9]+&lt;/span&gt;&lt;span class='se'&gt;\\&lt;/span&gt;&lt;span class='s2'&gt;..*&amp;quot;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
  &lt;span class='s2'&gt;&amp;quot;--targetSize&amp;quot;&lt;/span&gt;        &lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='s2'&gt;&amp;quot;128&amp;quot;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
  &lt;span class='s2'&gt;&amp;quot;--outputCodec&amp;quot;&lt;/span&gt;       &lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='s2'&gt;&amp;quot;lzo&amp;quot;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
  &lt;span class='s2'&gt;&amp;quot;--s3Endpoint&amp;quot;&lt;/span&gt;        &lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;config&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='ss'&gt;:s3&lt;/span&gt;&lt;span class='o'&gt;][&lt;/span&gt;&lt;span class='ss'&gt;:endpoint&lt;/span&gt;&lt;span class='o'&gt;]&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='o'&gt;]&lt;/span&gt;

&lt;span class='c1'&gt;# Add to our jobflow&lt;/span&gt;
&lt;span class='vi'&gt;@jobflow&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;add_step&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;filecrush_step&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And then we had to update our ETL job to take the &lt;code&gt;--dest&lt;/code&gt; of the S3DistCp step as its own input:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='ruby'&gt;&lt;span class='n'&gt;hadoop_step&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;arguments&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='o'&gt;[&lt;/span&gt;
            &lt;span class='s2'&gt;&amp;quot;com.snowplowanalytics.snowplow.enrich.hadoop.EtlJob&amp;quot;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='c1'&gt;# Job to run&lt;/span&gt;
            &lt;span class='s2'&gt;&amp;quot;--hdfs&amp;quot;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='c1'&gt;# Always --hdfs mode, never --local&lt;/span&gt;
            &lt;span class='s2'&gt;&amp;quot;--input_folder&amp;quot;&lt;/span&gt;      &lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;hadoop_input&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='c1'&gt;# Output of our S3DistCp step&lt;/span&gt;
            &lt;span class='s2'&gt;&amp;quot;--input_format&amp;quot;&lt;/span&gt;      &lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;config&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='ss'&gt;:etl&lt;/span&gt;&lt;span class='o'&gt;][&lt;/span&gt;&lt;span class='ss'&gt;:collector_format&lt;/span&gt;&lt;span class='o'&gt;]&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
            &lt;span class='s2'&gt;&amp;quot;--maxmind_file&amp;quot;&lt;/span&gt;      &lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;config&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='ss'&gt;:maxmind_asset&lt;/span&gt;&lt;span class='o'&gt;]&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
            &lt;span class='s2'&gt;&amp;quot;--output_folder&amp;quot;&lt;/span&gt;     &lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;partition&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;call&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;config&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='ss'&gt;:s3&lt;/span&gt;&lt;span class='o'&gt;][&lt;/span&gt;&lt;span class='ss'&gt;:buckets&lt;/span&gt;&lt;span class='o'&gt;][&lt;/span&gt;&lt;span class='ss'&gt;:out&lt;/span&gt;&lt;span class='o'&gt;]&lt;/span&gt;&lt;span class='p'&gt;),&lt;/span&gt;
            &lt;span class='s2'&gt;&amp;quot;--bad_rows_folder&amp;quot;&lt;/span&gt;   &lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;partition&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;call&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;config&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='ss'&gt;:s3&lt;/span&gt;&lt;span class='o'&gt;][&lt;/span&gt;&lt;span class='ss'&gt;:buckets&lt;/span&gt;&lt;span class='o'&gt;][&lt;/span&gt;&lt;span class='ss'&gt;:out_bad_rows&lt;/span&gt;&lt;span class='o'&gt;]&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt;
          &lt;span class='o'&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And that was it! If you want to see all of the code excerpted above, you can find it in the &lt;a href='https://github.com/snowplow/snowplow/blob/feature/perf-improvements/3-enrich/emr-etl-runner/lib/snowplow-emr-etl-runner/emr_jobs.rb'&gt;Snowplow project on GitHub&lt;/a&gt;. We did not have to make any changes to our main Hadoop ETL job, because Elastic MapReduce can handle LZO-compressed files invisibly to the job reading them. And no doubt the switch to LZO also contributed to the excellent performance we saw above.&lt;/p&gt;

&lt;p&gt;So that&amp;#8217;s everything - hopefully this post has helped to illustrate just how badly small files can slow down your Hadoop job, and what you can do about it: if you have a small file problem on Hadoop, there&amp;#8217;s now no excuse not to fix it!&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/05/24/snowplow-0.8.5-released-with-etl-bug-fixes</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/05/24/snowplow-0.8.5-released-with-etl-bug-fixes"/>
    <title>Snowplow 0.8.5 released with ETL bug fixes</title>
    <updated>2013-05-24T00:00:00+01:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We are pleased to announce the immediate availability of Snowplow &lt;strong&gt;0.8.5&lt;/strong&gt;. This is a bug fixing release, following on from our launch last week of Snowplow 0.8.4 with geo-IP lookups.&lt;/p&gt;

&lt;p&gt;This release fixes one showstopper issue with Snowplow 0.8.4, and also includes a set of smaller enhancements to help the Scalding ETL better handle &amp;#8220;bad quality&amp;#8221; event data from webpages. We recommend everybody on the Snowplow 0.8.x series upgrade to this version.&lt;/p&gt;

&lt;p&gt;Many thanks to community members &lt;a href='https://github.com/petervanwesep'&gt;Peter van Wesep&lt;/a&gt; and &lt;a href='https://github.com/rgabo'&gt;Gabor Ratky&lt;/a&gt; for their help identifying and debugging the issues fixed in this release!&lt;/p&gt;

&lt;p&gt;In this post we will cover:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href='/blog/2013/05/24/snowplow-0.8.5-released-with-etl-bug-fixes#showstopper'&gt;The showstopper bug fix&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/05/24/snowplow-0.8.5-released-with-etl-bug-fixes#other-improvements'&gt;Other improvements&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/05/24/snowplow-0.8.5-released-with-etl-bug-fixes#upgrading'&gt;Upgrading&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/05/24/snowplow-0.8.5-released-with-etl-bug-fixes#help'&gt;Getting help&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;!--more--&gt;&lt;h2&gt;&lt;a name='showstopper'&gt;1. The showstopper bug fix&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Many thanks to Peter van Wesep for spotting the &lt;a href='https://github.com/snowplow/snowplow/issues/258'&gt;showstopper issue&lt;/a&gt; in the Snowplow 0.8.4 release: when the Snowplow ETL process was run from an Amazon Web Services account other than Snowplow&amp;#8217;s own, the Hadoop ETL code was unable to read the MaxMind geo-IP data file from an S3:// link hosted from a Snowplow public bucket. This issue did not affect users who are self-hosting the ETL assets.&lt;/p&gt;

&lt;p&gt;This has now been fixed - we now provide the MaxMind geo-IP file on an HTTP:// link, and the Scalding ETL downloads it and adds it to HDFS before running.&lt;/p&gt;
&lt;h2&gt;&lt;a name='other-improvements'&gt;2. Other improvements&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;We have made a series of other improvements to the Scalding ETL, to make it more robust. These improvements are:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;We have widened the &lt;code&gt;page_urlport&lt;/code&gt; and &lt;code&gt;refr_urlport&lt;/code&gt; fields&lt;/li&gt;

&lt;li&gt;We now strip control characters (e.g. nulls) from fields alongside tabs and newlines, to prevent Redshift load errors&lt;/li&gt;

&lt;li&gt;The ETL no longer dies if a huge (larger than an integer) numeric value is sent in for a screen/view dimension&lt;/li&gt;

&lt;li&gt;We have increased the size of &lt;code&gt;se_value&lt;/code&gt; from a float to a double&lt;/li&gt;

&lt;li&gt;&lt;code&gt;se_value&lt;/code&gt; is always now output as a plain string, never in scientific notation, to prevent Redshift load errors&lt;/li&gt;

&lt;li&gt;It is now possible to build the ETL locally (we added a missing dependency to the project configuration)&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a name='upgrading'&gt;3. Upgrading&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;There are &lt;strong&gt;three components&lt;/strong&gt; to upgrade in this release:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The Scalding ETL, to version 0.3.1&lt;/li&gt;

&lt;li&gt;EmrEtlRunner, to version 0.2.1&lt;/li&gt;

&lt;li&gt;The Redshift events table, to version 0.2.1&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Alternatively, if you are still using Infobright with the legacy Hive ETL, you can upgrade your Infobright events table, to version 0.0.9.&lt;/p&gt;

&lt;p&gt;Let&amp;#8217;s take these in turn:&lt;/p&gt;

&lt;h3 id='hadoop_etl'&gt;Hadoop ETL&lt;/h3&gt;

&lt;p&gt;If you are using EmrEtlRunner, you need to update your configuration file, &lt;code&gt;config.yml&lt;/code&gt;, to the latest version of the Hadoop ETL:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:snowplow:
  :hadoop_etl_version: 0.3.1 # Version of the Hadoop ETL&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id='emretlrunner'&gt;EmrEtlRunner&lt;/h3&gt;

&lt;p&gt;You need to upgrade your EmrEtlRunner installation to the latest code (0.8.5 release) on GitHub:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ git clone git://github.com/snowplow/snowplow.git
$ git checkout 0.8.5&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id='redshift_events_table'&gt;Redshift events table&lt;/h3&gt;

&lt;p&gt;We have updated the Redshift table definition - you can find the latest version in the GitHub repository &lt;a href='https://github.com/snowplow/snowplow/blob/master/4-storage/redshift-storage/sql/table-def.sql'&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If you already have your Snowplow data in the previous version of the Redshift events table, we have written &lt;a href='https://github.com/snowplow/snowplow/blob/master/4-storage/redshift-storage/sql/migrate_0.2.0_to_0.2.1.sql'&gt;a migration script&lt;/a&gt; to handle the upgrade. &lt;strong&gt;Please review this script carefully before running and check that you are happy with how it handles the upgrade.&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id='infobright_events_table'&gt;Infobright events table&lt;/h3&gt;

&lt;p&gt;If you are storing your events in Infobright Community Edition, you can also update your table definition. To make this easier for you, we have created a script:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;4-storage/infobright-storage/migrate_008_to_009.sh&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Running this script will create a new table, &lt;code&gt;events_009&lt;/code&gt; (version 0.0.9 of the Infobright table definition) in your &lt;code&gt;snowplow&lt;/code&gt; database, copying across all your data from your existing &lt;code&gt;events_008&lt;/code&gt; table, which will not be modified in any way.&lt;/p&gt;

&lt;p&gt;Once you have run this, don&amp;#8217;t forget to update your StorageLoader&amp;#8217;s &lt;code&gt;config.yml&lt;/code&gt; to load into the new &lt;code&gt;events_009&lt;/code&gt; table, not your old &lt;code&gt;events_008&lt;/code&gt; table:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:storage:
  :table:    events_009 # NOT &amp;quot;events_008&amp;quot; any more&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Done!&lt;/p&gt;
&lt;h2&gt;&lt;a name='help'&gt;4. Getting help&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;As always, if you do run into any issues or don&amp;#8217;t understand any of the above changes, please &lt;a href='https://github.com/snowplow/snowplow/issues'&gt;raise an issue&lt;/a&gt; or get in touch with us via &lt;a href='https://github.com/snowplow/snowplow/wiki/Talk-to-us'&gt;the usual channels&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;You can see the full list of issues delivered in Snowplow 0.8.5 on &lt;a href='https://github.com/snowplow/snowplow/issues?milestone=24&amp;amp;page=1&amp;amp;state=closed'&gt;GitHub&lt;/a&gt;.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/05/22/measuring-how-much-individual-items-in-your-catalog-contribute-to-inbound-marketing</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/05/22/measuring-how-much-individual-items-in-your-catalog-contribute-to-inbound-marketing"/>
    <title>Measuring how much traffic individual items in your catalog drive to your website</title>
    <updated>2013-05-22T00:00:00+01:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We have just added a &lt;a href='/analytics/catalog-analytics/measuring-how-much-traffic-different-items-in-your-catalog-drive-to-your-website.html'&gt;new recipe&lt;/a&gt; to the &lt;a href='/analytics/catalog-analytics/overview.html'&gt;catalog analytics&lt;/a&gt; section of the &lt;a href='/analytics/index.html'&gt;Analytics Cookbook&lt;/a&gt;. This recipe describes:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;How to measure how effectively different items in your catalog drive visits to your website.&lt;/li&gt;

&lt;li&gt;How to use the data to unpick &lt;em&gt;how&lt;/em&gt; each item drives that traffic.&lt;/li&gt;
&lt;/ol&gt;
&lt;a href='/static/img/analytics/catalog-analytics/driving-traffic/2.jpg'&gt;&lt;img src='/static/img/analytics/catalog-analytics/driving-traffic/2-truncated.jpg' /&gt;&lt;/a&gt;
&lt;p&gt;In digital marketing, we can distinguish classic &amp;#8220;outbound marketing&amp;#8221;, where we &lt;em&gt;push&lt;/em&gt; visitors to our website using paid ad campaigns, for example, with &amp;#8221;&lt;a href='http://en.wikipedia.org/wiki/Inbound_marketing'&gt;inbound market&lt;/a&gt;&amp;#8221;, where we &lt;em&gt;pull&lt;/em&gt; visitors into our site by producing attractive, linkable content.&lt;/p&gt;
&lt;!--more--&gt;
&lt;p&gt;Each item in our catalog (be it a product on a retail site, an article on a news website, a post on a blog or a video on a media site) has the potential to contribute to inbound marketing, by:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Providing new content that people might create links to&lt;/li&gt;

&lt;li&gt;Providing additional content, including keywords and links for search engine crawlers&lt;/li&gt;

&lt;li&gt;Providing additional potential landing pages to run advertising against.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This recipe shows how we can measure how effectively different items contribute to inbound marketing, and drill into the data to explore how it contributes. (Which of the three effects above does it support.)&lt;/p&gt;

&lt;p&gt;In our previous recipe for &lt;a href='/analytics/catalog-analytics/measuring-and-comparing-product-page-performance.html'&gt;measuring product page performance&lt;/a&gt;, we showed how we can measure how effectively product pages on a retailer site drive &lt;em&gt;add-to-basket&lt;/em&gt; events. In our previous recipe for measuring &lt;a href='/analytics/catalog-analytics/measuring-and-comparing-content-page-performance.html'&gt;content page performance&lt;/a&gt;, we showed how to measure how much engagement different individual content items drive. &lt;a href='/analytics/catalog-analytics/measuring-how-much-traffic-different-items-in-your-catalog-drive-to-your-website.html'&gt;This recipe&lt;/a&gt; builds on the two previous recipes, by showing how we can measure how good each item is at drawing visitors to our website.&lt;/p&gt;

&lt;p&gt;The recipe uses &lt;a href='http://www.tableausoftware.com/'&gt;Tableau&lt;/a&gt;. It is, however, possible to use alternative OLAP or analysis tools (e.g. &lt;a href='http://www.microsoft.com/en-us/bi/PowerPivot.aspx'&gt;Microsoft PowerPivot&lt;/a&gt; or &lt;a href='http://www.r-project.org/'&gt;R&lt;/a&gt;): the same methodology applies.&lt;/p&gt;

&lt;p&gt;The recipe is available &lt;a href='/analytics/catalog-analytics/measuring-how-much-traffic-different-items-in-your-catalog-drive-to-your-website.html'&gt;here&lt;/a&gt;.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/05/20/performing-market-basket-analysis-with-r-arules-and-snowplow</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/05/20/performing-market-basket-analysis-with-r-arules-and-snowplow"/>
    <title>Performing market basket analysis on web analytics data with R</title>
    <updated>2013-05-20T00:00:00+01:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We have just added a &lt;a href='/analytics/catalog-analytics/market-basket-analysis-identifying-products-that-sell-well-together.html'&gt;new recipe&lt;/a&gt; to the &lt;a href='/analytics/index.html'&gt;Analytics Cookbook&lt;/a&gt;: one that walks through the process of performing a &lt;a href='/analytics/catalog-analytics/market-basket-analysis-identifying-products-that-sell-well-together.html'&gt;market basket analysis&lt;/a&gt;, to identify associations between products and/or content items based on user purchase / viewing behaviour. The recipe covers performing the analysis on Snowplow data using &lt;a href='http://www.r-project.org/'&gt;R&lt;/a&gt; and the &lt;a href='http://cran.r-project.org/web/packages/arules/index.html'&gt;arules&lt;/a&gt; package in particular. Although the example walked through uses Snowplow data, the same approach &lt;em&gt;can&lt;/em&gt; be used with other data sets: I&amp;#8217;d be interested in finding out if members of the #measure community can describe how to do the comparable analysis using data from Google Analytics.&lt;/p&gt;
&lt;p style='text-align: center'&gt;&lt;img src='/static/img/analytics/catalog-analytics/market-basket-analysis/market-basket-analysis-scatter-plot-arulesviz.JPG' width='400' /&gt;&lt;/p&gt;
&lt;p&gt;Market basket analysis is the mining of transaction data to identify associations between different items. This is typically performed by retailers who use it to identify products that a customer is likely to buy, given the products that they have already bought (or added to basket): most famously, it is the approach behind Amazon&amp;#8217;s &lt;em&gt;users who bought this product also bought these items&lt;/em&gt;&amp;#8230;&lt;/p&gt;
&lt;!--more--&gt;
&lt;p&gt;Although people usually think of market basket analysis with respect to retailer transaction data, exactly the same algorithms and approaches can be uses with viewing data on media sites. The results of this type of analysis can be used to inform website design (how items are grouped together) and to power recommendation engines and targeted marketing. (E.g. advertising content items or products that people are more likely to be interested in, based on their past behaviour.)&lt;/p&gt;

&lt;p&gt;This is the first recipe that primarily uses &lt;a href='http://www.r-project.org/'&gt;R&lt;/a&gt;. We&amp;#8217;re big fans of R at Snowplow, and a big motivation in building Snowplow was to enable the use of sophisticated data analysis tools like R on granular event-level data. We have a number of other recipes for R we hope to publish in the near future. This is the third recipe added to the &lt;a href='/analytics/catalog-analytics/overview.html'&gt;catalog analytics&lt;/a&gt; section of the &lt;a href='/analytics/index.html'&gt;Cookbook&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;As always, if there are specific types of analysis you&amp;#8217;d like us to cover, then &lt;a href='/about/index.html'&gt;get in touch&lt;/a&gt;, either directly or by dropping us a comment below.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/05/16/snowplow-0.8.4-released-with-maxmind-geoip</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/05/16/snowplow-0.8.4-released-with-maxmind-geoip"/>
    <title>Snowplow 0.8.4 released with MaxMind geo-IP lookups</title>
    <updated>2013-05-16T00:00:00+01:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We are pleased to announce the immediate availability of Snowplow &lt;strong&gt;0.8.4&lt;/strong&gt;. This is a big release, which adds geo-IP lookups to the Snowplow Enrichment stage, using the excellent &lt;a href='http://dev.maxmind.com/geoip/legacy/geolite'&gt;GeoLite City database&lt;/a&gt; from &lt;a href='http://www.maxmind.com'&gt;MaxMind, Inc&lt;/a&gt;. This has been one of the most requested features from the Snowplow community, so we are delighted to launch it. Now you can determine the location of your website visitors directly from the Snowplow events table, and plot that data on a wide range of mapping tools including &lt;a href='http://kb.tableausoftware.com/articles/knowledgebase/mapping-basics'&gt;Tableau&lt;/a&gt; or &lt;a href='http://wrobstory.github.io/2013/04/python-maps-chloropleth.html'&gt;Vincent&lt;/a&gt;:&lt;/p&gt;
&lt;a href='/static/img/blog/2013/05/pbz-global-visitors.jpg'&gt;&lt;img src='/static/img/blog/2013/05/pbz-global-visitors.jpg' /&gt;&lt;/a&gt;
&lt;p&gt;&lt;em&gt;Click on the image above to enlarge it&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Here is some example geo-IP data:&lt;/p&gt;
&lt;a href='/static/img/blog/2013/05/geoip-data.png'&gt;&lt;img src='/static/img/blog/2013/05/geoip-data.png' /&gt;&lt;/a&gt;
&lt;p&gt;&lt;em&gt;Click on the image above to enlarge it&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;As well as geo-IP enrichment, there are a number of other code improvements to the Hadoop ETL, plus some minor improvements to EmrEtlRunner and some corresponding updates to the Redshift table. In this post we will cover:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href='/blog/2013/05/16/snowplow-0.8.4-released-with-maxmind-geoip#geoip'&gt;The new geo-IP capabilities&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/05/16/snowplow-0.8.4-released-with-maxmind-geoip#other-changes'&gt;Other changes&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/05/16/snowplow-0.8.4-released-with-maxmind-geoip#upgrading'&gt;Upgrading&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/05/16/snowplow-0.8.4-released-with-maxmind-geoip#help'&gt;Getting help&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;!--more--&gt;&lt;h2&gt;&lt;a name='geoip'&gt;1. The new geo-IP capabilities&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;When we released Snowplow 0.8.0 back in April, we promised that the new Scalding-based ETL process would provide a solid bedrock on which we could build a bunch of data enrichments to perform on the raw Snowplow logs to make Snowplow data more interesting to analyse. With version 0.8.1, we included a referer parsing enrichment, which looked up external page referers against a database of search engines and social networks, and used associatd rules to infer additional data about what drove those visitors to your site. This release adds our second big enrichment: the ETL process now lookups every IP address against MaxMind&amp;#8217;s &lt;a href='http://dev.maxmind.com/geoip/legacy/geolite'&gt;GeoLite City database&lt;/a&gt; in order to determine the location of a visitors - specifically (wherever possible):&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;geo_country&lt;/code&gt; - the two-letter &lt;a href='http://en.wikipedia.org/wiki/ISO_3166-1_alpha-2'&gt;ISO 3166-1 alpha-2&lt;/a&gt; country code associated with the IP address&lt;/li&gt;

&lt;li&gt;&lt;code&gt;geo_region&lt;/code&gt; - the two letter &lt;a href='http://en.wikipedia.org/wiki/ISO_3166-2'&gt;ISO-3166-2&lt;/a&gt; or &lt;a href='http://en.wikipedia.org/wiki/FIPS_10-4'&gt;FIPS 10-4&lt;/a&gt; code for the state or region associated with the IP address&lt;/li&gt;

&lt;li&gt;&lt;code&gt;geo_city&lt;/code&gt; - the city or town name associated with the IP address&lt;/li&gt;

&lt;li&gt;&lt;code&gt;geo_zipcode&lt;/code&gt; - the zip or postal code associated with the IP address&lt;/li&gt;

&lt;li&gt;&lt;code&gt;geo_latitude&lt;/code&gt; - the latitude associated with the IP address&lt;/li&gt;

&lt;li&gt;&lt;code&gt;geo_longitude&lt;/code&gt; - the longitude associated with the IP address&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For more information on these six fields we recommend reading the &lt;a href='http://dev.maxmind.com/geoip/legacy/csv'&gt;GeoIP CSV Databases technical reference&lt;/a&gt; on the MaxMind website.&lt;/p&gt;

&lt;p&gt;Providing all this data directly in the Snowplow events table makes it easy to create geographic maps like the one below using Snowplow data directly: we will cover how to do this in a forthcoming blog post.&lt;/p&gt;
&lt;a href='/static/img/blog/2013/05/pbz-europe-visitors.jpg'&gt;&lt;img src='/static/img/blog/2013/05/pbz-europe-visitors.jpg' /&gt;&lt;/a&gt;&lt;h2&gt;&lt;a name='other-changes'&gt;2. Other changes&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;In this release we have also made some functional improvements to the Hadoop (Scalding) ETL, plus some minor improvements to EmrEtlRunner and some updates to the Redshift table. To take these in turn:&lt;/p&gt;

&lt;h3 id='hadoop_etl'&gt;Hadoop ETL&lt;/h3&gt;

&lt;p&gt;We have made various improvements to the Hadoop ETL:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;We have bumped the version of &lt;a href='https://github.com/snowplow/referer-parser'&gt;referer-parser&lt;/a&gt; - the latest version includes a fix to better attribute Google referer URLs&lt;/li&gt;

&lt;li&gt;We have added truncation of &lt;code&gt;refr_urlpath&lt;/code&gt;, &lt;code&gt;refr_urlquery&lt;/code&gt; and &lt;code&gt;urlfragment&lt;/code&gt;, to prevent Redshift load errors&lt;/li&gt;

&lt;li&gt;We now remove tabs and newlines from referer search terms (&lt;code&gt;refr_term&lt;/code&gt;), again to prevent Redshift load errors&lt;/li&gt;

&lt;li&gt;We have fixed a nasty bug where the client timestamp was being inaccurately localised to the Hadoop cluster&amp;#8217;s local time (&lt;a href='https://github.com/snowplow/snowplow/issues/238'&gt;issue #238&lt;/a&gt;) - thanks &lt;a href='https://github.com/rgabo'&gt;Gabor&lt;/a&gt; for spotting this&lt;/li&gt;

&lt;li&gt;We have made the code around page URL extraction more robust in the case that a page URL cannot be extracted&lt;/li&gt;

&lt;li&gt;If you are running the latest version of the Clojure Collector, then the specific version number will now be extracted into the &lt;code&gt;v_collector&lt;/code&gt; field&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id='emretlrunner'&gt;EmrEtlRunner&lt;/h3&gt;

&lt;p&gt;We have updated EmrEtlRunner to supply the location of the MaxMind GeoLite City database to the Scalding ETL.&lt;/p&gt;

&lt;p&gt;We have also improved the notification messages when the ETL job is started on Elastic MapReduce, and the notification message if the job should fail.&lt;/p&gt;

&lt;h3 id='redshift_events_table'&gt;Redshift events table&lt;/h3&gt;

&lt;p&gt;We have updated the Redshift events table to include new fields for the geo-IP location - see &lt;a href='#geoip'&gt;above&lt;/a&gt; for the six new field names.&lt;/p&gt;

&lt;p&gt;Also, we have renamed the five &lt;code&gt;ev_&lt;/code&gt; fields in the Redshift table definition to start with &lt;code&gt;se_&lt;/code&gt;, e.g. &lt;code&gt;se_action&lt;/code&gt;. This is to make these column names consistent with our structured events terminology.&lt;/p&gt;
&lt;h2&gt;&lt;a name='upgrading'&gt;3. Upgrading&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;There are &lt;strong&gt;three components&lt;/strong&gt; to upgrade in this release:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The Scalding ETL, to version 0.3.0&lt;/li&gt;

&lt;li&gt;EmrEtlRunner, to version 0.2.0&lt;/li&gt;

&lt;li&gt;The Redshift events table, to version 0.2.0&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Let&amp;#8217;s take these in turn:&lt;/p&gt;

&lt;h3 id='hadoop_etl'&gt;Hadoop ETL&lt;/h3&gt;

&lt;p&gt;If you are using EmrEtlRunner, you need to update your configuration file, &lt;code&gt;config.yml&lt;/code&gt;, to the latest version of the Hadoop ETL:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:snowplow:
  :hadoop_etl_version: 0.3.0 # Version of the Hadoop ETL&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id='emretlrunner'&gt;EmrEtlRunner&lt;/h3&gt;

&lt;p&gt;You need to upgrade your EmrEtlRunner installation to the latest code (0.8.4 release) on GitHub:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ git clone git://github.com/snowplow/snowplow.git
$ git checkout 0.8.4&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id='redshift_events_table'&gt;Redshift events table&lt;/h3&gt;

&lt;p&gt;We have updated the Redshift table definition - you can find the latest version in the GitHub repository &lt;a href='https://github.com/snowplow/snowplow/blob/master/4-storage/redshift-storage/sql/table-def.sql'&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If you already have your Snowplow data in the previous version of the Redshift events table, we have written &lt;a href='https://github.com/snowplow/snowplow/blob/master/4-storage/redshift-storage/sql/migrate_0.1.0_to_0.2.0.sql'&gt;a migration script&lt;/a&gt; to handle the upgrade. &lt;strong&gt;Please review this script carefully before running and check that you are happy with how it handles the upgrade.&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a name='help'&gt;4. Getting help&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;As always, if you do run into any issues or don&amp;#8217;t understand any of the above changes, please &lt;a href='https://github.com/snowplow/snowplow/issues'&gt;raise an issue&lt;/a&gt; or get in touch with us via &lt;a href='https://github.com/snowplow/snowplow/wiki/Talk-to-us'&gt;the usual channels&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;You can see the full list of issues delivered in Snowplow 0.8.4 on &lt;a href='https://github.com/snowplow/snowplow/issues?milestone=17&amp;amp;page=1&amp;amp;state=closed'&gt;GitHub&lt;/a&gt;.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/05/14/snowplow-unstructured-events-guide</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/05/14/snowplow-unstructured-events-guide"/>
    <title>A guide to unstructured events in Snowplow 0.8.3</title>
    <updated>2013-05-14T00:00:00+01:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;Earlier today we &lt;a href='/blog/2013/05/14/snowplow-0.8.3-released-with-unstructured-events'&gt;announced the release of Snowplow 0.8.3&lt;/a&gt;, which updated our JavaScript Tracker to add the ability to send custom unstructured events to a Snowplow collector with &lt;code&gt;trackUnstructEvent()&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;In our earlier blog post we briefly introduced the capabilities of &lt;code&gt;trackUnstructEvent&lt;/code&gt; with some example code. In this blog post, we will take a detailed look at Snowplow&amp;#8217;s custom unstructured events functionality, so you can understand how best to send unstructured events to Snowplow.&lt;/p&gt;

&lt;p&gt;Understanding the unstructured event format is important because our Enrichment process does not yet extract unstructured events, so you will not get any feedback yet from the ETL as to whether you are tracking them correctly. (Nor do we have validation for unstructured event properties in our JavaScript Tracker yet.)&lt;/p&gt;

&lt;p&gt;In the rest of this post, then, we will cover:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href='/blog/2013/05/14/snowplow-unstructured-events-guide#basic-usage'&gt;Basic usage&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/05/14/snowplow-unstructured-events-guide#properties-object'&gt;The &lt;code&gt;properties&lt;/code&gt; JavaScript object&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/05/14/snowplow-unstructured-events-guide#supported-datatypes'&gt;Supported datatypes&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/05/14/snowplow-unstructured-events-guide#help'&gt;Getting help&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;!--more--&gt;&lt;h2&gt;&lt;a name='basic-usage'&gt;1. Basic usage&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Tracking an unstructured event with the JavaScript Tracker is very straightforward - use the &lt;code&gt;trackUnstructEvent(name, properties)&lt;/code&gt; function:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;name&lt;/code&gt; is the name of the unstructured event. This is case-sensitive; spaces etc are allowed&lt;/li&gt;

&lt;li&gt;&lt;code&gt;properties&lt;/code&gt; is a JavaScript object&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here is an example:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='javascript'&gt;&lt;span class='nx'&gt;_snaq&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='nx'&gt;push&lt;/span&gt;&lt;span class='p'&gt;([&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;trackUnstructEvent&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;Viewed Product&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
                &lt;span class='p'&gt;{&lt;/span&gt;
                    &lt;span class='nx'&gt;product_id&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;ASO01043&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
                    &lt;span class='nx'&gt;category&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;Dresses&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
                    &lt;span class='nx'&gt;brand&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;ACME&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
                    &lt;span class='nx'&gt;returning&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='kc'&gt;true&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
                    &lt;span class='nx'&gt;price&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='mf'&gt;49.95&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
                    &lt;span class='nx'&gt;sizes&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='p'&gt;[&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;xs&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;s&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;l&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;xl&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;xxl&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;],&lt;/span&gt;
                    &lt;span class='nx'&gt;available_since$dt&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='k'&gt;new&lt;/span&gt; &lt;span class='nb'&gt;Date&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='mi'&gt;2013&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;&lt;span class='mi'&gt;3&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;&lt;span class='mi'&gt;7&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt;
                &lt;span class='p'&gt;}&lt;/span&gt;
            &lt;span class='p'&gt;]);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Every call to &lt;code&gt;trackUnstructEvent&lt;/code&gt; has the same structure - the complexity comes from knowing how to structure the &lt;code&gt;properties&lt;/code&gt; JavaScript object. We will discuss this next:&lt;/p&gt;
&lt;h2&gt;&lt;a name='properties-object'&gt;2. The 'properties' JavaScript object&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;properties&lt;/code&gt; JavaScript consists of a set of individual &lt;code&gt;name: value&lt;/code&gt; properties.&lt;/p&gt;

&lt;p&gt;The structure must be flat - in other words, properties cannot be nested. Continuing with the exampe code above, this means that the following is &lt;strong&gt;not&lt;/strong&gt; allowed:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='javascript'&gt;&lt;span class='p'&gt;{&lt;/span&gt;
    &lt;span class='nx'&gt;category&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='p'&gt;{&lt;/span&gt; &lt;span class='nx'&gt;primary&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;Womenswear&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='nx'&gt;secondary&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;Dresses&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;},&lt;/span&gt; &lt;span class='c1'&gt;// NOT allowed&lt;/span&gt;
&lt;span class='p'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The &lt;code&gt;properties&lt;/code&gt; JavaScript object supports a wide range of datatypes - see below for details.&lt;/p&gt;
&lt;h2&gt;&lt;a name='supported-datatypes'&gt;3. Supported datatypes&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Snowplow unstructured events support a relatively rich set of datatypes. Because these datatypes do not always map directly onto JavaScript datatypes, we have introduced some &amp;#8220;type suffixes&amp;#8221; for the JavaScript property names, to tell Snowplow what Snowplow datatype we want the JavaScript data to map onto.&lt;/p&gt;

&lt;p&gt;Our datatypes, then, are as follows:&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Snowplow datatype&lt;/th&gt;&lt;th&gt;Description&lt;/th&gt;&lt;th&gt;JavaScript datatype&lt;/th&gt;&lt;th&gt;Type suffix(es)&lt;/th&gt;&lt;th&gt;Supports array?&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Null&lt;/td&gt;&lt;td style='text-align: left;'&gt;Absence of a value&lt;/td&gt;&lt;td style='text-align: left;'&gt;Null&lt;/td&gt;&lt;td style='text-align: left;'&gt;-&lt;/td&gt;&lt;td style='text-align: left;'&gt;No&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;String&lt;/td&gt;&lt;td style='text-align: left;'&gt;String of characters&lt;/td&gt;&lt;td style='text-align: left;'&gt;String&lt;/td&gt;&lt;td style='text-align: left;'&gt;-&lt;/td&gt;&lt;td style='text-align: left;'&gt;Yes&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Boolean&lt;/td&gt;&lt;td style='text-align: left;'&gt;True or false&lt;/td&gt;&lt;td style='text-align: left;'&gt;Boolean&lt;/td&gt;&lt;td style='text-align: left;'&gt;-&lt;/td&gt;&lt;td style='text-align: left;'&gt;Yes&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Integer&lt;/td&gt;&lt;td style='text-align: left;'&gt;Number without decimal&lt;/td&gt;&lt;td style='text-align: left;'&gt;Number&lt;/td&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;$int&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;Yes&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Floating point&lt;/td&gt;&lt;td style='text-align: left;'&gt;Number with decimal&lt;/td&gt;&lt;td style='text-align: left;'&gt;Number&lt;/td&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;$flt&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;Yes&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Geo-coordinates&lt;/td&gt;&lt;td style='text-align: left;'&gt;Longitude and latitude&lt;/td&gt;&lt;td style='text-align: left;'&gt;[Number, Number]&lt;/td&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;$geo&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;Yes&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Date&lt;/td&gt;&lt;td style='text-align: left;'&gt;Date and time (ms precision)&lt;/td&gt;&lt;td style='text-align: left;'&gt;Number&lt;/td&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;$dt&lt;/code&gt;, &lt;code&gt;$tm&lt;/code&gt;, &lt;code&gt;$tms&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;Yes&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Array&lt;/td&gt;&lt;td style='text-align: left;'&gt;Array of values&lt;/td&gt;&lt;td style='text-align: left;'&gt;[x, y, z]&lt;/td&gt;&lt;td style='text-align: left;'&gt;-&lt;/td&gt;&lt;td style='text-align: left;'&gt;-&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;Let&amp;#8217;s go through each of these in turn, providing some examples as we go:&lt;/p&gt;

&lt;h3 id='null'&gt;Null&lt;/h3&gt;

&lt;p&gt;Tracking a Null value for a given field is straightforward:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='javascript'&gt;&lt;span class='p'&gt;{&lt;/span&gt;
    &lt;span class='nx'&gt;returns_id&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='kc'&gt;null&lt;/span&gt;
&lt;span class='p'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id='string'&gt;String&lt;/h3&gt;

&lt;p&gt;Tracking a String is easy:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='javascript'&gt;&lt;span class='p'&gt;{&lt;/span&gt;
    &lt;span class='nx'&gt;product_id&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;ASO01043&amp;#39;&lt;/span&gt; &lt;span class='c1'&gt;// Or &amp;quot;ASO01043&amp;quot;&lt;/span&gt;
&lt;span class='p'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id='boolean'&gt;Boolean&lt;/h3&gt;

&lt;p&gt;Tracking a Boolean is also straightforward:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='javascript'&gt;&lt;span class='p'&gt;{&lt;/span&gt;
    &lt;span class='nx'&gt;trial&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='kc'&gt;true&lt;/span&gt;
&lt;span class='p'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id='integer'&gt;Integer&lt;/h3&gt;

&lt;p&gt;To track an Integer, use a JavaScript Number but add a type suffix like so:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='javascript'&gt;&lt;span class='p'&gt;{&lt;/span&gt;
    &lt;span class='nx'&gt;in_stock$int&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='mi'&gt;23&lt;/span&gt;
&lt;span class='p'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Warning:&lt;/strong&gt; if you do not add the &lt;code&gt;$int&lt;/code&gt; type suffix, Snowplow will assume you are tracking a Floating point number.&lt;/p&gt;

&lt;h3 id='floating_point'&gt;Floating point&lt;/h3&gt;

&lt;p&gt;To track a Floating point number, use a JavaScript Number; adding a type suffix is optional:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='javascript'&gt;&lt;span class='p'&gt;{&lt;/span&gt;
    &lt;span class='nx'&gt;price$flt&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='mf'&gt;4.99&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; 
    &lt;span class='nx'&gt;sales_tax&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='mf'&gt;49.99&lt;/span&gt; &lt;span class='c1'&gt;// Same as $sales_tax:$flt&lt;/span&gt;
&lt;span class='p'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id='geocoordinates'&gt;Geo-coordinates&lt;/h3&gt;

&lt;p&gt;Tracking a pair of Geographic coordinates is done like so:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='javascript'&gt;&lt;span class='p'&gt;{&lt;/span&gt;
    &lt;span class='nx'&gt;check_in$geo&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='p'&gt;[&lt;/span&gt;&lt;span class='mf'&gt;40.11041&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='o'&gt;-&lt;/span&gt;&lt;span class='mf'&gt;88.21337&lt;/span&gt;&lt;span class='p'&gt;]&lt;/span&gt; &lt;span class='c1'&gt;// Lat, long&lt;/span&gt;
&lt;span class='p'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Please note that the datatype takes the format &lt;strong&gt;latitude&lt;/strong&gt; followed by &lt;strong&gt;longitude&lt;/strong&gt;. That is the same order used by services such as Google Maps.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Warning:&lt;/strong&gt; if you do not add the &lt;code&gt;$geo&lt;/code&gt; type suffix, then the value will be incorrectly interpreted by Snowplow as an Array of Floating points.&lt;/p&gt;

&lt;h3 id='date'&gt;Date&lt;/h3&gt;

&lt;p&gt;Snowplow Dates include the date &lt;em&gt;and&lt;/em&gt; the time, with milliseconds precision. There are three type suffixes supported for tracking a Date:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;$dt&lt;/code&gt; - the Number of days since the epoch&lt;/li&gt;

&lt;li&gt;&lt;code&gt;$tm&lt;/code&gt; - the Number of seconds since the epoch&lt;/li&gt;

&lt;li&gt;&lt;code&gt;$tms&lt;/code&gt; - the Number of milliseconds since the epoch. This is the default for JavaScript Dates if no type suffix supplied&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You can track a date by adding either a JavaScript Number &lt;em&gt;or&lt;/em&gt; JavaScript Date to your &lt;code&gt;properties&lt;/code&gt; object. The following are all valid dates:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='javascript'&gt;&lt;span class='p'&gt;{&lt;/span&gt;
    &lt;span class='nx'&gt;birthday$dt&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='k'&gt;new&lt;/span&gt; &lt;span class='nb'&gt;Date&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='mi'&gt;1980&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;&lt;span class='mi'&gt;11&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;&lt;span class='mi'&gt;10&lt;/span&gt;&lt;span class='p'&gt;),&lt;/span&gt; &lt;span class='c1'&gt;// Sent to Snowplow as birthday$dt: 3996&lt;/span&gt;
    &lt;span class='nx'&gt;birthday2$dt&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='mi'&gt;3996&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='c1'&gt;// ^ Same as above&lt;/span&gt;
    
    &lt;span class='nx'&gt;registered$tm&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='k'&gt;new&lt;/span&gt; &lt;span class='nb'&gt;Date&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='mi'&gt;2013&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;&lt;span class='mi'&gt;05&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;&lt;span class='mi'&gt;13&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;&lt;span class='mi'&gt;14&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;&lt;span class='mi'&gt;20&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;&lt;span class='mi'&gt;10&lt;/span&gt;&lt;span class='p'&gt;),&lt;/span&gt; &lt;span class='c1'&gt;// Sent to Snowplow as registered$tm: 1371129610&lt;/span&gt;
    &lt;span class='nx'&gt;registered2$tm&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='mi'&gt;1371129610&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='c1'&gt;// Same as above&lt;/span&gt;
    
    &lt;span class='nx'&gt;last_action$tms&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='mi'&gt;1368454114215&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='c1'&gt;// Accurate to milliseconds&lt;/span&gt;
    &lt;span class='nx'&gt;last_action2&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='k'&gt;new&lt;/span&gt; &lt;span class='nb'&gt;Date&lt;/span&gt;&lt;span class='p'&gt;()&lt;/span&gt; &lt;span class='c1'&gt;// Sent to Snowplow as last_action2$tms: 1368454114215&lt;/span&gt;
&lt;span class='p'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Note that the type prefix only indicates how the JavaScript Number sent to Snowplow is interpreted - all Snowplow Dates are stored to milliseconds precision (whether or not they include that level of precision).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Two warnings:&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;If you specify a JavaScript Number but do not add a valid Date suffix (&lt;code&gt;$dt&lt;/code&gt;, &lt;code&gt;$tm&lt;/code&gt; or &lt;code&gt;$tms&lt;/code&gt;), then the value will be incorrectly interpreted by Snowplow as a Number, not a Date&lt;/li&gt;

&lt;li&gt;If you specify a JavaScript Number but add the wrong Date suffix, then the Date will be incorrectly interpreted by Snowplow, for example:&lt;/li&gt;
&lt;/ol&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='javascript'&gt;&lt;span class='p'&gt;{&lt;/span&gt;
    &lt;span class='nx'&gt;last_ping$dt&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='mi'&gt;1371129610&lt;/span&gt; &lt;span class='c1'&gt;// Should have been $tm. Snowplow will interpret this as the year 3756521449&lt;/span&gt;
&lt;span class='p'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id='arrays'&gt;Arrays&lt;/h3&gt;

&lt;p&gt;You can track an Array of values of any data type other than Null.&lt;/p&gt;

&lt;p&gt;Arrays must be homogeneous - in other words, all values within the Array must be of the same datatype. This means that the following is &lt;strong&gt;not&lt;/strong&gt; allowed:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='javascript'&gt;&lt;span class='p'&gt;{&lt;/span&gt;
    &lt;span class='nx'&gt;sizes&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='p'&gt;[&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;xs&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='mi'&gt;28&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;l&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='mi'&gt;38&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;xxl&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;]&lt;/span&gt; &lt;span class='c1'&gt;// NOT allowed&lt;/span&gt;
&lt;span class='p'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;By contrast, the following are all allowed:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='javascript'&gt;&lt;span class='p'&gt;{&lt;/span&gt;
    &lt;span class='nx'&gt;sizes&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='p'&gt;[&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;xs&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;s&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;l&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;xl&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;xxl&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;],&lt;/span&gt;
    &lt;span class='nx'&gt;session_starts$tm&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='p'&gt;[&lt;/span&gt;&lt;span class='mi'&gt;1371129610&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='mi'&gt;1064329730&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='mi'&gt;1341127611&lt;/span&gt;&lt;span class='p'&gt;],&lt;/span&gt;
    &lt;span class='nx'&gt;check_ins$geo&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='p'&gt;[[&lt;/span&gt;&lt;span class='o'&gt;-&lt;/span&gt;&lt;span class='mf'&gt;88.21337&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='mf'&gt;40.11041&lt;/span&gt;&lt;span class='p'&gt;],&lt;/span&gt; &lt;span class='p'&gt;[&lt;/span&gt;&lt;span class='o'&gt;-&lt;/span&gt;&lt;span class='mf'&gt;78.81557&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='mf'&gt;30.22047&lt;/span&gt;&lt;span class='p'&gt;]]&lt;/span&gt;
&lt;span class='p'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2&gt;&lt;a name='help'&gt;4. Getting help&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;As always, if you do run into any issues or don&amp;#8217;t understand any of the above guide, please do get in touch with us via &lt;a href='https://github.com/snowplow/snowplow/wiki/Talk-to-us'&gt;the usual channels&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;And if you have any ideas or feedback for Snowplow&amp;#8217; custom unstructured events, do please share them, either in the comments below or through the usual channels.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/05/14/snowplow-0.8.3-released-with-unstructured-events</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/05/14/snowplow-0.8.3-released-with-unstructured-events"/>
    <title>Snowplow 0.8.3 released with unstructured events</title>
    <updated>2013-05-14T00:00:00+01:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We&amp;#8217;re pleased to announce the release of Snowplow &lt;strong&gt;0.8.3&lt;/strong&gt;. This release updates our JavaScript Tracker to version 0.11.2, adding the ability to send custom unstructured events to a Snowplow collector with &lt;code&gt;trackUnstructEvent()&lt;/code&gt;. The Clojure Collector is also bumped to 0.5.0, to include some important bug fixes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Please note that this release only adds unstructured events to the JavaScript Tracker - adding unstructured events to our Enrichment process and storage targets is on the roadmap - but rest assured we are working on it!&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Many thanks to community members &lt;a href='https://github.com/rgabo'&gt;Gabor Ratky&lt;/a&gt;, &lt;a href='https://github.com/tarsolya'&gt;Andras Tarsoly&lt;/a&gt; and &lt;a href='https://github.com/lackac'&gt;Laszlo Bacsi&lt;/a&gt;, all from &lt;a href='http://secretsaucepartners.com/'&gt;Secret Sauce Partners&lt;/a&gt;, for contributing this great feature: Gabor and his team took JavaScript unstructured events from an item on our roadmap to a code-complete feature, big thanks guys! (And if you are interested in seeing how the design and implementation of this powerful feature evolved, do have a read of the &lt;a href='https://github.com/snowplow/snowplow/pull/198'&gt;original GitHub pull request&lt;/a&gt;.)&lt;/p&gt;

&lt;p&gt;In the rest of this post, then, we will cover:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href='/blog/2013/05/14/snowplow-0.8.3-released-with-unstructured-events/#definition'&gt;What are unstructured events?&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/05/14/snowplow-0.8.3-released-with-unstructured-events/#when'&gt;When to use unstructured events?&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/05/14/snowplow-0.8.3-released-with-unstructured-events/#usage'&gt;Usage&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/05/14/snowplow-0.8.3-released-with-unstructured-events/#upgrading'&gt;Upgrading&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/05/14/snowplow-0.8.3-released-with-unstructured-events/#roadmap'&gt;Roadmap for unstructured events&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/05/14/snowplow-0.8.3-released-with-unstructured-events/#help'&gt;Getting help&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;!--more--&gt;&lt;h2&gt;&lt;a name='definition'&gt;1. What are unstructured events?&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a href='https://github.com/snowplow/snowplow/wiki/canonical-event-model#wiki-customunstruct'&gt;Custom unstructured events&lt;/a&gt; are user events which do not fit one of the existing Snowplow event types (page views, ecommerce transactions etc), and do not fit easily into our existing &lt;a href='https://github.com/snowplow/snowplow/wiki/canonical-event-model#wiki-customstruct'&gt;custom structured event&lt;/a&gt; format. A custom unstructured event consists of two elements:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A &lt;code&gt;name&lt;/code&gt;, e.g. &amp;#8220;Game saved&amp;#8221; or &amp;#8220;returned-order&amp;#8221;&lt;/li&gt;

&lt;li&gt;A set of &lt;code&gt;name: value&lt;/code&gt; properties (also known as a hash, associative array or dictionary)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You might recognise what we call custom unstructured events from other analytics tools including MixPanel, KISSmetrics and Keen.io, where they are the primary trackable event type.&lt;/p&gt;
&lt;h2&gt;&lt;a name='when'&gt;2. When to use unstructured events?&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Custom unstructured events are great for a couple of use cases:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Where you want to track event types which are proprietary/specific to your business (i.e. not already part of Snowplow)&lt;/li&gt;

&lt;li&gt;Where you want to track events which have unpredictable or frequently changing properties&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; because unstructured events are &lt;em&gt;not&lt;/em&gt; currently processed by the ETL and enrichment step, or added to storage, we recommend using &lt;a href='https://github.com/snowplow/snowplow/wiki/canonical-event-model#wiki-customstruct'&gt;custom structured events&lt;/a&gt; for custom events types, assuming that you can fit your events into our custom structured event schema.&lt;/p&gt;
&lt;h2&gt;&lt;a name='usage'&gt;3. Usage&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Tracking an unstructured event with the JavaScript Tracker is very straightforward - use the &lt;code&gt;trackUnstructEvent(name, properties)&lt;/code&gt; function.&lt;/p&gt;

&lt;p&gt;Here is an example taken from our codebase:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='javascript'&gt;&lt;span class='nx'&gt;_snaq&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='nx'&gt;push&lt;/span&gt;&lt;span class='p'&gt;([&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;trackUnstructEvent&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;Viewed Product&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
                &lt;span class='p'&gt;{&lt;/span&gt;
                    &lt;span class='nx'&gt;product_id&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;ASO01043&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
                    &lt;span class='nx'&gt;category&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;Dresses&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
                    &lt;span class='nx'&gt;brand&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;ACME&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
                    &lt;span class='nx'&gt;returning&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='kc'&gt;true&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
                    &lt;span class='nx'&gt;price&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='mf'&gt;49.95&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
                    &lt;span class='nx'&gt;sizes&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='p'&gt;[&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;xs&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;s&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;l&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;xl&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;xxl&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;],&lt;/span&gt;
                    &lt;span class='nx'&gt;available_since$dt&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='k'&gt;new&lt;/span&gt; &lt;span class='nb'&gt;Date&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='mi'&gt;2013&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;&lt;span class='mi'&gt;3&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;&lt;span class='mi'&gt;7&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt;
                &lt;span class='p'&gt;}&lt;/span&gt;
            &lt;span class='p'&gt;]);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We have written a &lt;a href='/blog/2013/05/14/snowplow-unstructured-events-guide'&gt;follow-up blog post&lt;/a&gt; to provide more information on using the new &lt;code&gt;trackUnstructEvent&lt;/code&gt; functionality - please &lt;a href='/blog/2013/05/14/snowplow-unstructured-events-guide'&gt;read this post&lt;/a&gt; for more information.&lt;/p&gt;
&lt;h2&gt;&lt;a name='upgrading'&gt;4. Upgrading&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;There are two components to upgrade in this release:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The JavaScript Tracker, to version 0.11.2&lt;/li&gt;

&lt;li&gt;The Clojure Collector, to version 0.5.0&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you are running the Clojure Collector, you must upgrade the Clojure Collector &lt;strong&gt;before&lt;/strong&gt; upgrading the JavaScript Tracker, or you will experience some data loss.&lt;/p&gt;

&lt;h3 id='clojure_collector'&gt;Clojure Collector&lt;/h3&gt;

&lt;p&gt;This release bumps the Clojure Collector to version &lt;strong&gt;0.5.0&lt;/strong&gt;. To upgrade to this release:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Download the new warfile by right-clicking on &lt;a href='http://s3-eu-west-1.amazonaws.com/snowplow-hosted-assets/2-collectors/clojure-collector/clojure-collector-0.5.0-standalone.war'&gt;this link&lt;/a&gt; and selecting &amp;#8220;Save As&amp;#8230;&amp;#8221;&lt;/li&gt;

&lt;li&gt;Log in to your Amazon Elastic Beanstalk console&lt;/li&gt;

&lt;li&gt;Browse to your Collector&amp;#8217;s application&lt;/li&gt;

&lt;li&gt;Click the &amp;#8220;Upload New Version&amp;#8221; and upload your warfile&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id='javascript_tracker'&gt;JavaScript Tracker&lt;/h3&gt;

&lt;p&gt;Please update your website(s) or tag manager to use the latest version of the JavaScript Tracker, which is version 0.11.2. As always, the updated minified tracker is available here:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http(s)://d1fc8wv8zag5ca.cloudfront.net/0.11.2/sp.js&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a name='roadmap'&gt;5. Roadmap&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;We are well aware that this release is only the start of adding custom unstructured events to Snowplow.&lt;/p&gt;

&lt;p&gt;It makes sense to work next on extracting unstructured events in our Enrichment process; unfortunately this is not trivial, because our Enrichment process currently only outputs to Redshift, and Redshift has no support for JSON objects or maps of properties, which we would need to store the unstructured event properties.&lt;/p&gt;

&lt;p&gt;Therefore we are exploring two different strands:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Storing Snowplow events in Avro&lt;/strong&gt;. Avro is a rich data serialization system that will allow us to store the unstructured event properties within the event object. Initially, you would be able to query these Avro-serialized events using a range of tools on Hadoop including Pig, Hive, Scalding and Cascalog. It should also be relatively straightforward to load these events into NoSQL databases such as MongoDB. We would then work on mapping the Avro events into Redshift&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Storing Snowplow events in PostgreSQL&lt;/strong&gt;. Postgres has a JSON datatype, although the querying capabilities on that JSON datatype are so-far very primitive. Nonetheless, it should be possible to at least store the unstructured event properties in an appropriate JSON field in Postgres&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If you have a preference for one of the two above options, or a suggested third approach, then &lt;a href='https://github.com/snowplow/snowplow/wiki/Talk-to-us'&gt;get in touch&lt;/a&gt; and let us know as soon as possible, as we are thining through these alternatives now.&lt;/p&gt;

&lt;p&gt;Please keep an eye on our &lt;a href='https://github.com/snowplow/snowplow/wiki/Product-roadmap'&gt;Roadmap wiki page&lt;/a&gt; to see how Snowplow&amp;#8217;s support for unstructured events evolves.&lt;/p&gt;
&lt;h2&gt;&lt;a name='help'&gt;6. Getting help&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;As always, if you do run into any issues or don&amp;#8217;t understand any of the above changes, please &lt;a href='https://github.com/snowplow/snowplow/issues'&gt;raise an issue&lt;/a&gt; or get in touch with us via &lt;a href='https://github.com/snowplow/snowplow/wiki/Talk-to-us'&gt;the usual channels&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;And if you want to find out more about the syntax for &lt;code&gt;trackUnstructEvent&lt;/code&gt;, do checkout our &lt;a href='/blog/2013/05/14/snowplow-unstructured-events-guide'&gt;Snowplow Unstructured Events Guide&lt;/a&gt;, which was also published today.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/05/10/where-does-your-traffic-really-come-from</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/05/10/where-does-your-traffic-really-come-from"/>
    <title>Where does your traffic *really* come from?</title>
    <updated>2013-05-10T00:00:00+01:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;img src='/static/img/blog/2013/05/lone-traveller.jpg' /&gt;
&lt;p&gt;Web analysts spend a lot of time exploring where visitors to their websites come from:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Which sites and marketing campaigns are driving visitors to your website?&lt;/li&gt;

&lt;li&gt;How valuable are those visitors?&lt;/li&gt;

&lt;li&gt;What should you be doing to drive up the number of high quality users? (In terms of spending more marketing, engaging with other websites / blogs / social networks etc.)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Unfortunately, identifying where your visitors come from is &lt;strong&gt;not&lt;/strong&gt; as straightforward as it often seems. In this post, we will cover:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href='/blog/2013/05/10/where-does-your-traffic-really-come-from/#how'&gt;How, technically, can we determine where visitors have come from?&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/05/10/where-does-your-traffic-really-come-from#errors'&gt;Potential sources of errors&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/05/10/where-does-your-traffic-really-come-from#ga'&gt;Problems with relying on the Google Analytics approach, and why the Snowplow approach is superior&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/05/10/where-does-your-traffic-really-come-from#adwords'&gt;Surprises when examining visitors acquired from AdWords search campaigns: most visitors clicked on an ad that was not shown on a Google domain&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/05/10/where-does-your-traffic-really-come-from/#conclusion'&gt;Pulling all the findings together: the value of high-fidelity data in determining where your visitors come from&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;!--more--&gt;&lt;a name='how'&gt;&lt;h2&gt;1. How, technically, can we determine where visitors have come from?&lt;/h2&gt;&lt;/a&gt;
&lt;p&gt;There are two sources of raw data that we can use to determine where a vistor to a website has come from: the &lt;a href='#page-referer'&gt;page referer&lt;/a&gt; and the &lt;a href='#page-url'&gt;page URL&lt;/a&gt;.&lt;/p&gt;
&lt;a name='page-referer'&gt;&lt;h3&gt;Page referer&lt;/h3&gt;&lt;/a&gt;
&lt;p&gt;When you load a web page in your browser, your browser makes an HTTP request to a web server to deliver that page. That request includes a header field that identifies the address of the web page that linked to the resource being requested: this is called the &lt;a href='http://en.wikipedia.org/wiki/HTTP_referer'&gt;HTTP referer&lt;/a&gt; (sic). It is also possible to access the current page&amp;#8217;s referer information from the browser itself, using &lt;code&gt;document.referrer&lt;/code&gt; in JavaScript.&lt;/p&gt;

&lt;p&gt;Web analytics programs typically read the HTTP referer header or JavaScript&amp;#8217;s &lt;code&gt;document.referrer&lt;/code&gt;, and use that page referer data as one the inputs to infer where a visitor has come from.&lt;/p&gt;
&lt;a name='page-url'&gt;&lt;h3&gt;Page URL&lt;/h3&gt;&lt;/a&gt;
&lt;p&gt;Page referers are a technical solution to identifying where traffic comes from. In addition, digital marketers may want to label incoming traffic so that they can identify which marketing campaigns that traffic should be attributed to. This is typically done by adding a querystring to the landing page URL.&lt;/p&gt;

&lt;p&gt;To give an example of how this technique works in practice, let&amp;#8217;s imagine that I am marketing the website &lt;code&gt;www.flowersdirect.com&lt;/code&gt;. I run a campaign on AdWords called &amp;#8220;November promotion&amp;#8221;. In my AdWords ad, I include a link (that I hope viewers of the add will click) to my homepage (&lt;code&gt;www.flowersdirect.com&lt;/code&gt;). However, instead of just including the standard link in my ad, i.e.&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='html'&gt;&lt;span class='nt'&gt;&amp;lt;a&lt;/span&gt; &lt;span class='na'&gt;href=&lt;/span&gt;&lt;span class='s'&gt;&amp;quot;http://www.flowersdirect.com&amp;quot;&lt;/span&gt;&lt;span class='nt'&gt;&amp;gt;&lt;/span&gt;www.flowersdirect.com&lt;span class='nt'&gt;&amp;lt;/a&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;I add a query parameter onto the end of my link labelling the campaign:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='html'&gt;&lt;span class='nt'&gt;&amp;lt;a&lt;/span&gt; &lt;span class='na'&gt;href=&lt;/span&gt;&lt;span class='s'&gt;&amp;quot;http://www.flowersdirect.com?utm_campaign=November_promotion&amp;quot;&lt;/span&gt;&lt;span class='nt'&gt;&amp;gt;&lt;/span&gt;www.flowersdirect.com&lt;span class='nt'&gt;&amp;lt;/a&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Adding the query parameter does not change the experience of the user clicking on the ad. Then, on the landing page (in this case, the &lt;code&gt;www.flowersdirect.com&lt;/code&gt; homepage) the web analytics JavaScript tag will pass the querystring to the web analytics program, which can then infer that the traffic should be attributed to the &amp;#8220;November promotion&amp;#8221;.&lt;/p&gt;

&lt;p&gt;Different web analytics programs look for different query parameters when assigning traffic to different marketing campaigns. In the case of &lt;a href='https://support.google.com/analytics/answer/1033863?hl=en-GB'&gt;Google Analytics&lt;/a&gt;, the following parameters are used to set the following fields:&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;&lt;strong&gt;Parameter&lt;/strong&gt;&lt;/th&gt;&lt;th&gt;&lt;strong&gt;Field in GA&lt;/strong&gt;&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;utm_medium&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;Medium&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;utm_source&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;Source&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;utm_term&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;Term&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;utm_content&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;Content&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;utm_campaign&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;Campaign&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;To keep things simple, Snowplow uses the same query parameters, so that businesses running Snowplow alongside GA only need to set the parameters once for each campaign in order to successfully track them in both GA and Snowplow.&lt;/p&gt;

&lt;p&gt;Web analytics programs use a combination of the page URL and the page referer to infer where traffic to the website has come from.&lt;/p&gt;
&lt;a name='errors'&gt;&lt;h2&gt;2. Potential sources of errors&lt;/h2&gt;&lt;/a&gt;
&lt;p&gt;In general, there is much more scope for errors to arise deducing the source of traffic from the querystring on the page URL than there are when using the HTTP referer field. This is because querystring parameters are set manually by humans, rather than programmatically by machines. The following are two of the most common sources of errors:&lt;/p&gt;

&lt;h3 id='a_visitors_share_page_urls_with_campaign_parameters_in_the_querystring_using_copyandpaste'&gt;(a) Visitors share page URLs with campaign parameters in the querystring, using copy-and-paste&lt;/h3&gt;

&lt;p&gt;Many times, you will see a link in e.g. a Twitter post containing a &lt;code&gt;utm_&lt;/code&gt; parameter that suggests it is a CPC campaign or some other non-social channel. This sort of error arises when, for example, a visitor clicks on a link from a CPC campaign, views the web page, then wants to share the web page - and does so by copying and pasting the URL. Website visitors are mostly oblivious to marketing parameters in the page&amp;#8217;s URL, and will leave them in place. Everywhere that the user pastes that link, that link will contain the query parameter; any other users clicking on that link will be misclassified as coming from a CPC campaign.&lt;/p&gt;

&lt;h3 id='b_typos_in_the_campaign_parameters_on_the_querystring'&gt;(b) Typos in the campaign parameters on the querystring&lt;/h3&gt;

&lt;p&gt;The individual who sets up the campaign may make a mistake adding the querystring to the link in the ad. This is very easy to do: setting up campaigns can be tedious (especially if many are set up at the same time) and error-prone. An error as simple as typing &lt;code&gt;utm-campaign&lt;/code&gt; instead of &lt;code&gt;utm_campaign&lt;/code&gt; is enough that most web analytics software will misclassify &lt;em&gt;all&lt;/em&gt; the visitors who clicked on that link.&lt;/p&gt;

&lt;p&gt;Note that using a traditional web analytics solution, it is:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Impossible to spot that an error has been made on the query string in most cases&lt;/li&gt;

&lt;li&gt;Even if you can spot the error, it is impossible to reprocess the data (and correct the error) or even &amp;#8216;ignore&amp;#8217; the erroneous data&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id='identifying_errors_with_snowplow'&gt;Identifying errors with Snowplow&lt;/h3&gt;

&lt;p&gt;Using Snowplow, however, spotting errors is easy, because you have access to the raw page URL parameters in your Snowplow events table. The following is some example data from &lt;a href='http://www.psychicbazaar.com/'&gt;Psychic Bazaar&lt;/a&gt;, an online retailer running Snowplow. We have executed the following query to identify page views where the referer is not internal (so that we only look at the first page view in each visit, i.e. the one with all the interesting page referer and page URL data to determine the source of traffic):&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='psql'&gt;&lt;span class='cm'&gt;/* PostgreSQL / Redshift */&lt;/span&gt;
&lt;span class='k'&gt;SELECT&lt;/span&gt;
	&lt;span class='n'&gt;collector_tstamp&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='n'&gt;page_urlhost&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='n'&gt;page_urlpath&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='n'&gt;page_urlquery&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='n'&gt;mkt_medium&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='n'&gt;mkt_source&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='n'&gt;mkt_term&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='n'&gt;mkt_campaign&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='n'&gt;refr_urlhost&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='n'&gt;refr_urlpath&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='n'&gt;refr_urlquery&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='n'&gt;refr_medium&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='n'&gt;refr_source&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='n'&gt;refr_term&lt;/span&gt;
&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='s-Name'&gt;&amp;quot;public&amp;quot;&lt;/span&gt;&lt;span class='mf'&gt;.&lt;/span&gt;&lt;span class='s-Name'&gt;&amp;quot;events_new&amp;quot;&lt;/span&gt;
&lt;span class='k'&gt;WHERE&lt;/span&gt; &lt;span class='s-Name'&gt;&amp;quot;refr_medium&amp;quot;&lt;/span&gt; &lt;span class='o'&gt;!=&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;internal&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Note how for each line of data, you can see the raw page URL data (&lt;code&gt;page_urlquery&lt;/code&gt; in particular) that is used to derive the marketing source, medium and campaign (&lt;code&gt;mkt_source&lt;/code&gt;, &lt;code&gt;mkt_medium&lt;/code&gt; and &lt;code&gt;mkt_campaign&lt;/code&gt; fields):&lt;/p&gt;
&lt;a href='/static/img/blog/2013/05/inferring-mkt-source-medium-term-campaign-from-pageurl.png'&gt;&lt;img src='/static/img/blog/2013/05/inferring-mkt-source-medium-term-campaign-from-pageurl.png' /&gt;&lt;/a&gt;
&lt;p&gt;&lt;em&gt;Click on the above image above to zoom in.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;This makes errors very easy to spot. As you can see from the screenshot below, Psychic Bazaar ran a campaign from September through November last year where there was a simple typo in the querystring. As a result, Snowplow did not set the marketing fields correctly for visitors who clicked on these links. Nonetheless, it is straightforward to spot the mistake, and adjust the data accordingly:&lt;/p&gt;
&lt;a href='/static/img/blog/2013/05/erroneous-query-string-data.png'&gt;&lt;img src='/static/img/blog/2013/05/erroneous-query-string-data.png' /&gt;&lt;/a&gt;
&lt;p&gt;&lt;em&gt;Click on the above image above to zoom in.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In Google Analytics, SiteCatalyst or most other tools, spotting the above error and handling it is correctly is impossible.&lt;/p&gt;
&lt;a name='ga'&gt;&lt;h2&gt;3. Problems with relying on Google Analytics approach, and why the Snowplow approach is superior&lt;/h2&gt;&lt;/a&gt;
&lt;p&gt;The traditional approach to inferring a visitor&amp;#8217;s origin has further weaknesses - related to the way in which page referer data is combined with page URL data to make these inferences.&lt;/p&gt;

&lt;p&gt;To work out where a visitor has come from, Google Analytics &lt;em&gt;first&lt;/em&gt; examines the page URL for the first page view of the visit, and &lt;em&gt;then&lt;/em&gt;, if no suitable &lt;code&gt;utm_&lt;/code&gt; parameters are found on the querystring, looks at the page referer to see if a URL for a referring web page is available. (For a more detailed view of the process GA uses, see &lt;a href='https://developers.google.com/analytics/devguides/platform/features/campaigns-trafficsources'&gt;this detailed description from Google&lt;/a&gt;.) There are two problems with this approach:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Google Analytics &lt;em&gt;starts&lt;/em&gt; with the source of data that is inherently more error-prone, before looking at the more reliable (if sometimes less informative) data source&lt;/li&gt;

&lt;li&gt;Google Analytics does not apply any intelligence to using the combination of both sources together to infer where a visitor has come from. If, for example, a visitor has clicked on a link from a webmail client (evident from the HTTP referer URL), but that link contains a &lt;code&gt;utm_source=bing&lt;/code&gt; parameter, we can guess, fairly reliably, that the user has not clicked on a paid Bing search campaign, but on a link that was shared by someone who originally clicked on the Bing search campaign ad. Google Analytics only considers each data point in isolation, and does not make the raw data available to us, as analysts, to make a more intelligent attribution&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In Snowplow, by contrast, we do &lt;strong&gt;not&lt;/strong&gt; collapse the two separate data sources (page URL parameters and page referer) into a single &amp;#8216;source&amp;#8217; for the visitor. Instead, we expose both to the analyst: we offer five &lt;code&gt;mkt&lt;/code&gt; fields that are set based on &lt;code&gt;utm&lt;/code&gt; parameters on the page URL (&lt;code&gt;mkt_medium&lt;/code&gt;, &lt;code&gt;mkt_source&lt;/code&gt;, &lt;code&gt;mkt_term&lt;/code&gt;, &lt;code&gt;mkt_content&lt;/code&gt;, &lt;code&gt;mkt_campaign&lt;/code&gt;) and separately we offer three &lt;code&gt;refr&lt;/code&gt; fields (&lt;code&gt;refr_medium&lt;/code&gt;, &lt;code&gt;refr_source&lt;/code&gt;, &lt;code&gt;refr_term&lt;/code&gt;) that are derived from the HTTP referer header. We can view these fields alongside one-another by executing the following query:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='psql'&gt;&lt;span class='cm'&gt;/* PostgreSQL / Redshift */&lt;/span&gt;
&lt;span class='k'&gt;SELECT&lt;/span&gt;
	&lt;span class='n'&gt;mkt_medium&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='n'&gt;mkt_source&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='n'&gt;mkt_term&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='n'&gt;refr_medium&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='n'&gt;refr_source&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='n'&gt;refr_term&lt;/span&gt;
&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='s-Name'&gt;&amp;quot;public&amp;quot;&lt;/span&gt;&lt;span class='mf'&gt;.&lt;/span&gt;&lt;span class='s-Name'&gt;&amp;quot;events_new&amp;quot;&lt;/span&gt;
&lt;span class='k'&gt;WHERE&lt;/span&gt; &lt;span class='s-Name'&gt;&amp;quot;refr_medium&amp;quot;&lt;/span&gt; &lt;span class='o'&gt;!=&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;internal&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And check the results below. Note how in some cases the &lt;code&gt;mkt&lt;/code&gt; fields are set, but the &lt;code&gt;refr&lt;/code&gt; fields are not and vice-versa. On other occasions, both sets of fields are set:&lt;/p&gt;
&lt;a href='/static/img/blog/2013/05/mkt-and-refr-fields-alongside-one-another.png'&gt;&lt;img src='/static/img/blog/2013/05/mkt-and-refr-fields-alongside-one-another.png' /&gt;&lt;/a&gt;
&lt;p&gt;This is part of the Snowplow commitment to &lt;a href='/blog/2013/04/10/snowplow-event-validation/'&gt;high fidelity analytics&lt;/a&gt;, a concept we introduced in &lt;a href='/blog/2013/04/10/snowplow-event-validation/'&gt;this blog post&lt;/a&gt;.&lt;/p&gt;
&lt;a name='adwords'&gt;&lt;h2&gt;4. Surprises when examining visitors acquired from AdWords search campaigns: most visitors clicked on ads that were not shown on Google domains&lt;/h2&gt;&lt;/a&gt;
&lt;p&gt;Another advantage of keeping your referer data separate to your marketing campaign data is that you can learn more about &lt;em&gt;where&lt;/em&gt; your marketing ads are displayed based on the additional referer data that GA ignores.&lt;/p&gt;

&lt;p&gt;To give a concrete example: Psychic Bazaar buys AdWords on the Google Search network. It does not buy ads on the Google Display network. By running the following query, we can identify which domains those AdWords ads that were clicked on were displayed:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='psql'&gt;&lt;span class='cm'&gt;/* PostgreSQL / Redshift */&lt;/span&gt;
&lt;span class='k'&gt;SELECT&lt;/span&gt;
	&lt;span class='n'&gt;refr_urlhost&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='n'&gt;COUNT&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='o'&gt;*&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='s-Name'&gt;&amp;quot;Number of click-throughs&amp;quot;&lt;/span&gt;
&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='s-Name'&gt;&amp;quot;public&amp;quot;&lt;/span&gt;&lt;span class='mf'&gt;.&lt;/span&gt;&lt;span class='s-Name'&gt;&amp;quot;events_new&amp;quot;&lt;/span&gt;
&lt;span class='k'&gt;WHERE&lt;/span&gt; &lt;span class='s-Name'&gt;&amp;quot;mkt_source&amp;quot;&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;GoogleSearch&amp;#39;&lt;/span&gt;
&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='n'&gt;refr_urlhost&lt;/span&gt;
&lt;span class='k'&gt;ORDER&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='s-Name'&gt;&amp;quot;Number of click-throughs&amp;quot;&lt;/span&gt; &lt;span class='k'&gt;DESC&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Plotting the results in Tableau, there are a few surprises:&lt;/p&gt;
&lt;a href='/static/img/blog/2013/05/google-adwords-referer-domain-analysis.jpg'&gt;&lt;img src='/static/img/blog/2013/05/google-adwords-referer-domain-analysis.jpg' /&gt;&lt;/a&gt;
&lt;p&gt;The top two domains by amount of AdWords traffic directed towards Psychic Bazaar are &lt;strong&gt;not&lt;/strong&gt; Google owned domains. They are eBay and Amazon - both websites that Psychic Bazaar sells on as a third party merchant.&lt;/p&gt;

&lt;p&gt;We expected &lt;em&gt;some&lt;/em&gt; of the domains to be non-Google domains - after all, we were aware that search engines like Ask serve results and advertising powered by Google. We &lt;em&gt;were&lt;/em&gt; surprised, however, that Amazon and eBay would do this: it seems strange that they would show ads for merchants who are competing with themselves and their own merchants. Nonetheless, if you visit either website, perform a search, and scroll down to the bottom of the result set, you will see AdWords ads displayed at the bottom:&lt;/p&gt;
&lt;a href='/static/img/blog/2013/05/amazon-with-adwords-links-screenshot.png'&gt;&lt;img src='/static/img/blog/2013/05/amazon-with-adwords-links-screenshot.png' /&gt;&lt;/a&gt;
&lt;p&gt;This puts Psychic Bazaar in the uncomfortable position of competing not only with other merchants on eBay and Amazon, but also competing with its own website ads.&lt;/p&gt;

&lt;p&gt;We were also surprised to learn that in total, 69% of the click-throughs received were from non-Google domains: in this case at least, powering search advertising on other sites doesn&amp;#8217;t simply add additional advertising inventory to Google&amp;#8217;s core search inventory, it actually makes up the bulk of that inventory. (We&amp;#8217;d be interested in finding out from other Snowplow users who buy on AdWords whether they see similar results.)&lt;/p&gt;
&lt;a name='conclusion'&gt;&lt;h2&gt;Pulling all the findings together: the value of high-fidelity data in determining where your visitors come from&lt;/h2&gt;&lt;/a&gt;
&lt;p&gt;In this post, we have seen that the extra level of data provided by Snowplow related to where visitors come from, over-and-above that provided by standard web analytics programs like Google Analytics, is incredibly valuable for a number of reasons:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;It makes it possible to identify and manage errors that are invariably introduced in the data&lt;/li&gt;

&lt;li&gt;It leads to more intelligent and robust inferences about where you traffic comes from&lt;/li&gt;

&lt;li&gt;It identifies surprising results related to the placement of your paid campaigns, which may have significant implications for your overall marketing strategy.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id='want_to_do_more_intelligent_more_robust_attribution'&gt;Want to do more intelligent, more robust attribution?&lt;/h2&gt;

&lt;p&gt;Then &lt;a href='/about/index.html'&gt;get in touch&lt;/a&gt; with the Snowplow Professional Services team.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/05/08/snowplow-0.8.2-released-with-clojure-collector-enhancements</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/05/08/snowplow-0.8.2-released-with-clojure-collector-enhancements"/>
    <title>Snowplow 0.8.2 released with Clojure Collector enhancements</title>
    <updated>2013-05-08T00:00:00+01:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We&amp;#8217;re pleased to announce the immediate availability of Snowplow &lt;strong&gt;0.8.2&lt;/strong&gt;. This release updates the Clojure Collector only; if you are using the CloudFront Collector, then no upgrade to 0.8.2 is necessary.&lt;/p&gt;

&lt;p&gt;Many thanks to community member &lt;a href='https://github.com/butlermh'&gt;Mark H. Butler&lt;/a&gt; for his major contributions to this release - much appreciated Mark!&lt;/p&gt;

&lt;p&gt;This release bumps the Clojure Collector to version &lt;strong&gt;0.4.0&lt;/strong&gt;. There are three main changes to the Collector:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Building the Collector&amp;#8217;s warfile is now much simpler, thanks to a new &lt;code&gt;lein aws&lt;/code&gt; command contributed by Mark&lt;/li&gt;

&lt;li&gt;We have fixed a bug (&lt;a href='https://github.com/snowplow/snowplow/issues/220'&gt;#220&lt;/a&gt;) where occasionally the Collector&amp;#8217;s event logging would log &lt;code&gt;&amp;quot;&amp;quot;&lt;/code&gt; (empty string) rather than &lt;code&gt;&amp;quot;-&amp;quot;&lt;/code&gt; for a missing referer. While rare, when this occurred this would cause the Snowplow ETL process to break (requiring manual editing of the offending log file)&lt;/li&gt;

&lt;li&gt;Some code tidy-up, including making the Clojure Collector work with newer versions of Leiningen&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To upgrade to this release:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Download the new warfile by right-clicking on &lt;a href='http://s3-eu-west-1.amazonaws.com/snowplow-hosted-assets/2-collectors/clojure-collector/clojure-collector-0.4.0-standalone.war'&gt;this link&lt;/a&gt; and selecting &amp;#8220;Save As&amp;#8230;&amp;#8221;&lt;/li&gt;

&lt;li&gt;Log in to your Amazon Elastic Beanstalk console&lt;/li&gt;

&lt;li&gt;Browse to your Collector&amp;#8217;s application&lt;/li&gt;

&lt;li&gt;Click the &amp;#8220;Upload New Version&amp;#8221; and upload your warfile&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;And that should be it. Thanks again to Mark for his contribution to this release - and we&amp;#8217;re excited to have some further community contributions coming very soon to Snowplow, so watch this space!&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/04/23/performing-funnel-analysis-with-snowplow</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/04/23/performing-funnel-analysis-with-snowplow"/>
    <title>Funnel analysis with Snowplow (Platform analytics part 1)</title>
    <updated>2013-04-23T00:00:00+01:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;&lt;a href='/blog/2013/04/12/online-catalog-analytics-with-snowplow/'&gt;Eleven days ago&lt;/a&gt;, we started building out the &lt;a href='/analytics/catalog-analytics/overview.html'&gt;Catalog Analytics&lt;/a&gt; section of the &lt;a href='/analytics/index.html'&gt;Analytics Cookbook&lt;/a&gt;, with a set of recipes covering how to measure the performance of &lt;a href='/analytics/catalog-analytics/measuring-and-comparing-content-page-performance.html'&gt;content pages&lt;/a&gt; and &lt;a href='/analytics/catalog-analytics/measuring-and-comparing-product-page-performance.html'&gt;product pages&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Today we&amp;#8217;ve published the first set of recipes in the new &lt;a href='/analytics/platform-analytics/overview.html'&gt;platform analytics&lt;/a&gt; section of the Cookbook. By &amp;#8216;platform analytics&amp;#8217;, we mean analytics performed to answer questions about how your platform (or &amp;#8216;website&amp;#8217;, &amp;#8216;application&amp;#8217; or &amp;#8216;product&amp;#8217;) performs. This is one of the most important branches of analytics that can be performed with clickstream, event data.&lt;/p&gt;

&lt;p&gt;The first recipes published cover how to perform &lt;a href='/analytics/platform-analytics/funnel-analysis.html'&gt;funnel analysis&lt;/a&gt; with Snowplow, like the example below, that compares the purchase funnel for an online shop by month.&lt;/p&gt;

&lt;p&gt;&lt;img alt='funnel-analysis' src='/static/img/analytics/platform-analytics/funnel-analysis/visualization-in-tableau.jpg' /&gt;&lt;/p&gt;

&lt;p&gt;Funnel analysis is an important example of platform analytics. Snowplow makes it straightforward to define and analyse funnels on the fly: unlike Google Analytics, you do not have to predefine funnels in advance, then collect data, before you can analyse how users progress through them. This is important as it makes it possible to spot e.g. that your visitors are using your platform in a particular (perhaps unexpected) way, and then immediately drill into how many exactly, are doing so, and at what point in that workflow do different users &amp;#8216;drop out&amp;#8217; of the funnel.&lt;/p&gt;

&lt;p&gt;In addition, Snowplow also makes it easy to compare multiple funnels (e.g. like the time series example above, or, instead, comparing how successfully different audience segments progress through different funnels).&lt;/p&gt;

&lt;p&gt;This will be the first of many platform analytics recipes - if there are specific analyses you&amp;#8217;d like us to cover, or would like to contribute yourself, then let us know below or by &lt;a href='/about/index.html'&gt;getting in touch&lt;/a&gt; directly.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/04/18/measuring-content-page-performance-with-snowplow</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/04/18/measuring-content-page-performance-with-snowplow"/>
    <title>Measuring content page performance with Snowplow (Catalog Analytics part 2)</title>
    <updated>2013-04-18T00:00:00+01:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;&lt;em&gt;This is the second part in our blog post series on Catalog Analytics. The &lt;a href='/blog/2013/04/12/online-catalog-analytics-with-snowplow/'&gt;first part&lt;/a&gt; was published last week.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Last week, we started building out the &lt;a href='/analytics/catalog-analytics/overview.html'&gt;Catalog Analytics&lt;/a&gt; section of the &lt;a href='/analytics/index.html'&gt;Analytics Cookbook&lt;/a&gt;, with a section documenting how to &lt;a href='/analytics/catalog-analytics/measuring-and-comparing-product-page-performance.html'&gt;measure the effectiveness of your product pages&lt;/a&gt;. Those recipes were geared specifically towards retailers.&lt;/p&gt;

&lt;p&gt;This week, we&amp;#8217;ve added an extra section to the cookbook, covering &lt;a href='/analytics/catalog-analytics/measuring-and-comparing-content-page-performance.html'&gt;how to measure engagement levels with content pages&lt;/a&gt;. The recipes covered should be of interest to any company that produces content-rich web pages. (Indeed, all the example analytics were performed using data from this very website.) However, they should be of special interest to publishers and newspaper sites that depend on driving high levels of user engagement with content to make money&lt;/p&gt;

&lt;p&gt;In the new section, we cover a range of recipes, including comparing web pages by what fraction of them is read, on average, by visitors to those pages:&lt;/p&gt;
&lt;a href='/static/img/analytics/catalog-analytics/content-page-performance/fraction-of-web-page-read.jpg'&gt;&lt;img src='/static/img/analytics/catalog-analytics/content-page-performance/fraction-of-web-page-read.jpg' /&gt;&lt;/a&gt;
&lt;p&gt;Plotting the distribution of visitors to a particular web page by the fraction of the web page that they have viewed:&lt;/p&gt;
&lt;!--more--&gt;&lt;a href='/static/img/analytics/catalog-analytics/content-page-performance/distribution-of-readers-by-fraction-of-hive-udf-post-read.jpg'&gt;&lt;img src='/static/img/analytics/catalog-analytics/content-page-performance/distribution-of-readers-by-fraction-of-hive-udf-post-read.jpg' /&gt;&lt;/a&gt;
&lt;p&gt;Comparing how long visitors dwell on average on different pages:&lt;/p&gt;
&lt;a href='/static/img/analytics/catalog-analytics/content-page-performance/average-pings-per-page-blog-only.jpg'&gt;&lt;img src='/static/img/analytics/catalog-analytics/content-page-performance/average-pings-per-page-blog-only.jpg' /&gt;&lt;/a&gt;
&lt;p&gt;Visualizing individual user journeys through a site, including identifying web pages on that visit that were particularly significant:&lt;/p&gt;
&lt;a href='/static/img/analytics/catalog-analytics/content-page-performance/customer-journey-1.jpg'&gt;&lt;img src='/static/img/analytics/catalog-analytics/content-page-performance/customer-journey-1.jpg' /&gt;&lt;/a&gt;
&lt;p&gt;Visualizing an individual user journey across a web page:&lt;/p&gt;
&lt;a href='/static/img/analytics/catalog-analytics/content-page-performance/tableau-visualization-2.JPG'&gt;&lt;img src='/static/img/analytics/catalog-analytics/content-page-performance/tableau-visualization-2.JPG' /&gt;&lt;/a&gt;
&lt;p&gt;We have much more content planned for the &lt;a href='/analytics/index.html'&gt;Analytics Cookbook&lt;/a&gt;. As always, we invite suggestions as to the type of analysis you&amp;#8217;d like us to cover, and contributions from people who&amp;#8217;ve used Snowplow data to perform useful analytics.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/04/12/snowplow-0.8.1-released-with-referer-url-parsing</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/04/12/snowplow-0.8.1-released-with-referer-url-parsing"/>
    <title>Snowplow 0.8.1 released with referer URL parsing</title>
    <updated>2013-04-12T00:00:00+01:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;Just nine days after our Snowplow 0.8.0 release, we are pleased to have our next release ready: Snowplow &lt;strong&gt;0.8.1&lt;/strong&gt;. With the last release we promised that the new Scalding-based ETL/enrichment process would lay a strong technical foundation for our roadmap - and hopefully this release bears that out!&lt;/p&gt;

&lt;p&gt;Until this release, Snowplow has provided users the raw referer URL, from which analysts can deduce who the referer was. In this release, Snowplow processes that referer URL to identify what drove a visitor to your website, specifically:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Were they driven by a search engine, social network, or link in an email program?&lt;/li&gt;

&lt;li&gt;If so, which search engine / social network / email program?&lt;/li&gt;

&lt;li&gt;If they were driven by a search engine, what query did they enter?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This data is key for performing attribution analytics.&lt;/p&gt;

&lt;p&gt;Snowplow delivers the above functionality by parsing the page referer URIs which the JavaScript tracker sends to the collector. The Snowplow enrichment layer does a couple of things with these referer URIs:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;It splits the referer URL into its six components (scheme, host, port, path, query, fragment). This makes querying referer data significantly easier, as we hope to show in future blog posts and attribution analytics recipes&lt;/li&gt;

&lt;li&gt;It looks up the referer URL in a database of known referers and attempts to extract details about this referer, which you can then use for marketing attribution. (For example - is the referer a search engine, or social network? What query did the user enter in the search engine?)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We will publish a post on how to use the data in a blog post in the near-future. In the rest of this post, then, we will cover:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href='/blog/2013/04/12/snowplow-0.8.1-released-with-referer-url-parsing#referer-parsing'&gt;Referer parsing implementation&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/04/12/snowplow-0.8.1-released-with-referer-url-parsing#example-data'&gt;Some example data&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/04/12/snowplow-0.8.1-released-with-referer-url-parsing#upgrading-usage'&gt;Upgrading and usage&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/04/12/snowplow-0.8.1-released-with-referer-url-parsing#help'&gt;Getting help&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Read on below the fold to find out more.&lt;/p&gt;
&lt;!--more--&gt;&lt;h2&gt;&lt;a name='referer-parsing'&gt;1. Referer parsing implementation&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;The extraction of the referer URL into its six components is relatively straightforward. The second stage, looking up the referer URL in a database of known referers, is worth discussing in a little more detail.&lt;/p&gt;

&lt;p&gt;Our referer analysis process uses the latest version of our separate and standalone &lt;a href='https://github.com/snowplow/referer-parser/tree/feature/social'&gt;referer-parser&lt;/a&gt; library. This library comes with a sizeable database of known referers, including search engines, social networks and webmail providers. You can view the library &lt;a href='https://github.com/snowplow/referer-parser/blob/feature/social/referers.yml'&gt;here&lt;/a&gt;. It is a straightforward YAML file containing a long list of referers. Because this is an open source list, anyone can contribute additional referers to it. In this way, we hope it will remain one of the most extensive and authoritative lists of referers to use in web analytics.&lt;/p&gt;

&lt;p&gt;Snowplow feeds the referer URI and page URI to referer-parser to identify which &lt;code&gt;refr_medium&lt;/code&gt; this referer URL belongs to:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&amp;#8220;search&amp;#8221; means that this referer is a known search engine&lt;/li&gt;

&lt;li&gt;&amp;#8220;social&amp;#8221; means a social network or similar site&lt;/li&gt;

&lt;li&gt;&amp;#8220;email&amp;#8221; means a webmail provider such as Yahoo! Mail&lt;/li&gt;

&lt;li&gt;&amp;#8220;internal&amp;#8221; means that the referer was another page on the same domain&lt;/li&gt;

&lt;li&gt;&amp;#8220;unknown&amp;#8221; means that there was a referer, but we couldn&amp;#8217;t identify it as belonging to one of the other categories&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If the referer can be found in the referer-parser database, Snowplow stores the name of the refering site in the &lt;code&gt;refr_source&lt;/code&gt; field.&lt;/p&gt;

&lt;p&gt;Finally, if the referer is a &amp;#8220;search&amp;#8221; referer and Snowplow can pull out a search query from the referer URL, it then stores this search term in the &lt;code&gt;refr_term&lt;/code&gt; field.&lt;/p&gt;
&lt;h2&gt;&lt;a name='example-data'&gt;2. Example data&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Here is an excerpt of referer data from an ecommerce retailer - right-click and select &amp;#8220;Open in New Tab&amp;#8221; to see this at full size:&lt;/p&gt;

&lt;p&gt;&lt;img alt='parsed-referers-img' src='/static/img/blog/2013/04/parsed-referers.png' /&gt;&lt;/p&gt;

&lt;p&gt;As you can see, in this excerpt we have a variety of different referers - some internal pages and some search pages (Google, Google Images, Bing Images and AOL).&lt;/p&gt;
&lt;h2&gt;&lt;a name='upgrading-usage'&gt;3. Upgrading and usage&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;As with the 0.8.0 release, this new release assumes that you are running the Hadoop (Scalding) ETL and feeding your data into Redshift.&lt;/p&gt;

&lt;p&gt;To upgrade to 0.8.1 from 0.8.0, follow these steps:&lt;/p&gt;

&lt;h3 id='31_etl'&gt;3.1 ETL&lt;/h3&gt;

&lt;p&gt;If you are using EmrEtlRunner, you need to update your configuration file, &lt;code&gt;config.yml&lt;/code&gt;, to the latest version of the Hadoop ETL:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:snowplow:
  :hadoop_etl_version: 0.2.0 # Version of the Hadoop ETL&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id='32_redshift'&gt;3.2 Redshift&lt;/h3&gt;

&lt;p&gt;We have updated the Redshift table definition, you can find the latest version in the GitHub repository &lt;a href='https://github.com/snowplow/snowplow/blob/master/4-storage/redshift-storage/sql/table-def.sql'&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If you already have your Snowplow data in the previous version of the Redshift events table, then we have written &lt;a href='https://github.com/snowplow/snowplow/blob/master/4-storage/redshift-storage/sql/migrate_0.0.1_to_0.1.0.sql'&gt;a migration script&lt;/a&gt; to handle the upgrade. &lt;strong&gt;Please review this script carefully before running and check that you are happy with how it handles the upgrade.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Also please note that we have had to remove the &amp;#8220;raw&amp;#8221; &lt;code&gt;referrer_url&lt;/code&gt; field from our Redshift events table for space reasons. This means that your historical data will lose &lt;strong&gt;all&lt;/strong&gt; referer information in your events table unless you run a re-computation, see below.&lt;/p&gt;

&lt;h3 id='33_optional_recomputation'&gt;3.3 (Optional) Re-computation&lt;/h3&gt;

&lt;p&gt;If you would like to see referer details for historic Snowplow events (i.e. events already in your Snowplow events table in Redshift), then we recommend re-running your Snowplow ETL process across all of your historical raw data.&lt;/p&gt;

&lt;p&gt;This is also advisable given that we have removed the raw &lt;code&gt;referrer_url&lt;/code&gt; field from our Redshift table definition for space reasons.&lt;/p&gt;

&lt;p&gt;To re-run your Snowplow ETL process across all your historical data, please see our answer to &lt;a href='https://github.com/snowplow/snowplow/wiki/Troubleshooting#wiki-recompute-events'&gt;I want to recompute my Snowplow events, how?&lt;/a&gt; on the Troubleshooting wiki page.&lt;/p&gt;

&lt;h3 id='34_usage'&gt;3.4 Usage&lt;/h3&gt;

&lt;p&gt;And that&amp;#8217;s it! Once you have made these changes, you should have Snowplow populating the referer details for all new events.&lt;/p&gt;
&lt;h2&gt;&lt;a name='help'&gt;4. Getting help&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;As always, if you do run into any issues or don&amp;#8217;t understand any of the above changes, please &lt;a href='https://github.com/snowplow/snowplow/issues'&gt;raise an issue&lt;/a&gt; or get in touch with us via &lt;a href='https://github.com/snowplow/snowplow/wiki/Talk-to-us'&gt;the usual channels&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;You can see the full list of issues delivered in Snowplow 0.8.1 on &lt;a href='https://github.com/snowplow/snowplow/issues?milestone=16&amp;amp;page=1&amp;amp;state=closed'&gt;GitHub&lt;/a&gt;.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/04/12/online-catalog-analytics-with-snowplow</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/04/12/online-catalog-analytics-with-snowplow"/>
    <title>Measuring product page performance with Snowplow (Catalog Analytics part 1)</title>
    <updated>2013-04-12T00:00:00+01:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We built Snowplow to enable businesses to execute the widest range of analytics on their web event data. One area of analysis we are particularly excited about is catalog analytics for retailers. Today, we&amp;#8217;ve published the &lt;a href='/analytics/catalog-analytics/measuring-and-comparing-product-page-performance.html'&gt;first recipes&lt;/a&gt; in the &lt;a href='/analytics/catalog-analytics/overview.html'&gt;catalog analytics&lt;/a&gt; section of the &lt;a href='/analytics/index.html'&gt;Snowplow Analytics Cookbook&lt;/a&gt;. These cover &lt;a href='/analytics/catalog-analytics/measuring-and-comparing-product-page-performance.html'&gt;how to measure and compare the performance of different product pages on an ecommerce site&lt;/a&gt;, using plots like the one below:&lt;/p&gt;

&lt;p&gt;&lt;img alt='Example-catalog-analytics' src='/static/img/analytics/catalog-analytics/product-page-performance/scatter-plot.jpg' /&gt;&lt;/p&gt;

&lt;p&gt;In this blog post, we will briefly outline:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href='/blog/2013/04/12/online-catalog-analytics-with-snowplow#what'&gt;What is catalog analytics?&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/04/12/online-catalog-analytics-with-snowplow#today'&gt;What recipes have been published today?&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/04/12/online-catalog-analytics-with-snowplow#tomorrow'&gt;What catalog analytics recipes can we expect published in the next few weeks and months?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!--more--&gt;&lt;a name='what'&gt;&lt;h3&gt;What is catalog analytics?&lt;/h3&gt;&lt;/a&gt;
&lt;p&gt;For very many online businesses, a &amp;#8220;catalog&amp;#8221; is a central part of the user-proposition. For a retailer, for example, a catalog is the collection of products they are selling. For a media site, a catalog is the collection of content items (be they articles or videos) offered. For an affiliate site, a catalog is the collection of links or offers available. For a vertical search site, a catalog is the list of indexed entries presented to the user.&lt;/p&gt;

&lt;p&gt;Understanding how well different items in that catalog &amp;#8220;perform&amp;#8221; is key to enabling these businesses to:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Source better catalog items - e.g. by buying more effectively if they are a retailer, or designing new products if they are a manufacturer, or producing better content if they are a media company&lt;/li&gt;

&lt;li&gt;Present catalog items more effectively - e.g. by surfacing more popular items, using search and recommendation to enable users to dive more deeply into the catalog, or personalise the items shown based on user or item data&lt;/li&gt;

&lt;li&gt;Optimize the existing catalog items - e.g. by tweaking product prices, adjusting content copy and so on&lt;/li&gt;
&lt;/ol&gt;
&lt;a name='today'&gt;&lt;h3&gt;Which catalog analytics recipes have been published today?&lt;/h3&gt;&lt;/a&gt;
&lt;p&gt;Today, we published a &lt;a href='/analytics/catalog-analytics/measuring-and-comparing-product-page-performance.html'&gt;set of recipes to enable businesses to compare the performance of product pages&lt;/a&gt;. The published analysis is particularly relevant to online retailers - it makes it easy for them to identify:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Which products are good candidates for increased marketing spend: namely, highly converting pages with low traffic levels&lt;/li&gt;

&lt;li&gt;Which product pages are underperforming: perhaps because the products on them are not competitively priced, or because the content or images are weak&lt;/li&gt;

&lt;li&gt;Which products are &amp;#8220;star performers&amp;#8221;: attracting large volumes of traffic and converting those users effectively&lt;/li&gt;

&lt;li&gt;Which products are &amp;#8220;dogs&amp;#8221;: products which do not attract traffic, and do not convert the traffic they do get&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;You can check out the recipes &lt;a href='/analytics/catalog-analytics/measuring-and-comparing-product-page-performance.html'&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;a name='tomorrow'&gt;&lt;h3&gt;Which catalog analytics recipes will be published next?&lt;/h3&gt;&lt;/a&gt;
&lt;p&gt;These recipes are just the start in what we hope will develop into a long series of recipes for catalog analytics. Some of the other recipes that we plan to add include:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Analysing how well individual content pieces drive engagement-on-site&lt;/li&gt;

&lt;li&gt;Analysing how much different catalog items contribute to driving traffic to a site&lt;/li&gt;

&lt;li&gt;Analysing how well different catalog items contribute to basket growth through up-sell and increased time-on-site&lt;/li&gt;

&lt;li&gt;Personalising the items displayed to users based on user data and item data&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If there are other examples of catalog analyses you would like us to include - &lt;a href='https://github.com/snowplow/snowplow/wiki/Talk-to-us'&gt;drop us a line&lt;/a&gt;! We&amp;#8217;re always interested to explore new and innovative ways of using Snowplow data to drive business value&amp;#8230;&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/04/10/snowplow-event-validation</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/04/10/snowplow-event-validation"/>
    <title>Towards high-fidelity web analytics - introducing Snowplow's innovative new event validation capabilities</title>
    <updated>2013-04-10T00:00:00+01:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;A key goal of the Snowplow project is enabling &lt;strong&gt;high-fidelity analytics&lt;/strong&gt; for businesses running Snowplow.&lt;/p&gt;

&lt;p&gt;What do we mean by high-fidelity analytics? Simply put, high-fidelity analytics means Snowplow faithfully recording &lt;em&gt;all&lt;/em&gt; customer events in a rich, granular, non-lossy and unopinionated way.&lt;/p&gt;

&lt;p&gt;This data is incredibly valuable: it enables companies to better understand their customers and develop and tailor products and services to them. Ensuring that the data is high fidelity is essential to ensuring that any operational and strategic decision making that&amp;#8217;s made on the basis of that data is sound. Guaranteeing data fidelity is not a sexy topic. But it&amp;#8217;s an important one.&lt;/p&gt;

&lt;p&gt;Surprisingly, ensuring your data is high fidelity is &lt;strong&gt;not&lt;/strong&gt; something that is enforced by other analytics products.&lt;/p&gt;

&lt;p&gt;&lt;img alt='high-fidelity' src='/static/img/blog/2013/04/high-fidelity-2000.jpg' /&gt;&lt;/p&gt;

&lt;p&gt;Why is Snowplow so unusual in aiming for high-fidelity analytics? Most often, analytics vendors sacrifice the goal of high-fidelity data at the altar of these three compromises:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Premature aggregation&lt;/strong&gt; - when the data store gets too large, or the reports take too long to generate, it&amp;#8217;s tempting to perform the aggregation and roll-up of the raw event data earlier, sometimes even at the point of collection. Of course this offers a huge potential performance boost to the tool, but at the cost of a huge degree of customer data fidelity&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Ignoring bad news&lt;/strong&gt; - the nature of event data means that often incomplete, corrupted or plain wrong data is sent in to the analytics tool by the event trackers. Handling bad event data is complicated (let&amp;#8217;s go shopping!). Instead of dealing with the complexity, most analytics packages just throw the bad data away silently; this is why tag audit companies like &lt;a href='http://www.observepoint.com/'&gt;ObservePoint&lt;/a&gt; exist&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Being over-opinionated&lt;/strong&gt; - customer analytics is full of challenging questions which need answering before you can analyse the data: do I track users by their first-party cookie, third-party cookie, business ID and/or IP address? Do I use the server clock, or the user&amp;#8217;s clock to log the event time? When does a user session start and end? Because these questions can be difficult to answer, most analytics tools don&amp;#8217;t ask them: instead they take an opinionated view of the &amp;#8220;right answer&amp;#8221; and silently enforce that view through their event collection, storage and analysis. By the time users realize that the logic enforced is one that does not work for their business, they are already tied to that vendor and the imperfect data set they have created with that vendor to date.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To deliver on the goal of high-fidelity analytics, then, we&amp;#8217;re trying to steer Snowplow around these three common pitfalls as best we can.&lt;/p&gt;

&lt;p&gt;We have talked in detail on our website and wiki about avoiding pitfall #1, Premature aggregation. In short: we do &lt;strong&gt;no&lt;/strong&gt; aggregation - Snowplow users have access to granular, event level data, so that they can work out how best they should aggregate it for each type of analysis they wish to perform.&lt;/p&gt;

&lt;p&gt;We will blog more about our ideas to combat #3, Being over-opinionated, in the future.&lt;/p&gt;

&lt;p&gt;For the rest of this blog post, though, we will look at our solution to pitfall #2, Ignoring bad news: namely, &lt;strong&gt;event validation&lt;/strong&gt;.&lt;/p&gt;
&lt;!--more--&gt;
&lt;p&gt;Our new Scalding-based event enrichment process (introduced in &lt;a href='/blog/2013/04/03/snowplow-0.8.0-released-with-all-new-scalding-based-data-enrichment/'&gt;our last blog post&lt;/a&gt;) introduces the concept of &lt;strong&gt;event validation&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Instead of &amp;#8220;ignoring bad news&amp;#8221;, the Snowplow enrichment engine now validates that every logged event matches the format that we expect for Snowplow events - be they page views, ecommerce transactions, custom structured events or some other type of event. Events which do not match this format are stored in a new &amp;#8220;Bad Rows&amp;#8221; bucket in Amazon S3, along with the specific data validations which the event failed.&lt;/p&gt;

&lt;p&gt;By way of example, here are a couple of &lt;a href='https://github.com/snowplow/snowplow/wiki/snowplow-tracker-protocol#wiki-event'&gt;custom structured events&lt;/a&gt; generated by a ecommerce site running Snowplow; both of these events failed the new validation step in our Scalding ETL process. You will note that the bad rows are logged to the S3 bucket in JSON format - we have &amp;#8220;pretty printed&amp;#8221; the rows to make them easier to read:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='json'&gt;&lt;span class='p'&gt;{&lt;/span&gt;
  &lt;span class='nt'&gt;&amp;quot;line&amp;quot;&lt;/span&gt;&lt;span class='p'&gt;:&lt;/span&gt; &lt;span class='s2'&gt;&amp;quot;2012-11-14\t11:53:07\tDUB2\t3707\t92.237.59.86\tGET\td10wr4jwvp55f9.cloudfront.net\t\/ice.png\t200\thttps:\/\/www.psychicbazaar.com\/shop\/checkout\/?token=EC-6H7658847D893744L\tMozilla\/5.0%20(Windows%20NT%206.0)%20AppleWebKit\/537.11%20(KHTML,%20like%20Gecko)%20Chrome\/23.0.1271.64%20Safari\/537.11\tev_ca=ecomm&amp;amp;ev_ac=checkout&amp;amp;ev_la=id_city&amp;amp;ev_pr=SUCCESS&amp;amp;ev_va=Liverpool&amp;amp;tid=404245&amp;amp;uid=4434aa64ebbefad6&amp;amp;vid=1&amp;amp;lang=en-US&amp;amp;refr=https%253A%252F%252Fwww.paypal.com%252Fuk%252Fcgi-bin%252Fwebscr%253Fcmd%253D_flow%2526SESSION%345DiuJgdNO9t8v06miTqv5EHhhGukkGNH3dfRqrKhe0i-UM9FCbVNg26G10sRC%2526dispatch%253D50a222a57771920b6a3d7b606239e4d529b525e0b7e69bf0224adecfb0124e9b61f737ba21b0819882a9058c69cd92dcdac469a145272506&amp;amp;f_pdf=1&amp;amp;f_qt=0&amp;amp;f_realp=0&amp;amp;f_wma=1&amp;amp;f_dir=1&amp;amp;f_fla=1&amp;amp;f_java=1&amp;amp;f_gears=0&amp;amp;f_ag=1&amp;amp;res=1920x1080&amp;amp;cookie=1&amp;amp;url=https%253A%252F%252Fwww.psychicbazaar.com%252Fshop%252Fcheckout%252F%253Ftoken%253DEC-6H7658847D893744L\t-\tHit\tAN6xpNsbS0JS05bqjmnbJdZDkl-cVkTPQsAJDlIOgAIG4hcPTTlMFA==&amp;quot;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
  &lt;span class='nt'&gt;&amp;quot;errors&amp;quot;&lt;/span&gt;&lt;span class='p'&gt;:&lt;/span&gt; &lt;span class='p'&gt;[&lt;/span&gt;
    &lt;span class='s2'&gt;&amp;quot;Field [ev_va]: cannot convert [Liverpool] to Float&amp;quot;&lt;/span&gt;
  &lt;span class='p'&gt;]&lt;/span&gt;
&lt;span class='p'&gt;}&lt;/span&gt;
&lt;span class='p'&gt;{&lt;/span&gt;
  &lt;span class='nt'&gt;&amp;quot;line&amp;quot;&lt;/span&gt;&lt;span class='p'&gt;:&lt;/span&gt; &lt;span class='s2'&gt;&amp;quot;2012-11-14\t11:53:13\tDUB2\t3707\t92.237.59.86\tGET\td10wr4jwvp55f9.cloudfront.net\t\/ice.png\t200\thttps:\/\/www.psychicbazaar.com\/shop\/checkout\/?token=EC-6H7658847D893744L\tMozilla\/5.0%20(Windows%20NT%206.0)%20AppleWebKit\/537.11%20(KHTML,%20like%20Gecko)%20Chrome\/23.0.1271.64%20Safari\/537.11\tev_ca=ecomm&amp;amp;ev_ac=checkout&amp;amp;ev_la=id_state&amp;amp;ev_pr=SUCCESS&amp;amp;ev_va=Merseyside&amp;amp;tid=462879&amp;amp;uid=4434aa64ebbefad6&amp;amp;vid=1&amp;amp;lang=en-US&amp;amp;refr=https%253A%252F%252Fwww.paypal.com%252Fuk%252Fcgi-bin%252Fwebscr%253Fcmd%253D_flow%2526SESSION%345DiuJgdNO9t8v06miTqv5EHhhGukkGNH3dfRqrKhe0i-UM9FCbVNg26G10sRC%2526dispatch%253D50a222a57771920b6a3d7b606239e4d529b525e0b7e69bf0224adecfb0124e9b61f737ba21b0819882a9058c69cd92dcdac469a145272506&amp;amp;f_pdf=1&amp;amp;f_qt=0&amp;amp;f_realp=0&amp;amp;f_wma=1&amp;amp;f_dir=1&amp;amp;f_fla=1&amp;amp;f_java=1&amp;amp;f_gears=0&amp;amp;f_ag=1&amp;amp;res=1920x1080&amp;amp;cookie=1&amp;amp;url=https%253A%252F%252Fwww.psychicbazaar.com%252Fshop%252Fcheckout%252F%253Ftoken%253DEC-6H7658847D893744L\t-\tHit\tXbvEfkx7BvngWyY23OLDvyFi8mXe2E_nhBaJwkzCG3aNxUng1jz4hQ==&amp;quot;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
  &lt;span class='nt'&gt;&amp;quot;errors&amp;quot;&lt;/span&gt;&lt;span class='p'&gt;:&lt;/span&gt; &lt;span class='p'&gt;[&lt;/span&gt;
    &lt;span class='s2'&gt;&amp;quot;Field [ev_va]: cannot convert [Merseyside] to Float&amp;quot;&lt;/span&gt;
  &lt;span class='p'&gt;]&lt;/span&gt;
&lt;span class='p'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;These validation errors occurred because the ecommerce site incorrectly tried to log customer address information in the &lt;code&gt;value&lt;/code&gt; field of a custom structured event; the &lt;code&gt;value&lt;/code&gt; field only supports numeric values (and is stored in Redshift in a float field). When we saw these validation errors, we notified the site and they corrected their Google Tag Manager implementation.&lt;/p&gt;

&lt;p&gt;Currently these bad rows are simply stored for inspection in the Bad Rows bucket in S3, while Snowplow carries on with the raw event processing. This lets the Snowplow user tackle the tagging/data quality issues offline, without disrupting the loading of all their high-fidelity, now-validated event data into Redshift. It leaves open the possibility that the user can fix and reprocess the bad rows.&lt;/p&gt;

&lt;p&gt;In the future we could look into ways of sending alerts when bad rows are generated, or even look into ways of automatically fixing bad rows and submitting them for re-processing.&lt;/p&gt;

&lt;p&gt;This is straight forward stuff - but compare it with the approach taken by other web analytics vendors. If a Google Analytics user sends incorrectly configured data into GA, for example, one of two things happens:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;GA silently ignores the data&lt;/li&gt;

&lt;li&gt;GA accommodates the data, so that it corrupts reports produced in GA&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For the GA user, spotting the error is impossible in either case. Not only has a data point been lost, but potentially an erroneous data point has been introduced, one that will be very hard to debug given that users can never inspect the underlying data.&lt;/p&gt;

&lt;p&gt;This becomes more of a problem as we move to a Unviersal Analytics world: one in which companies feed GA with &lt;strong&gt;all&lt;/strong&gt; their customer event data from a variety of systems. Ensuring that the system is fed with perfect data will only get harder, whilst dealing with situations where erroneous data has been pushed in will remain impossible.&lt;/p&gt;

&lt;p&gt;That completes our brief look at event validation. We hope it is clear why this is such an important topic. For us at Snowplow, event validation is a key part of our quest for high-fidelity event analytics - so expect to hear more from us on this topic soon!&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/04/03/snowplow-0.8.0-released-with-all-new-scalding-based-data-enrichment</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/04/03/snowplow-0.8.0-released-with-all-new-scalding-based-data-enrichment"/>
    <title>Snowplow 0.8.0 released with all-new Scalding-based data enrichment</title>
    <updated>2013-04-03T00:00:00+01:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;A new month, a new release! We&amp;#8217;re excited to announce the immediate availability of Snowplow version &lt;strong&gt;0.8.0&lt;/strong&gt;. This has been our most complex release to date: we have done a full rewrite our ETL (aka enrichment) process, adding a few nice data quality enhancements along the way.&lt;/p&gt;

&lt;p&gt;This release has been heavily informed by our January blog post, &lt;a href='/blog/2013/01/09/from-etl-to-enrichment/#scalding'&gt;The Snowplow development roadmap for the ETL step - from ETL to enrichment&lt;/a&gt;. In technical terms, we have ported our existing ETL process (which was a combination of HiveQL scripts plus a custom Java deserializer) to a new Hadoop-only ETL process which does not require Hive. The new ETL process is written in Scala, using &lt;a href='https://github.com/twitter/scalding'&gt;Scalding&lt;/a&gt;, a Scala API built on top of &lt;a href='http://www.cascading.org'&gt;Cascading&lt;/a&gt;, the Hadoop ETL framework.&lt;/p&gt;

&lt;p&gt;In the rest of this post we will cover:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href='/blog/2013/04/03/snowplow-0.8.0-released-with-all-new-scalding-based-data-enrichment/#benefits'&gt;The benefits of the new ETL&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/04/03/snowplow-0.8.0-released-with-all-new-scalding-based-data-enrichment/#limitations'&gt;Limitations of the new ETL&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/04/03/snowplow-0.8.0-released-with-all-new-scalding-based-data-enrichment/#infobright-hive-note'&gt;A note for Infobright/Hive users&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/04/03/snowplow-0.8.0-released-with-all-new-scalding-based-data-enrichment/#upgrading-usage'&gt;Upgrading and usage&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/04/03/snowplow-0.8.0-released-with-all-new-scalding-based-data-enrichment/#help'&gt;Getting help&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Read on below the fold to find out more.&lt;/p&gt;
&lt;!--more--&gt;&lt;h2&gt;&lt;a name='benefits'&gt;1. Benefits of the new ETL&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;The new ETL process is essentially a direct re-write of the existing Hive-based ETL process, however we have made some functionality improvements along the way. The benefits of the new Scalding-based ETL process as we see them are as follows:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Fewer moving parts&lt;/strong&gt; - the new ETL process no longer requires Hive running on top of Hadoop. This should make it simpler to setup and more robust&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Data validation&lt;/strong&gt; - the new ETL process runs a set of validation checks on each raw line of Snowplow log data. If a line does not pass validation, then the line along with its validation errors is written to a new bucket for &amp;#8220;bad rows&amp;#8221;&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Better handling of unexpected errors&lt;/strong&gt; - if you set your ETL process to continue on unexpected errors, any raw lines which trigger unexpected errors will appear in a new &amp;#8220;errors&amp;#8221; bucket&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Fewer Redshift import errors&lt;/strong&gt; - we now truncate six &amp;#8220;high-risk&amp;#8221; fields (&lt;code&gt;useragent&lt;/code&gt;, &lt;code&gt;page_title&lt;/code&gt; et al) and validate that &lt;code&gt;ev_value&lt;/code&gt; is a float, to prevent the most common Redshift load errors&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Stronger technical foundation for our roadmap&lt;/strong&gt; - the foundations are now in-place for us adding more enrichment of our Snowplow events (e.g. &lt;a href='https://github.com/snowplow/snowplow/issues?milestone=16&amp;amp;state=open'&gt;referer parsing&lt;/a&gt; and &lt;a href='https://github.com/snowplow/snowplow/issues?milestone=17&amp;amp;state=open'&gt;geo-location&lt;/a&gt; - both coming soon), and the &amp;#8220;gang of three&amp;#8221; cross-row ETL processes which we are planning (&lt;a href='https://github.com/snowplow/snowplow/issues/20'&gt;one&lt;/a&gt;, &lt;a href='https://github.com/snowplow/snowplow/issues/169'&gt;two&lt;/a&gt;, &lt;a href='https://github.com/snowplow/snowplow/issues/187'&gt;three&lt;/a&gt;)&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a name='limitations'&gt;2. Limitations of the new ETL&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;We want to be very clear about the limitations of the new ETL process as it stands today:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Redshift only&lt;/strong&gt; - the new ETL process only supports writing out in Redshift format. We discuss this further in &lt;a href='#infobright-hive-note'&gt;A note for Infobright/Hive users&lt;/a&gt; below.&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Performance&lt;/strong&gt; - the new ETL process takes almost twice as long as the old Hive process. This is because it is essentially running twice: once to generate the Redshift output, and once to generate the &amp;#8220;bad rows&amp;#8221;: in a Hadoop world these two outputs are handled sequentially as separate MapReduce jobs&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Small files problem&lt;/strong&gt; - being Hadoop-based, our ETL inherits Hadoop&amp;#8217;s &lt;a href='http://amilaparanawithana.blogspot.co.uk/2012/06/small-file-problem-in-hadoop.html'&gt;&amp;#8220;small files problem&amp;#8221;&lt;/a&gt;. Above around 3,000 raw Snowplow log files, the job can slow down considerably, so aim to keep your runs smaller than this&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Prototype&lt;/strong&gt; - please treat the new ETL process as a prototype. We &lt;strong&gt;strongly recommend&lt;/strong&gt; trying it out away from your existing Snowplow installation rather than upgrading your existing process in-place&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;On points 2 and 3: rest assured that improving the performance of the new Hadoop ETL (not least by tackling the small files problem) is a key priority for the Snowplow Analytics team going forwards.&lt;/p&gt;
&lt;h2&gt;&lt;a name='upgrading-usage'&gt;3. A note for Infobright/Hive users&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;If you are using Infobright or plain-Hive to store your Snowplow data, we understand that you&amp;#8217;ll be feeling a little left out of this release. Unfortunately, supporting Redshift, Infobright and Hive all in this version 1 simply wasn&amp;#8217;t feasible from a development-effort perspective.&lt;/p&gt;

&lt;p&gt;This does not mean that we are giving up on Hive and Infobright: on the contrary, we have big plans for both data storage targets.&lt;/p&gt;

&lt;p&gt;For &lt;strong&gt;Hive users&lt;/strong&gt; - we are working on a new Avro-based storage format for Snowplow events. Being based on &lt;a href='http://avro.apache.org/'&gt;Avro&lt;/a&gt;, it should be less fragile than our existing flatfile approach, easily queryable from Hive using the &lt;a href='https://cwiki.apache.org/Hive/avroserde-working-with-avro-from-hive.html'&gt;AvroSerde&lt;/a&gt;, and &lt;em&gt;faster&lt;/em&gt; to query (because data is stored more efficiently in binary format). Evolving the Avro schema to incorporate our &lt;a href='/blog/2013/02/04/help-us-build-out-the-snowplow-event-model/'&gt;event dictionary&lt;/a&gt; will also be much more straightforward. This will be the 0.9.x release series and should come later in Q2 or early Q3.&lt;/p&gt;

&lt;p&gt;For &lt;strong&gt;Infobright users&lt;/strong&gt; - we will be adding Infobright support into the new Hadoop-based ETL later this year. If you would rather not wait, we recommend switching to Redshift now or switching to PostgreSQL support when this is released in late Q2.&lt;/p&gt;

&lt;p&gt;We should also stress: it is totally safe for Infobright/Hive users to upgrade to 0.8.0: the Hive-based ETL process continues to work as before, and we will be continuing to support the Hive ETL with bug fixes etc for the foreseeable future.&lt;/p&gt;

&lt;p&gt;As always, you can check out upcoming features on our &lt;a href='https://github.com/snowplow/snowplow/wiki/Product-roadmap'&gt;Product roadmap&lt;/a&gt; wiki page.&lt;/p&gt;
&lt;h2&gt;&lt;a name='limitations'&gt;4. Upgrading and usage&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Upgrading is simply a matter of:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Upgrading your EmrEtlRunner installation to the latest code on GitHub&lt;/li&gt;

&lt;li&gt;Updating your &lt;code&gt;config.yml&lt;/code&gt; configuration file&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Nothing else is changed in this release.&lt;/p&gt;

&lt;p&gt;You can see the template for the new &lt;code&gt;config.yml&lt;/code&gt; file format &lt;a href='https://github.com/snowplow/snowplow/blob/master/3-etl/emr-etl-runner/config/config.yml'&gt;here&lt;/a&gt;. The new format introduces a few new configuration options:&lt;/p&gt;

&lt;h3 id='updated_and_new_buckets'&gt;Updated and new buckets&lt;/h3&gt;

&lt;p&gt;Under &lt;code&gt;:buckets:&lt;/code&gt; we have changed the path to our hosted assets:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:assets: s3://snowplow-hosted-assets&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We have also added two new buckets:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:out_bad_rows: ADD HERE # Leave blank for Hive ETL.
:out_errors: ADD HERE # Leave blank for Hive ETL.&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;out_bad_rows&lt;/code&gt; bucket will contain any raw Snowplow log lines which did not pass the ETL&amp;#8217;s validation. If you set &lt;code&gt;continue_on_unexpected_error&lt;/code&gt; to true, then the &lt;code&gt;out_errors&lt;/code&gt; bucket will contain any raw Snowplow log lines which caused an unexpected error.&lt;/p&gt;

&lt;h3 id='new_etl_configuration'&gt;New ETL configuration&lt;/h3&gt;

&lt;p&gt;Under &lt;code&gt;:etl:&lt;/code&gt; we have added:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:job_name: Snowplow ETL # Give your job a name
:implementation: hadoop # Or &amp;#39;hive&amp;#39; for legacy ETL&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;job_name&lt;/code&gt; should make it easier to identify your ETL job in the Elastic MapReduce console.&lt;/p&gt;

&lt;p&gt;Change &lt;code&gt;implementation&lt;/code&gt; to &amp;#8220;hive&amp;#8221; to use our alternative Hive-based ETL process.&lt;/p&gt;

&lt;h3 id='new_version'&gt;New version&lt;/h3&gt;

&lt;p&gt;Under &lt;code&gt;:snowplow:&lt;/code&gt; we have added a new version to track our new ETL process:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:hadoop_etl_version: 0.1.0&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That&amp;#8217;s it! Once you have made those configuration changes, you should be up-and-running with the new ETL process.&lt;/p&gt;
&lt;h2&gt;&lt;a name='help'&gt;5. Getting help&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;As always, if you do run into any issues or don&amp;#8217;t understand any of the above changes, please &lt;a href='https://github.com/snowplow/snowplow/issues'&gt;raise an issue&lt;/a&gt; or get in touch with us via &lt;a href='https://github.com/snowplow/snowplow/wiki/Talk-to-us'&gt;the usual channels&lt;/a&gt;.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/03/25/snowplow-tracker-for-arduino-released-sensor-and-event-analytics-for-the-internet-of-things</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/03/25/snowplow-tracker-for-arduino-released-sensor-and-event-analytics-for-the-internet-of-things"/>
    <title>Snowplow Arduino Tracker released - sensor and event analytics for the internet of things</title>
    <updated>2013-03-25T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;Today we are releasing our first non-Web tracker for Snowplow - an event tracker for the &lt;a href='http://www.arduino.cc/'&gt;Arduino&lt;/a&gt; open-source electronics prototyping platform. The &lt;a href='https://github.com/snowplow/snowplow-arduino-tracker'&gt;Snowplow Arduino Tracker&lt;/a&gt; lets you track sensor and event-stream information from one or more IP-connected Arduino boards.&lt;/p&gt;

&lt;p&gt;We chose this as our first non-Web tracker because we&amp;#8217;re hugely excited about the potential of sophisticated analytics for the &lt;a href='http://www.forbes.com/sites/ericsavitz/2013/01/14/ces-2013-the-break-out-year-for-the-internet-of-things/'&gt;Internet of Things&lt;/a&gt;, following in the footsteps of great projects like &lt;a href='https://cosm.com/'&gt;Cosm&lt;/a&gt; and &lt;a href='http://exosite.com/'&gt;Exosite&lt;/a&gt;. And of course, Snowplow&amp;#8217;s extremely-scalable architecture is a great fit for the huge volumes of events and sensor readings which machines are able to generate - you could say that we are already &amp;#8220;machine-scale&amp;#8221;!&lt;/p&gt;

&lt;p&gt;&lt;img alt='arduino-photo' src='/static/img/blog/2013/03/arduino-board-photo.jpg' /&gt;&lt;/p&gt;

&lt;p&gt;As far as we know, this is the first time an event analytics platform has released a dedicated tracker for the maker community; we can&amp;#8217;t wait to see what the Arduino and Snowplow communities will use it for! Some ideas we had were:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Deploying a set of Snowplow-connected Arduinos to monitor the environment (temperature, humidity, light levels etc) in your home&lt;/li&gt;

&lt;li&gt;Tracking the movement of products around your shop/warehouse/factory using Arduino, &lt;a href='http://arduino.cc/blog/category/wireless/rfid/'&gt;RFID readers&lt;/a&gt; and Snowplow&lt;/li&gt;

&lt;li&gt;Sending vehicle fleet information (locations, speeds, fuel levels etc) back to Snowplow using Arduino&amp;#8217;s &lt;a href='http://www.cooking-hacks.com/index.php/documentation/tutorials/arduino-3g-gprs-gsm-gps'&gt;3G and GPS&lt;/a&gt; shields&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In fact Alex has gone ahead and written a sample Arduino sketch to track temperatures and log the readings to Snowplow - you can find his project on GitHub at &lt;a href='https://github.com/alexanderdean/arduino-temp-tracker'&gt;alexanderdean/arduino-temp-tracker&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Want to find out more? To get started using our event tracker for Arduino, check out:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The &lt;a href='https://github.com/snowplow/snowplow-arduino-tracker'&gt;GitHub repository&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;The &lt;a href='https://github.com/snowplow/snowplow/wiki/Arduino-Tracker'&gt;Technical Documentation&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;The &lt;a href='https://github.com/snowplow/snowplow/wiki/Arduino-Tracker-Setup'&gt;Setup Guide&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Happy making!&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/03/20/rob-slifka-elasticity</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/03/20/rob-slifka-elasticity"/>
    <title>Inside the Plow - Rob Slifka's Elasticity</title>
    <updated>2013-03-20T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;&lt;em&gt;The Snowplow platform is built standing on the shoulders of a whole host of different open source frameworks, libraries and tools. Without the amazing ongoing work by these individuals, companies and not-for-profits, the Snowplow project literally could not exist.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;As part of our &amp;#8220;Inside the Plow&amp;#8221; series, we will also be showcasing some of these core components of the Snowplow stack, and talking to their creators. To kick us off, we are delighted to have &lt;a href='https://twitter.com/robslifka'&gt;Rob Slifka&lt;/a&gt;, VP of &lt;a href='http://www.sharethrough.com/engineering'&gt;Engineering&lt;/a&gt; at &lt;a href='http://www.sharethrough.com'&gt;Sharethrough&lt;/a&gt; in San Francisco, talking to us about his &lt;a href='https://github.com/rslifka/elasticity'&gt;Elasticity&lt;/a&gt; project. For those who aren&amp;#8217;t aware: Elasticity is a Ruby library which we use as part of our &lt;a href='https://github.com/snowplow/snowplow/wiki/setting-up-EmrEtlRunner'&gt;EmrEtlRunner&lt;/a&gt;, to make it easy to automate the Snowplow ETL Job on Amazon Elastic MapReduce. The Elasticity library is a great piece of tech - and indeed was a major factor in us deciding to write EmrEtlRunner in Ruby.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;With the introductions done, let&amp;#8217;s hand over to Rob to tell us a bit about himself, Elasticity and what he&amp;#8217;s working on next:&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img alt='rob-slifka-img' src='/static/img/blog/2013/03/rob-slifka.jpeg' /&gt;&lt;/p&gt;

&lt;p&gt;Thanks Alex! Quick bit about me: I&amp;#8217;ve been in software development since the mid 90s working on everything from Java Swing (design automation tools) to embedded Jetty (email encryption) and now a mixture of Ruby and Scala. Since 2010, I’ve been responsible for &lt;a href='http://www.sharethrough.com/engineering'&gt;engineering&lt;/a&gt; at &lt;a href='http://www.sharethrough.com'&gt;Sharethrough&lt;/a&gt; - an ad tech company based out of San Francisco. We’re building a native advertising platform based on the belief that advertising is no longer sustainable as banners and punch-the-monkey ads and has begun the transition to engaging, non-interruptive choice-based experiences. One thing that a lot of people outside of ad tech don’t realize is that online advertising is synonymous with scale and some of the most interesting technology problems are driven from those demands. This is where &lt;a href='https://github.com/rslifka/elasticity'&gt;Elasticity&lt;/a&gt; comes in.&lt;/p&gt;
&lt;!--more--&gt;
&lt;p&gt;Our ads report a significant amount of information around user behaviour which we then use in decisioning, pricing and insight derivation (e.g. “Do people share videos before watching them?”). In the early days, we were handling what we now consider a small volume of logs (1GB/day) with a correspondingly quick and dirty ETL: a log parser that updated the MySQL instance backing our reporting dashboards. Fast forward to 2013 and our log intake is north of 30GB/day. With this volume of data and with the insights we wanted to derive, that process didn’t cut it and we determined that the quickest way for us to begin deriving value from our data was via &lt;a href='http://aws.amazon.com/elasticmapreduce/'&gt;Amazon Elastic MapReduce&lt;/a&gt; (hereon referred to as EMR).&lt;/p&gt;

&lt;p&gt;If you’re unfamiliar with AWS service interaction and evolution, it often follows this pattern (using EMR as an example):&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Use the AWS UI to become familiar with EMR. Manually step through the creation of Job Flows, choose your job type, asset location, cluster configuration options, etc.&lt;/li&gt;

&lt;li&gt;(Optionally) Graduate to the AWS CLI once you see a pattern in your interaction. You’re in active development and moving more quickly than before. You’ve decided on Pig, your assets and clusters are stored and predicted consistently, etc.&lt;/li&gt;

&lt;li&gt;Use an Amazon or 3rd-party API to integrate the tool into your workflow so you don’t have to copy the command line tools around your infrastructure.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Amazon’s tools are developer services, not meant for absolutely streamlined consumption; some legwork is required. The AWS CLI is a thin wrapper around the EMR REST API meaning there are numerous and frequently mutually exclusive options. If you chose to use the CLI, you’ll spend a significant amount of time learning how to use the command line tools by reading the developer API guide. Why isn’t there a programmatic way to work with EMR that follows the same mental model as that which is exposed via the UI and doesn’t require you to understand the EMR REST API?&lt;/p&gt;

&lt;p&gt;That’s where Elasticity comes in.&lt;/p&gt;

&lt;p&gt;As an API author you can choose to represent the EMR model directly or layer your own model on top of it. As a point of reference, this is a partial list of EMR REST API calls: AddInstanceGroups, AddJobFlowSteps, DescribeJobFlows, etc.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;One way to provide access to EMR might be via Ruby methods that wrap each of these calls, something like &lt;a href='https://github.com/rslifka/elasticity/blob/master/lib/elasticity/emr.rb'&gt;this&lt;/a&gt;. And by providing only this, you as a developer would be required to understand the EMR API documentation to use Elasticity - still not much better than using the CLI tools&lt;/li&gt;

&lt;li&gt;Another option might be for Elasticity to say, &amp;#8220;Forget about job flows! I&amp;#8217;m going to give you a &amp;#8216;Session&amp;#8217; and each step of your job flow is a &amp;#8216;Batch Processing Function&amp;#8217;&amp;#8220;… and you’d be properly confused, having to map between your understanding of EMR and what Elasticity exposes&lt;/li&gt;

&lt;li&gt;Elasticity went with a third option - mirroring what was offered in the AWS EMR UI: &lt;strong&gt;Elasticity is a Ruby gem for working with EMR that requires you only understand the EMR user&amp;#8217;s manual, not the EMR developer’s manual.&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Elasticity v1 split (2) and (3) above, encapsulating an entire “job” as your unit of interaction with the API. You&amp;#8217;d create and configure a &amp;#8220;HiveJob&amp;#8221; and start it. This was assuming that most interactions with EMR are single-step.&lt;/p&gt;

&lt;p&gt;Elasticity v2 was a major rewrite focusing wholly on option (3) above. You create and configure &amp;#8220;JobFlows&amp;#8221; and add steps to them, just as you do in the UI; a much more comfortable model for those familiar with the EMR UI (which we all were at some point when we learned how to use EMR).&lt;/p&gt;

&lt;p&gt;Elasticity v3&amp;#8230; who knows? First and foremost, I work on features that Sharethrough requires. We&amp;#8217;re in a steady state with EMR at the moment and now I&amp;#8217;m hoping the community has some suggestions :)&lt;/p&gt;

&lt;p&gt;Thanks for making it this far! &lt;strong&gt;And if anything I touched on sounds interesting, Sharethrough is hiring and we&amp;#8217;re relo-friendly! Check us out at &lt;a href='http://www.sharethrough.com/engineering'&gt;Sharethrough Engineering&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/03/03/snowplow-0.7.6-released-with-redshift-data-warehouse-support</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/03/03/snowplow-0.7.6-released-with-redshift-data-warehouse-support"/>
    <title>Snowplow 0.7.6 released with Redshift data warehouse support</title>
    <updated>2013-03-03T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We&amp;#8217;re excited to announce the immediate release of Snowplow version &lt;strong&gt;0.7.6&lt;/strong&gt; with support for storing your Snowplow events in &lt;a href='http://aws.amazon.com/redshift/'&gt;Amazon Redshift&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We were very excited when Amazon announced Redshift back in late 2012, and we have been working to integrate Snowplow data since Redshift became generally available two weeks ago. Our tests with Redshift since launch have not disappointed - and we can&amp;#8217;t wait to see what the Snowplow community do with the new platform!&lt;/p&gt;

&lt;p&gt;In this post we will cover:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href='/blog/2013/03/03/snowplow-0.7.6-released-with-redshift-data-warehouse-support#why-redshift'&gt;Why Redshift is a great fit for Snowplow data&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/03/03/snowplow-0.7.6-released-with-redshift-data-warehouse-support#this-version'&gt;Changes in this version&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/03/03/snowplow-0.7.6-released-with-redshift-data-warehouse-support#snowplow-redshift'&gt;Setting up Snowplow for Redshift&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/03/03/snowplow-0.7.6-released-with-redshift-data-warehouse-support#upgrading'&gt;Upgrading for Infobright/Hive users&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/03/03/snowplow-0.7.6-released-with-redshift-data-warehouse-support#roadmap'&gt;Roadmap and next steps&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/03/03/snowplow-0.7.6-released-with-redshift-data-warehouse-support#help'&gt;Getting help&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Read on below the fold to find out more.&lt;/p&gt;
&lt;!--more--&gt;&lt;h2&gt;&lt;a name='why-redshift'&gt;1. Why Amazon Redshift is a great fit for Snowplow data&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Snowplow datasets get big very quickly: we store at least one line of data for every single event that occurs on your website or application. Our largest users are recording 100M+ events every day; these data volumes get very big, very quickly.&lt;/p&gt;

&lt;p&gt;Whereas traditional web analytics packages deal with this by aggregating data to feed pre-cut reports, we built Snowplow specifically to maintain that granularity, because that granularity is critical to:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Performing bespoke analyses e.g. analysing conversion rates by product, or segmenting users by behaviour&lt;/li&gt;

&lt;li&gt;Joining Snowplow data with third party datasets e.g. from your CMS, CRM, adserver or marketing data&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As a result, we built Snowplow on technologies like Hadoop and Hive from the get-go to enable Snowplow users to record and analyse massive volumes of event data.&lt;/p&gt;

&lt;p&gt;The trouble with Hadoop and Hive is that they are not great tools for &lt;a href='http://en.wikipedia.org/wiki/Online_analytical_processing'&gt;OLAP analysis&lt;/a&gt;. As a result, we added support for &lt;a href='http://www.infobright.com/'&gt;Infobright Community Edition&lt;/a&gt;: an open source columnar database you deploy yourself which scales to terabytes.&lt;/p&gt;

&lt;p&gt;With &lt;a href='http://aws.amazon.com/redshift/'&gt;Amazon Redshift&lt;/a&gt;, we now support a columnar database that scales to petabytes. Not only that, but:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Amazon Redshift is a fully managed service. Unlike Infobright, you do not need to setup and run your own servers: you simply connect to AWS and ask Amazon to fire up a Redshift cluster for you&lt;/li&gt;

&lt;li&gt;Redshift clusters can easily be scaled up and down over time with your data requirements: it is simple to add and remove nodes; you can even snapshot and hibernate them&lt;/li&gt;

&lt;li&gt;A wide range of analytics tools can be plugged directly into Redshift via well supported PostgreSQL JDBC and ODBC drivers. It already works with &lt;a href='http://chartio.com/'&gt;Chartio&lt;/a&gt;. A dedicated connectors for Tableau is currently in development&lt;/li&gt;

&lt;li&gt;Redshift supports a broader set of SQL functionality than Infobright. In particular, loading data into Redshift is much more straightforward, and debugging errors when they occur much easier&lt;/li&gt;

&lt;li&gt;Data can be loaded directly from S3 into Redshift, making the Snowplow ETL pipeline simpler and more efficient. And support for loading Redshift directly from Elastic MapReduce is in Amazon&amp;#8217;s roadmap&lt;/li&gt;

&lt;li&gt;Redshift is part of the AWS cloud, around which Snowplow has been built&lt;/li&gt;

&lt;li&gt;Redshift is highly cost-effective: costing as little $1,000 per TB per year&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Please read on to find out how to get started with Snowplow and Redshift.&lt;/p&gt;
&lt;h2&gt;&lt;a name='this-version'&gt;2. Changes in this version&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;This version makes a set of changes to Snowplow to add support for Redshift; it is important to understand these changes even if you are using our Hive or Infobright storage options and are not interested in using Redshift.&lt;/p&gt;

&lt;p&gt;The main changes are as follows:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The HiveQL scripts have been renamed to align with the three storage formats they generate: &lt;code&gt;mysql-infobright&lt;/code&gt;, &lt;code&gt;hive&lt;/code&gt; and &lt;code&gt;redshift&lt;/code&gt;.&lt;/li&gt;

&lt;li&gt;EmrEtlRunner and StorageLoader have both been upgraded, to versions 0.0.9 and 0.0.5 respectively&lt;/li&gt;

&lt;li&gt;The configuration file formats for EmrEtlRunner and StorageLoader have been updated to add support for Redshift and reflect the new naming conventions&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Additionally we have fixed two bugs in this version:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Our Bash files for automating EmrEtlRunner and/or StorageLoader had a bug where the &lt;code&gt;BUNDLE_GEMFILE&lt;/code&gt; configuration lines did not end in &lt;code&gt;/Gemfile&lt;/code&gt;. This has now been fixed. Many thanks to &lt;a href='https://github.com/EZWrighter'&gt;Eric Zimmerman&lt;/a&gt; for reporting this!&lt;/li&gt;

&lt;li&gt;We have widened the field storing the raw useragent string in Infobright (and Redshift) to 1000 characters: 500 characters wasn&amp;#8217;t enough&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If you are a Snowplow Hive/Infobright user with no interest in Redshift, please jump to &lt;a href='FIXME#hive-ice-upgrade'&gt;Upgrading for Infobright/Hive users&lt;/a&gt; for information on how to upgrade.&lt;/p&gt;
&lt;h2&gt;&lt;a name='snowplow-redshift'&gt;3. Setting up Snowplow for Redshift&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;This is a relatively simple process, which is fully documented on our wiki:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href='https://github.com/snowplow/snowplow/wiki/setting-up-redshift'&gt;Setup a Redshift cluster&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='https://github.com/snowplow/snowplow/wiki/1-Installing-EmrEtlRunner'&gt;Configure EmrEtlRunner to output Snowplow events in Redshift format&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='https://github.com/snowplow/snowplow/wiki/1-Installing-the-StorageLoader'&gt;Configure StorageLoader to load Snowplow events into Redshift&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;(Optional) &lt;a href='https://github.com/snowplow/snowplow/wiki/Setting-up-ChartIO-to-visualise-your-data#wiki-redshift'&gt;Connect Chartio to Snowplow data in Redshift&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Once you have completed these steps, you should now have a Snowplow eventstream data warehouse setup in Redshift!&lt;/p&gt;
&lt;h2&gt;&lt;a name='upgrading'&gt;4. Upgrading for Infobright/Hive users&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;These are the steps to upgrade Snowplow to version 0.7.6 if you are using the Hive or Infobright output formats:&lt;/p&gt;

&lt;h3 id='41_emretlrunner'&gt;4.1 EmrEtlRunner&lt;/h3&gt;

&lt;p&gt;If you are using EmrEtlRunner, you need to update your configuration file, &lt;code&gt;config.yml&lt;/code&gt;, to use the latest versions of the Hive serde and HiveQL scripts:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:snowplow:
  :serde_version: 0.5.5
  :hive_hiveql_version: 0.5.7
  :mysql_infobright_hiveql_version: 0.0.8
  :redshift_hiveql_version: 0.0.1&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you are outputting Snowplow events in Infobright format, you need to update this line too:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:etl:
  ...
  :storage_format: mysql-infobright # Used to be &amp;#39;non-hive&amp;#39;&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id='42_infobright_table_definition'&gt;4.2 Infobright table definition&lt;/h3&gt;

&lt;p&gt;If you are using Infobright Community Edition for analysis, you will need to update your table definition, because we have widened the &lt;code&gt;useragent&lt;/code&gt; field.&lt;/p&gt;

&lt;p&gt;To make this easier for you, we have created a script:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;4-storage/infobright-storage/migrate_to_008.sh&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Running this script will create a new table, &lt;code&gt;events_008&lt;/code&gt; (version 0.0.8 of the Infobright table definition) in your &lt;code&gt;snowplow&lt;/code&gt; database, copying across all your data from your existing &lt;code&gt;events&lt;/code&gt; table, which will not be modified in any way.&lt;/p&gt;

&lt;h3 id='43_storageloader'&gt;4.3 StorageLoader&lt;/h3&gt;

&lt;p&gt;If you are using StorageLoader, you need to update your configuration file, &lt;code&gt;config.yml&lt;/code&gt;, to the new format:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:storage:
  :type:     infobright
  :host:     # Not used by Infobright
  :database: ADD IN HERE
  :port:     # Not used by Infobright
  :table:    events_008 # NOT &amp;quot;events_007&amp;quot; any more
  :username: ADD IN HERE
  :password: ADD IN HERE&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that the &lt;code&gt;table&lt;/code&gt; field now points to the new &lt;code&gt;events_008&lt;/code&gt; table created in section 4.2 above.&lt;/p&gt;

&lt;p&gt;Done!&lt;/p&gt;
&lt;h2&gt;&lt;a name='roadmap'&gt;5. Roadmap and next steps&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;We&amp;#8217;re really excited about the opportunities for building web-scale, low-cost data warehouses for marketing and product analytics with Amazon Redshift, and we&amp;#8217;re super-excited about all of the potential uses of Snowplow event data within these data warehouses. If you&amp;#8217;re excited too, do &lt;a href='mailto:sales@snowplowanalytics.com'&gt;get in touch&lt;/a&gt;!&lt;/p&gt;

&lt;p&gt;Separately, this is the last planned release in the 0.7.x series. We&amp;#8217;re already hard at work on the next release, which will see us swap out the current Hive-based ETL process for a more robust, performant and extensible Hadoop (Cascading/Scalding) ETL process.&lt;/p&gt;

&lt;p&gt;To keep track of this new release, please sign up for our &lt;a href='https://groups.google.com/forum/?fromgroups#!forum/snowplow-user'&gt;mailing list&lt;/a&gt; and checkout our &lt;a href='https://github.com/snowplow/snowplow/wiki/Product-roadmap'&gt;Roadmap&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a name='help'&gt;6. Getting help&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;As always, if you do run into any issues or don&amp;#8217;t understand any of the above changes, please &lt;a href='https://github.com/snowplow/snowplow/issues'&gt;raise an issue&lt;/a&gt; or get in touch with us via &lt;a href='https://github.com/snowplow/snowplow/wiki/Talk-to-us'&gt;the usual channels&lt;/a&gt;.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/02/25/snowplow-0.7.5-released-with-important-javascript-fix</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/02/25/snowplow-0.7.5-released-with-important-javascript-fix"/>
    <title>Snowplow 0.7.5 released with important JavaScript fix</title>
    <updated>2013-02-25T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We are releasing Snowplow version &lt;strong&gt;0.7.5&lt;/strong&gt; - which upgrades the JavaScript tracker to version &lt;strong&gt;0.11.1&lt;/strong&gt;. This is a small but important release - because we are fixing an issue introduced in Snowplow version a month ago: &lt;strong&gt;if you are on versions 0.9.1 to 0.11.0 of the JavaScript tracker, please upgrade!&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Essentially, version 0.9.1 of the JavaScript tracker (released in Snowplow 0.7.2) fixed &lt;a href='https://github.com/snowplow/snowplow/pull/147'&gt;an old bug&lt;/a&gt; which we inherited from the Piwik JavaScript tracker when we forked it early last year; this bug was stopping the secure flag from being set on Snowplow&amp;#8217;s first-party cookies when sent via HTTPS pages.&lt;/p&gt;

&lt;p&gt;Unfortunately, fixing this bug caused a larger problem: sending cookies securely from HTTPS pages meant that HTTP pages could no longer read the cookies, causing the cookies to be regenerated. As a result, Snowplow&amp;#8217;s &lt;code&gt;domain_userid&lt;/code&gt; field (the first-party user ID) was being &lt;strong&gt;reset&lt;/strong&gt; when a user browsed from an HTTPS page (e.g. checkout) back to an HTTP page.&lt;/p&gt;

&lt;p&gt;So, this release fixes &lt;a href='https://github.com/snowplow/snowplow/issues/181'&gt;a nasty bug&lt;/a&gt;. Thanks to the Piwik team for their help in brainstorming this problem!&lt;/p&gt;

&lt;p&gt;Please update your website(s) to use the latest version of the JavaScript tracker, which is version 0.11.1. As always, the updated minified tracker is available here:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http(s)://d1fc8wv8zag5ca.cloudfront.net/0.11.1/sp.js&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As always, if you do run into any issues or don&amp;#8217;t understand any of the above changes, please &lt;a href='https://github.com/snowplow/snowplow/issues'&gt;raise an issue&lt;/a&gt; or get in touch with us via &lt;a href='https://github.com/snowplow/snowplow/wiki/Talk-to-us'&gt;the usual channels&lt;/a&gt;.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/02/22/snowplow-0.7.4-released-for-better-eventstream-analytics</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/02/22/snowplow-0.7.4-released-for-better-eventstream-analytics"/>
    <title>Snowplow 0.7.4 released for better eventstream analytics</title>
    <updated>2013-02-22T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;Another week, another release! We&amp;#8217;re excited to announce Snowplow version &lt;strong&gt;0.7.4&lt;/strong&gt;. The primary purpose of this release is to clean up and rationalise our event data model, in particular around &lt;strong&gt;user IDs&lt;/strong&gt; and &lt;strong&gt;event timestamps&lt;/strong&gt;. This release should lay the foundations for more sophisticated eventstream analytics (such as funnel analysis), by:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Enabling companies to assign custom user IDs (e.g. when a customer logs on)&lt;/li&gt;

&lt;li&gt;Distinguish between IDs set at a domain level (via first-party cookies) and at a network level (via third-party cookies)&lt;/li&gt;

&lt;li&gt;Enable precise ordering of events in a user&amp;#8217;s click stream with accuracy correct to the milli-second&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Many thanks to Snowplow users &lt;a href='http://www.simplybusiness.co.uk/'&gt;Simply Business&lt;/a&gt; and &lt;a href='https://github.com/shermozle'&gt;Simon Rumble&lt;/a&gt; (APN) for suggesting many of these changes and helping us to design them.&lt;/p&gt;

&lt;p&gt;In this post we will cover:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href='/blog/2013/02/22/snowplow-0.7.4-released-for-better-eventstream-analytics#user-ids'&gt;Our new user IDs&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/02/22/snowplow-0.7.4-released-for-better-eventstream-analytics#event-tstamps'&gt;Our new event timestamps&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/02/22/snowplow-0.7.4-released-for-better-eventstream-analytics#bug-fixes'&gt;Bug fixes&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/02/22/snowplow-0.7.4-released-for-better-eventstream-analytics#deprecations'&gt;Breaking changes&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/02/22/snowplow-0.7.4-released-for-better-eventstream-analytics#upgrading'&gt;Upgrading&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/02/22/snowplow-0.7.4-released-for-better-eventstream-analytics#help'&gt;Getting help&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Read on below the fold to find out more!&lt;/p&gt;
&lt;!--more--&gt;&lt;h2&gt;&lt;a name='user-ids'&gt;1. Our new user IDs&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Historically, Snowplow has supported a single &lt;code&gt;user_id&lt;/code&gt; field. Unfortunately, there were three issues with this:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Snowplow was &lt;strong&gt;overloading&lt;/strong&gt; the field with two different meanings - if a user was running the CloudFront collector, the &lt;code&gt;user_id&lt;/code&gt; field contained a user ID from a first-party cookie (set by the JavaScript tracker). If a user was running the Clojure collector, the &lt;code&gt;user_id&lt;/code&gt; field contained a cross-domain user ID as set by in a third-party cookie (and the JavaScript-set first-party cookie was ignored).&lt;/li&gt;

&lt;li&gt;Both meanings of &lt;code&gt;user_id&lt;/code&gt; were &lt;strong&gt;web-specific&lt;/strong&gt; - neither made sense for user tracking in a mobile app or any other platform which does not support cookies&lt;/li&gt;

&lt;li&gt;No support for a &lt;strong&gt;custom&lt;/strong&gt; user ID - Snowplow did not allow you to track a custom &lt;code&gt;user_id&lt;/code&gt; specific to your business, such as your users&amp;#8217; account numbers in your ecommerce package&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In this release, we aim to solve these issues by separating out user IDs into three separate fields:&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Field&lt;/th&gt;&lt;th&gt;Description&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;user_id&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;A custom user ID which you can set. Will be supported by all trackers (except the no-JS tracker)&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;domain_userid&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;A user ID set by the JavaScript tracker in a first-party cookie; tied to the current domain&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;network_userid&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;A user ID set by the Clojure collector in a third-party cookie; shared across a network of different domains&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;To make use of the new custom user ID, you can use the following new method in the JavaScript tracker:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='javascript'&gt;&lt;span class='nx'&gt;_snaq&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='nx'&gt;push&lt;/span&gt;&lt;span class='p'&gt;([&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;setUserId&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;alex-123&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;]);&lt;/span&gt; &lt;span class='c1'&gt;// Business-defined user ID&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Please note that you must call &lt;code&gt;setUserId()&lt;/code&gt; on every page where you know the user ID - in other words the setting does not survive a pageload.&lt;/p&gt;

&lt;p&gt;Whether or not each type of user ID is available for your analysis depends on the combination of your tracker and collector:&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Tracker&lt;/th&gt;&lt;th&gt;Collector&lt;/th&gt;&lt;th&gt;-&amp;gt;&lt;/th&gt;&lt;th&gt;&lt;code&gt;user_id&lt;/code&gt;*&lt;/th&gt;&lt;th&gt;&lt;code&gt;domain_userid&lt;/code&gt;&lt;/th&gt;&lt;th&gt;&lt;code&gt;network_userid&lt;/code&gt;&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;JS tracker&lt;/td&gt;&lt;td style='text-align: left;'&gt;CloudFront&lt;/td&gt;&lt;td style='text-align: left;'&gt;-&amp;gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;Yes&lt;/td&gt;&lt;td style='text-align: left;'&gt;Yes&lt;/td&gt;&lt;td style='text-align: left;'&gt;No&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;JS tracker&lt;/td&gt;&lt;td style='text-align: left;'&gt;Clojure&lt;/td&gt;&lt;td style='text-align: left;'&gt;-&amp;gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;Yes&lt;/td&gt;&lt;td style='text-align: left;'&gt;Yes&lt;/td&gt;&lt;td style='text-align: left;'&gt;Yes&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;No-JS tracker&lt;/td&gt;&lt;td style='text-align: left;'&gt;CloudFront&lt;/td&gt;&lt;td style='text-align: left;'&gt;-&amp;gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;N/A&lt;/td&gt;&lt;td style='text-align: left;'&gt;No&lt;/td&gt;&lt;td style='text-align: left;'&gt;No&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;No-JS tracker&lt;/td&gt;&lt;td style='text-align: left;'&gt;Clojure&lt;/td&gt;&lt;td style='text-align: left;'&gt;-&amp;gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;N/A&lt;/td&gt;&lt;td style='text-align: left;'&gt;No&lt;/td&gt;&lt;td style='text-align: left;'&gt;Yes&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Non-web tracker&lt;/td&gt;&lt;td style='text-align: left;'&gt;Any&lt;/td&gt;&lt;td style='text-align: left;'&gt;-&amp;gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;Yes&lt;/td&gt;&lt;td style='text-align: left;'&gt;No&lt;/td&gt;&lt;td style='text-align: left;'&gt;No&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;* Assuming you have added a call to &lt;code&gt;setUserId()&lt;/code&gt; - which isn&amp;#8217;t possible in the no-JS tracker.&lt;/p&gt;
&lt;h2&gt;&lt;a name='event-tstamps'&gt;2. Our new event timestamps&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Previously our data model included two fields, &lt;code&gt;dt&lt;/code&gt; and &lt;code&gt;tm&lt;/code&gt;, to track the date and time at which each event occurred. This timestamp was based on when the Snowplow event collector &lt;em&gt;received&lt;/em&gt; the event, &lt;strong&gt;not&lt;/strong&gt; when the tracker &lt;em&gt;sent&lt;/em&gt; the event.&lt;/p&gt;

&lt;p&gt;There are a couple of limitations to using a collector-based timestamp for eventstream analysis:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;If two events occur almost simultaneously in the client, there is no guarantee which will be received by the collector first (because of the unpredictability of the HTTP connection)&lt;/li&gt;

&lt;li&gt;If a tracker batches events and then sends them in one batch (e.g. a cellphone out of cell coverage) , then all of the events in that batch will end up with the same collector timestamp, despite occurring at different times&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For this reason, in this release we are introducing a tracker-based timestamp, which is set by the tracker when the event occurs, and is stored in our data model alongside the collector timestamp. This means that we now have five timestamp fields:&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Field&lt;/th&gt;&lt;th&gt;Datatype&lt;/th&gt;&lt;th&gt;Description&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;collector_dt&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;string&lt;/td&gt;&lt;td style='text-align: left;'&gt;Date when the collector received the event&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;collector_tm&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;string&lt;/td&gt;&lt;td style='text-align: left;'&gt;Time when the collector received the event&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;dvce_dt&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;string&lt;/td&gt;&lt;td style='text-align: left;'&gt;Date on the client device when the event occurred&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;dvce_tm&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;string&lt;/td&gt;&lt;td style='text-align: left;'&gt;Time on the client device when the event occurred&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;dvce_epoch&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;bigint&lt;/td&gt;&lt;td style='text-align: left;'&gt;Milliseconds since the epoch (1/1/1970) on the client device when the tracker sent the event&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;Note that we include a super-precise &lt;code&gt;dvce_epoch&lt;/code&gt; field because our &lt;code&gt;dvce_tm&lt;/code&gt; field is not accurate to milliseconds; when querying within a given user session, simply order by &lt;code&gt;dvce_epoch&lt;/code&gt; to get the user&amp;#8217;s eventstream accurately ordered to the millisecond.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A word of warning:&lt;/strong&gt; tracker timestamps are great for understanding the correct order of, and elapsed time between, events from a specific user session. However, they are not a safe way of understanding when a given event actually occurred, because you &lt;strong&gt;cannot&lt;/strong&gt; trust the clocks on users&amp;#8217; devices. So, stick to the collector timestamp if you need to understand when in the real-world events occurred across multiple users.&lt;/p&gt;
&lt;a name='bug-fixes'&gt;&lt;h2&gt;3. Bug fixes&lt;/h2&gt;&lt;/a&gt;
&lt;p&gt;As well as the new fields introduced above, this release also includes an important bug fix in the JavaScript tracker, related to our newly-named &lt;code&gt;domain_userid&lt;/code&gt;. Many thanks to &lt;a href='https://github.com/ngsmrk'&gt;Angus Mark&lt;/a&gt; at &lt;a href='http://www.simplybusiness.co.uk/'&gt;Simply Business&lt;/a&gt; for alerting us to this.&lt;/p&gt;

&lt;p&gt;Previously, the site/app ID as set by &lt;code&gt;setSiteId()&lt;/code&gt; was used as an input into naming the first-party cookie which stores the &lt;code&gt;domain_userid&lt;/code&gt;. This had the unfortunate side effect that, if you used multiple site IDs for different parts of your site, your visitors would end up with different &lt;code&gt;domain_userid&lt;/code&gt;s for the different parts of your site.&lt;/p&gt;

&lt;p&gt;This release fixes this problem - and it does so in a way that should not corrupt or reset any of your existing &lt;code&gt;domain_userids&lt;/code&gt;. Going forwards, you can set different parts of your site to different app IDs without &amp;#8220;fragmenting&amp;#8221; your &lt;code&gt;domain_userid&lt;/code&gt;s.&lt;/p&gt;
&lt;a name='deprecations'&gt;&lt;h2&gt;4. Deprecations&lt;/h2&gt;&lt;/a&gt;
&lt;p&gt;Making the above changes to clean up our event data model have necessarily involved some deprecations, as set out in the table below. &lt;strong&gt;When upgrading to the new version of the JavaScript tracker (0.11.0), please update your JavaScript tags as per the instructions below to avoid problems:&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Type of change&lt;/th&gt;&lt;th&gt;Component&lt;/th&gt;&lt;th&gt;Change&lt;/th&gt;&lt;th&gt;Comment&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Deprecation&lt;/td&gt;&lt;td style='text-align: left;'&gt;JavaScript tracker&lt;/td&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;attachUserId()&lt;/code&gt; deprecated&lt;/td&gt;&lt;td style='text-align: left;'&gt;Remove - this doesn&amp;#8217;t do anything any more&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Deprecation&lt;/td&gt;&lt;td style='text-align: left;'&gt;JavaScript tracker&lt;/td&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;setSiteId()&lt;/code&gt; deprecated&lt;/td&gt;&lt;td style='text-align: left;'&gt;Use &lt;code&gt;setAppId()&lt;/code&gt; instead&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Deprecation&lt;/td&gt;&lt;td style='text-align: left;'&gt;JavaScript tracker&lt;/td&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;getVisitorId()&lt;/code&gt; deprecated&lt;/td&gt;&lt;td style='text-align: left;'&gt;Use &lt;code&gt;getDomainUserId()&lt;/code&gt; instead&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Deprecation&lt;/td&gt;&lt;td style='text-align: left;'&gt;JavaScript tracker&lt;/td&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;getVisitorInfo()&lt;/code&gt; deprecated&lt;/td&gt;&lt;td style='text-align: left;'&gt;Use &lt;code&gt;getDomainUserInfo()&lt;/code&gt; instead&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Data change&lt;/td&gt;&lt;td style='text-align: left;'&gt;S3 &amp;amp; Infobright storage&lt;/td&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;visit_id&lt;/code&gt; renamed&lt;/td&gt;&lt;td style='text-align: left;'&gt;Now called &lt;code&gt;domain_sessionidx&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;The first change is because we are no longer overloading the &lt;code&gt;user_id&lt;/code&gt; field with multiple different meanings. The next three changes are simply to bring the JavaScript method names inline with the field names we are using in our data model.&lt;/p&gt;

&lt;p&gt;The final change is to rename the &lt;code&gt;visit_id&lt;/code&gt; field to &lt;code&gt;domain_sessionidx&lt;/code&gt;. The field&amp;#8217;s contents is unchanged, but we have updated the name to reflect that:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The field holds the current count (aka index) of visits by this user, not a random ID&lt;/li&gt;

&lt;li&gt;Going forwards we will be tracking different types of sessions (mobile, desktop etc), not just website visits&lt;/li&gt;

&lt;li&gt;The field is generated by the JavaScript tracker, using a first party cookie. The name &lt;code&gt;domain_sessionidx&lt;/code&gt; makes the limited scope of this field clearer&lt;/li&gt;
&lt;/ol&gt;
&lt;a name='upgrading'&gt;&lt;h2&gt;5. Upgrading&lt;/h2&gt;&lt;/a&gt;
&lt;p&gt;Because we are making some significant changes to the event data model, such as &amp;#8220;unpacking&amp;#8221; the overloaded &lt;code&gt;user_id&lt;/code&gt; field, this upgrade is relatively complex. &lt;strong&gt;Please read this upgrade guide in full first before starting your upgrade&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The upgrade process has multiple steps - we will discuss each step in turn, and then suggest a way of scheduling this upgrade to prevent any data corruption.&lt;/p&gt;

&lt;h3 id='41_javascript_tracker'&gt;4.1 JavaScript tracker&lt;/h3&gt;

&lt;p&gt;Please update your website(s) to use the latest version of the JavaScript tracker, which is version &lt;strong&gt;0.11.0&lt;/strong&gt;. As always, the updated minified tracker is available here:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http(s)://d1fc8wv8zag5ca.cloudfront.net/0.11.0/sp.js&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Don&amp;#8217;t forget to update your Snowplow tags as per the updates in &lt;a href='#deprecations'&gt;Deprecations&lt;/a&gt; above.&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id='42_clojure_collector'&gt;4.2 Clojure collector&lt;/h3&gt;

&lt;p&gt;If you are using the CloudFront collector, you can skip this step.&lt;/p&gt;

&lt;p&gt;If you are using the Clojure collector, you will need to upgrade it to the latest version, &lt;strong&gt;0.3.0&lt;/strong&gt;. You can find the new version packaged as a complete WAR file on our &lt;a href='https://github.com/snowplow/snowplow/wiki/Hosted-assets'&gt;Hosted assets&lt;/a&gt; page. If you have forgotten how to deploy the Clojure-based collector, you will find full instructions on our Wiki, &lt;a href='https://github.com/snowplow/snowplow/wiki/setting-up-the-clojure-collector'&gt;Setting up the Clojure collector&lt;/a&gt; (you can skip most of the setup steps).&lt;/p&gt;

&lt;h3 id='43_etl'&gt;4.3 ETL&lt;/h3&gt;

&lt;p&gt;If you are using EmrEtlRunner, you need to update your configuration file, &lt;code&gt;config.yml&lt;/code&gt;, to use the latest versions of the Hive serde and HiveQL scripts:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:snowplow:
  :serde_version: 0.5.5
  :hive_hiveql_version: 0.5.6
  :non_hive_hiveql_version: 0.0.7&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id='44_infobright'&gt;4.4 Infobright&lt;/h3&gt;

&lt;p&gt;If you are using Infobright Community Edition for analysis, you will need to update your table definition. To make this easier for you, we have created two scripts:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;4-storage/infobright-storage/migrate_006_cf_to_007.sh
4-storage/infobright-storage/migrate_006_clj_to_007.sh&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Choose the appropriate script depending on which collector you are using: &amp;#8220;cf&amp;#8221; means the CloudFront collector, &amp;#8220;clj&amp;#8221; the Clojure collector.&lt;/p&gt;

&lt;p&gt;Running this script will create a new table, &lt;code&gt;events_007&lt;/code&gt; (version &lt;strong&gt;0.0.7&lt;/strong&gt; of the Infobright table definition) in your &lt;code&gt;snowplow&lt;/code&gt; database, copying across all your data from your existing &lt;code&gt;events_006&lt;/code&gt; table, which will not be modified in any way.&lt;/p&gt;

&lt;p&gt;Once you have run this, don&amp;#8217;t forget to update your StorageLoader&amp;#8217;s &lt;code&gt;config.yml&lt;/code&gt; to load into the new &lt;code&gt;events_007&lt;/code&gt; table, not your old &lt;code&gt;events&lt;/code&gt; table:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:storage:
  :type: infobright
  :database: snowplow
  :table:    events_007 # NOT &amp;quot;events_006&amp;quot; any more&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id='45_scheduling_the_upgrade'&gt;4.5 Scheduling the upgrade&lt;/h3&gt;

&lt;p&gt;This upgrade has to be carefully scheduled because we are changing the meaning of the &lt;code&gt;uid&lt;/code&gt; field in the JavaScript tracker, and we are moving data from the old &lt;code&gt;user_id&lt;/code&gt; field into the new &lt;code&gt;network_userid&lt;/code&gt; or &lt;code&gt;domain_userid&lt;/code&gt; fields.&lt;/p&gt;

&lt;p&gt;Our suggested approach is as follows:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Setup the new JavaScript tracker version 0.11.0 in your tag manager as per section 4.1 above, but &lt;strong&gt;do not&lt;/strong&gt; publish it live yet&lt;/li&gt;

&lt;li&gt;(If you are using the Clojure collector) Get the Clojure collector version 0.3.0 ready in Elastic Beanstalk as per section 4.2 above, but &lt;strong&gt;do not&lt;/strong&gt; deploy it live yet&lt;/li&gt;

&lt;li&gt;Start a manual run of the EmrEtlRunner for your site&amp;#8230;&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;As soon as&lt;/strong&gt; the manual run has copied all of your available Snowplow logs into your Processing Bucket, publish the new JavaScript tracker live, and deploy your new Clojure collector live (if you are using it)&lt;/li&gt;

&lt;li&gt;Wait for the EmrEtlRunner operation complete&lt;/li&gt;

&lt;li&gt;If you are using Infobright, run the StorageLoader and wait for it to finish&lt;/li&gt;

&lt;li&gt;Now upgrade the ETL as per section 4.3 above&lt;/li&gt;

&lt;li&gt;Now upgrade Infobright (if you are using it) as per section 4.4 above&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This upgrade approach should prevent any user ID data from ending up in the wrong fields in your Snowplow event store.&lt;/p&gt;
&lt;h2&gt;&lt;a name='help'&gt;6. Getting help&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;As always, if you do run into any issues or don&amp;#8217;t understand any of the above changes, please &lt;a href='https://github.com/snowplow/snowplow/issues'&gt;raise an issue&lt;/a&gt; or get in touch with us via &lt;a href='https://github.com/snowplow/snowplow/wiki/Talk-to-us'&gt;the usual channels&lt;/a&gt;.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/02/20/transferring-data-from-s3-to-redshift-at-the-command-line</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/02/20/transferring-data-from-s3-to-redshift-at-the-command-line"/>
    <title>Bulk loading data from Amazon S3 into Redshift at the command line</title>
    <updated>2013-02-20T00:00:00+00:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;On Friday Amazon launched &lt;a href='http://aws.amazon.com/redshift/'&gt;Redshift&lt;/a&gt;, a fully managed, petabyte-scale data warehouse service. We&amp;#8217;ve been busy since building out Snowplow support for Redshift, so that Snowplow users can use Redshift to store their granular, customer-level and event-level data for OLAP analysis.&lt;/p&gt;

&lt;p&gt;In the course of building out Snowplow support for Redshift, we need to bulk load data stored in S3 into Redshift, programmatically. Unfortunately, the Redshift Java SDK is very slow at inserts, so not suitable bulk loading. We found a simple workaround that might be helpful for anyone who wishes to bulk load data into Redshift from S3, and have documented it below.&lt;/p&gt;

&lt;h2 id='an_overview_of_the_workaround'&gt;An overview of the workaround&lt;/h2&gt;

&lt;p&gt;Amazon enables users to bulk load data from S3 into Redshift by executing queries with the following form:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='postgresql'&gt;&lt;span class='k'&gt;copy&lt;/span&gt; &lt;span class='n'&gt;events&lt;/span&gt; 
&lt;span class='k'&gt;from&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;s3://$MY-BUCKET/PATH/TO/FILES/FOR/UPLOAD&amp;#39;&lt;/span&gt; 
&lt;span class='n'&gt;credentials&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;aws_access_key_id=$ACCESS-KEY;aws_secret_access_key=$SECRET-ACCESS-KEY&amp;#39;&lt;/span&gt; 
&lt;span class='k'&gt;delimiter&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;\t&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;However, these queries can only be executed in a SQL client running a JDBC or ODBC driver compatible with Redshift. (Links to those drivers can be found &lt;a href='http://docs.aws.amazon.com/redshift/latest/gsg/before-you-begin.html#getting-started-download-tools'&gt;here&lt;/a&gt;. )&lt;/p&gt;
&lt;!--more--&gt;
&lt;p&gt;In order to orchestrate bulk loading programmatically, we used &lt;a href='http://www.xigole.com/software/jisql/jisql.jsp'&gt;JiSQL&lt;/a&gt;, a Java based command-line tool for executing SQL queries that uses a JDBC driver. JiSQL enables us to specify the specific, Redshift-compatible JDBC driver to use to establish the connection. This meant we could upgrade our Ruby &lt;a href='https://github.com/snowplow/snowplow/wiki/setting-up-alternative-data-stores'&gt;StorageLoader&lt;/a&gt; to execute the relevant command-line syntax to initiate the regular data loads of Snowplow data from S3 into Redshift.&lt;/p&gt;

&lt;h2 id='using_jisql_to_bulk_load_data_from_s3_to_redshift_at_the_commandline_a_step_by_step_guide'&gt;Using JiSQL to bulk load data from S3 to Redshift at the command-line: a step by step guide&lt;/h2&gt;

&lt;h3 id='1_download_and_install_jisql'&gt;1. Download and install JiSQL&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Download JiSQL from &lt;a href='http://www.xigole.com/software/jisql/jisql.jsp'&gt;http://www.xigole.com/software/jisql/jisql.jsp&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;Unzip the contents of the compressed file to a suitable location&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id='2_download_the_redshiftcompatible_jdbc_driver'&gt;2. Download the Redshift-compatible JDBC driver&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Download the driver from &lt;a href='http://jdbc.postgresql.org/download/postgresql-8.4-703.jdbc4.jar'&gt;http://jdbc.postgresql.org/download/postgresql-8.4-703.jdbc4.jar&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;We saved the driver to the &lt;code&gt;lib&lt;/code&gt; folder in the jisql subdirectory&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id='3_identify_the_jdbc_url_for_your_redshift_cluster'&gt;3. Identify the JDBC URL for your Redshift cluster&lt;/h3&gt;

&lt;p&gt;In the AWS Console, go to the Redshift and select the cluster you want to load data into. A window will appear with details about the cluster, including the JDBC URL.&lt;/p&gt;

&lt;p&gt;&lt;img alt='screenshot' src='/static/img/blog/2013/02/redshift-jdbc-url.png' /&gt;&lt;/p&gt;

&lt;h3 id='4_initiate_your_bulk_load_of_data_at_the_command_line'&gt;4. Initiate your bulk load of data at the command line&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;At the command line, navigate to the Ji-SQL folder&lt;/li&gt;

&lt;li&gt;Execute the following command:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='bash'&gt;&lt;span class='nv'&gt;$ &lt;/span&gt;java -cp lib/jisql-2.0.11.jar:lib/jopt-simple-3.2.jar:lib/postgresql-8.4-703.jdbc4.jar com.xigole.util.sql.Jisql  -driver postgresql -cstring jdbc:postgresql://snowplow.cjbccnwghslt.us-east-1.redshift.amazonaws.com:5439/snplow -user &lt;span class='nv'&gt;$USERNAME&lt;/span&gt; -password &lt;span class='nv'&gt;$PASSWORD&lt;/span&gt; -c &lt;span class='se'&gt;\;&lt;/span&gt; -query &lt;span class='s2'&gt;&amp;quot;copy events from &amp;#39;s3://$MY_BUCKET/PATH/TO/FILES/FOR/UPLOAD credentials &amp;#39;aws_access_key_id=$ACCESS-KEY;aws_secret_access_key=$SECRET-ACCESS-KEY&amp;#39; delimiter &amp;#39;\t&amp;#39;;&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Some notes about the above query:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;You will need to replace e.g. &lt;code&gt;$USERNAME&lt;/code&gt; with your Redshift cluster username, &lt;code&gt;$PASSWORD&lt;/code&gt; with your Redshift password etc.&lt;/li&gt;

&lt;li&gt;You will need to replace the jdbc url in the example &lt;code&gt;jdbc:postgresql://snowplow.cjbccnwghslt.us-east-1.redshift.amazonaws.com:5439/snplow&lt;/code&gt; with the value you fetched from the AWS console in step 2&lt;/li&gt;

&lt;li&gt;The &lt;code&gt;-c \;&lt;/code&gt; flag sets &lt;code&gt;;&lt;/code&gt; as the query delimiter. If this is not specified, the query delimiter is taken to be &lt;code&gt;go&lt;/code&gt; by default. As the query specified is terminated by a &lt;code&gt;;&lt;/code&gt; rather than a &lt;code&gt;go&lt;/code&gt;, leaving out the &lt;code&gt;-c \;&lt;/code&gt; flag would cause the program to hang, as it waits for the terminating characters before executing the query&lt;/li&gt;

&lt;li&gt;If you want to experiment with the tool, you can leave off the &lt;code&gt;-query&lt;/code&gt; parameter, in which case you&amp;#8217;ll invoke an interactive command-line session&lt;/li&gt;

&lt;li&gt;If the query is successful, it will return &lt;code&gt;0 rows affected.&lt;/code&gt; at the command line. This is a bit misleading: if you then look in Redshift you&amp;#8217;ll see the new rows have loaded.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A note about bulk loading data from S3 into Redshift:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Amazon will only let you use the above syntax to load data from S3 into Redshift if the S3 bucket and the Redshift cluster are located in the &lt;strong&gt;same&lt;/strong&gt; region. If they are not (and Redshift is not available in all regions, at the time of writing), you will need to copy your S3 data into a new bucket in the same region as your Redshift cluster, prior to running the bulk upload.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Happy bulk loading from the command line!&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/02/18/ideas-coming-out-of-februarys-measurecamp</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/02/18/ideas-coming-out-of-februarys-measurecamp"/>
    <title>Reflections on Saturday's Measurecamp</title>
    <updated>2013-02-18T00:00:00+00:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;On Satuday both Alex and I were lucky enough to attend London&amp;#8217;s second &lt;a href='http://www.measurecamp.org/'&gt;Measurecamp&lt;/a&gt;, an unconference dedicated to digital analytics. The venue was packed with smart people sharing some really interesting ideas - we can&amp;#8217;t do justice to all those ideas here, so I&amp;#8217;ve just outlined my favourite two from the day:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href='/blog/2013/02/18/ideas-coming-out-of-februarys-measurecamp#keywords'&gt;Using keywords to segment audience by product and interest match&lt;/a&gt;, courtesy of &lt;a href='https://twitter.com/carmenmardiros'&gt;Carmen Mardiros&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/02/18/ideas-coming-out-of-februarys-measurecamp#server-side-datalayer'&gt;Transferring commercially sensitive data into your web analytics platform via a server-side dataLayer&lt;/a&gt;, courtesy of &lt;a href='https://twitter.com/TechPad'&gt;Matt Clarke&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I&amp;#8217;ve also post the slides I&amp;#8217;d put together on &lt;a href='/blog/2013/02/18/ideas-coming-out-of-februarys-measurecamp#clv'&gt;customer lifetime value&lt;/a&gt; for the event: I didn&amp;#8217;t end up sharing these on the day, because the room where the session took place didn&amp;#8217;t have a projector. That was just as well, as I think we had a much more interesting conversation about customer lifetime value as a result.&lt;/p&gt;
&lt;h2&gt;&lt;a name='keywords'&gt;1. Using keywords to segment audience by product and interest match&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;In this rather excellent presentation, &lt;a href='https://twitter.com/carmenmardiros'&gt;Carmen&lt;/a&gt; showed how you can use keywords users enter (either in searches directing them to your website, or on internal searches) to classify audience in especially meaningful buckets e.g.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Are they already a customer?&lt;/li&gt;

&lt;li&gt;Are they brand aware?&lt;/li&gt;

&lt;li&gt;Are they looking to purchase vs looking for support?&lt;/li&gt;

&lt;li&gt;Are they interested broadly or narrowly in your area?&lt;/li&gt;
&lt;/ul&gt;
&lt;!--more--&gt;
&lt;p&gt;And then test whether those audience behaved in significantly different ways to validate your segmentation. (For example, comparing bounce rates or conversion rates between segments.) Once validated, the segmentation enables you to apply segment-specific KPI: for example, it is meaningless analysing purchase conversion rates for users who have already bought, and are returning to the site for support-related queries.&lt;/p&gt;
&lt;iframe frameborder='0' height='356' marginheight='0' marginwidth='0' scrolling='no' src='http://www.slideshare.net/slideshow/embed_code/16581811' style='border:1px solid #CCC;border-width:1px 1px 0;margin-bottom:5px' width='427'&gt; &lt;/iframe&gt;&lt;div style='margin-bottom:5px'&gt; &lt;strong&gt; &lt;a href='http://www.slideshare.net/carmenmardiros/getting-to-the-people-behind-the-keywords-16581811' target='_blank' title='Getting to the People Behind The Keywords'&gt;Getting to the People Behind The Keywords&lt;/a&gt; &lt;/strong&gt; from &lt;strong&gt;&lt;a href='http://www.slideshare.net/carmenmardiros' target='_blank'&gt;Carmen Mardiros&lt;/a&gt;&lt;/strong&gt; &lt;/div&gt;
&lt;p&gt;An especially exciting prospect going forwards is to use machine learning to extend the segmentation beyond the subset of users for whom we have keyword data: so we can classify users who have not entered keywords, but appear to behave in a similar way to those who have, into the same buckets. This would be especially powerful with Snowplow data, as we have a user&amp;#8217;s complete click stream to work with when identifying users who &amp;#8220;look-like&amp;#8221; those we have classified on the basis of keywords alone. I hope to explore this in the near future, and blog about it here.&lt;/p&gt;
&lt;h2&gt;&lt;a name='server-side-datalayer'&gt;Transferring commercially-sensitive data into your web analytics platform via a server-side dataLayer&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;In another excellent presentation, &lt;a href='https://twitter.com/TechPad'&gt;Matt Clarke&lt;/a&gt; talked through his experience implementing &lt;a href='http://support.google.com/analytics/bin/answer.py?hl=en&amp;amp;answer=2790010&amp;amp;topic=2790009&amp;amp;ctx=topic'&gt;Universal Analytics&lt;/a&gt; at an online retailer. The key driver for Matt in choosing to implement Universal Analytics was a desire to deliver more commercially meaningful from Google Analytics, including:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Profitability of individual marketing campaigns (e.g. AdWords)&lt;/li&gt;

&lt;li&gt;Conversion rates by product and by brand&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In order to report on the above, it is necessary to capture margin data with every sale (in the case of 1) and record views, add to baskets and transactions by product and brand (in the case of 2). In both instances, Matt needed to pass more data into his web analytics platform than Google Analytics has traditionally allowed, which made Google&amp;#8217;s new Universal Analytics an attractive alternative.&lt;/p&gt;

&lt;p&gt;Passing this commercially sensitive data into Univesal Analytics is not trivial, however. Pushing it through the client-side dataLayer would make it available to any competitor. So instead, Matt passed the data server side into Universal Analytics. Universal Analytics does not have a PHP tracker, but Matt was able to effectively build his own using Google&amp;#8217;s &lt;a href='https://developers.google.com/analytics/devguides/collection/protocol/v1/'&gt;Measurement Protocol&lt;/a&gt;. By syncronising the client ID sent by the Javascript tracker with his own server-side tracker, Matt enables Universal Analtyics to stitch together the data generated client and server side. You can see more details in Matt&amp;#8217;s presentation below:&lt;/p&gt;
&lt;iframe frameborder='0' height='356' marginheight='0' marginwidth='0' scrolling='no' src='http://www.slideshare.net/slideshow/embed_code/16578670' style='border:1px solid #CCC;border-width:1px 1px 0;margin-bottom:5px' width='427'&gt; &lt;/iframe&gt;&lt;div style='margin-bottom:5px'&gt; &lt;strong&gt; &lt;a href='http://www.slideshare.net/MattClarke4/measurecamp-improving-e-commerce-tracking-with-universal-analytics' target='_blank' title='Measurecamp - Improving e commerce tracking with universal analytics'&gt;Measurecamp - Improving e commerce tracking with universal analytics&lt;/a&gt; &lt;/strong&gt; from &lt;strong&gt;&lt;a href='http://www.slideshare.net/MattClarke4' target='_blank'&gt;Matt Clarke&lt;/a&gt;&lt;/strong&gt; &lt;/div&gt;
&lt;p&gt;Matt&amp;#8217;s presentation is a must-read for anyone who wants to push web analytics tools into a more powerful business analytics tool. For us at Snowplow, it really highlights the need to enable server side tracking alongside client-side tracking: previously it hadn&amp;#8217;t occurred to me that a Snowplow user might want to use both approaches together, and replicate the dataLayer approach that has become best practice client-side on the server-side.&lt;/p&gt;
&lt;h2&gt;&lt;a name='clv'&gt;Customer lifetime value&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;These are the slides I prepared for my session on customer lifetime value:&lt;/p&gt;
&lt;iframe frameborder='0' height='356' marginheight='0' marginwidth='0' scrolling='no' src='http://www.slideshare.net/slideshow/embed_code/16598692' style='border:1px solid #CCC;border-width:1px 1px 0;margin-bottom:5px' width='427'&gt; &lt;/iframe&gt;&lt;div style='margin-bottom:5px'&gt; &lt;strong&gt; &lt;a href='http://www.slideshare.net/yalisassoon/customer-lifetime-value-16598692' target='_blank' title='Customer lifetime value'&gt;Customer lifetime value&lt;/a&gt; &lt;/strong&gt; from &lt;strong&gt;&lt;a href='http://www.slideshare.net/yalisassoon' target='_blank'&gt;yalisassoon&lt;/a&gt;&lt;/strong&gt; &lt;/div&gt;
&lt;p&gt;We had a very interesting discussion about the challenges both of tracking user behaviour across user lifetime, and started to explore approaches to developing predictive models for customer lifetime value. In 30 minutes we didn&amp;#8217;t get the chance to develop these very far - I hope we find another forum (maybe G+?) to continue the conversation. Many thanks to all those who attended: I learnt a lot from you.&lt;/p&gt;

&lt;h2 id='thank_you_measurecamp'&gt;Thank you Measurecamp&lt;/h2&gt;

&lt;p&gt;Big thanks to the &lt;a href='http://www.measurecamp.org/attendees/'&gt;Measurecamp Team&lt;/a&gt;. It was a brilliant event, and we&amp;#8217;re looking forward to the next one in 6 months time!&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/02/15/snowplow-0.7.3-released</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/02/15/snowplow-0.7.3-released"/>
    <title>Snowplow 0.7.3 released, tracking additional data</title>
    <updated>2013-02-15T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We&amp;#8217;re excited to announce the release of Snowplow version &lt;strong&gt;0.7.3&lt;/strong&gt;. This release adds a set of &lt;strong&gt;16 all-new fields&lt;/strong&gt; to our event model:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A new Event Vendor field&lt;/li&gt;

&lt;li&gt;The Page URL split out into its component parts (scheme, host, port, path, querystring, fragment/anchor)&lt;/li&gt;

&lt;li&gt;The web page&amp;#8217;s character set&lt;/li&gt;

&lt;li&gt;The web page&amp;#8217;s width and height&lt;/li&gt;

&lt;li&gt;The browser&amp;#8217;s viewport (i.e. visible width and height)&lt;/li&gt;

&lt;li&gt;For page pings, we are now tracking the user&amp;#8217;s scrolling during the last ping period (four fields)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These fields should make a new set of analyses on Snowplow data, including analysing how deeply users engage with different web pages (e.g. what percentage of a web page have they viewed, and how fast). In addition, it should make some analyses easier, e.g. aggregating (and comparing) metrics by page by page and domain.&lt;/p&gt;

&lt;p&gt;In addition, the new release includes some minor bug fixes. In this post we will cover:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href='/blog/2013/02/15/snowplow-0.7.3-released#new-fields'&gt;The new fields&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/02/15/snowplow-0.7.3-released#bug-fixes'&gt;Bug fixes&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/02/15/snowplow-0.7.3-released#breaking-changes'&gt;Breaking changes&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/02/15/snowplow-0.7.3-released#upgrade'&gt;Upgrading&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a name='new-fields'&gt;1. New fields&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;We are hugely excited to be including 16 new fields in this release - we believe that these fields should unlock a whole host of new analyses on Snowplow data.&lt;/p&gt;

&lt;p&gt;For completeness, we list out all of the new fields below. Note that all of the new fields are available in both the S3 (aka Hive) and Infobright (aka non-Hive) storage outputs:&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Field&lt;/th&gt;&lt;th&gt;Datatype&lt;/th&gt;&lt;th&gt;Description&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;event_vendor&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;string&lt;/td&gt;&lt;td style='text-align: left;'&gt;Which company or org. defined this event type&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;page_urlscheme&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;string&lt;/td&gt;&lt;td style='text-align: left;'&gt;Scheme aka protocol, e.g. &amp;#8220;https&amp;#8221;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;page_urlhost&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;string&lt;/td&gt;&lt;td style='text-align: left;'&gt;Host aka domain, e.g. &amp;#8220;www.snowplowanalytics.com&amp;#8221;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;page_urlport&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;int&lt;/td&gt;&lt;td style='text-align: left;'&gt;Port if specified, 80 if not&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;page_urlpath&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;string&lt;/td&gt;&lt;td style='text-align: left;'&gt;Path to page, e.g. &amp;#8220;/product/index.html&amp;#8221;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;page_urlquery&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;string&lt;/td&gt;&lt;td style='text-align: left;'&gt;Querystring, e.g. &amp;#8220;id=GTM-DLRG&amp;#8221;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;page_urlfragment&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;string&lt;/td&gt;&lt;td style='text-align: left;'&gt;Fragment aka anchor, e.g. &amp;#8220;4-conclusion&amp;#8221;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;br_viewwidth&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;integer&lt;/td&gt;&lt;td style='text-align: left;'&gt;The width of the browser&amp;#8217;s viewport in pixels&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;br_viewheight&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;integer&lt;/td&gt;&lt;td style='text-align: left;'&gt;The height of the browser&amp;#8217;s viewport in pixels&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;doc_charset&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;string&lt;/td&gt;&lt;td style='text-align: left;'&gt;The page&amp;#8217;s character encoding, e.g. &amp;#8220;UTF-8&amp;#8221;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;doc_width&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;integer&lt;/td&gt;&lt;td style='text-align: left;'&gt;The total width of the page (incl. non-viewed area)&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;doc_height&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;integer&lt;/td&gt;&lt;td style='text-align: left;'&gt;The total height of the page (incl. non-viewed area)&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;pp_xoffset_min&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;integer&lt;/td&gt;&lt;td style='text-align: left;'&gt;Minimum page x offset seen in the last ping period&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;pp_xoffset_max&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;integer&lt;/td&gt;&lt;td style='text-align: left;'&gt;Maximum page x offset seen in the last ping period&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;pp_yoffset_min&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;integer&lt;/td&gt;&lt;td style='text-align: left;'&gt;Minimum page y offset seen in the last ping period&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;pp_yoffset_max&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;integer&lt;/td&gt;&lt;td style='text-align: left;'&gt;Maximum page y offset seen in the last ping period&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;Don&amp;#8217;t worry if some of these new fields don&amp;#8217;t make immediate sense based on the descriptions above - we will take a look at each of these fields in the sub-sections below:&lt;/p&gt;
&lt;!--more--&gt;
&lt;h3 id='11_event_vendor'&gt;1.1 Event vendor&lt;/h3&gt;

&lt;p&gt;As we have &lt;a href='/blog/2013/02/04/help-us-build-out-the-snowplow-event-model/'&gt;previously blogged&lt;/a&gt;, we are in the process of developing the Snowplow event model: the list of first-class events for which we&amp;#8217;ve defined a structured data model. As we stressed in the &lt;a href='/blog/2013/02/04/help-us-build-out-the-snowplow-event-model/'&gt;blog post&lt;/a&gt;, we well understand that different models will be appropriate for different websites and applications, and that model we develop will not be ideal for everyone. In the future, we plan to enable different companies to develop their own first class data model within Snowplow. As a first step in this direction, we have added the &lt;strong&gt;Event vendor&lt;/strong&gt; field to the Snowplow data model: when a company develops its own event data model, it will be identifiable to that vendor using this field. This will open up the possibility of:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Ingesting proprietary events from third-party systems (e.g. &lt;code&gt;event_vendor&lt;/code&gt;=&amp;#8221;com.sendgrid&amp;#8221; or &amp;#8220;com.appnexus&amp;#8221;)&lt;/li&gt;

&lt;li&gt;Ingesting clickstream events from other analytics services (e.g. &lt;code&gt;event_vendor&lt;/code&gt;=&amp;#8221;com.adobe&amp;#8221; or &amp;#8220;com.mixpanel&amp;#8221;)&lt;/li&gt;

&lt;li&gt;Tracking custom events defined by a specific Snowplow user (e.g. &lt;code&gt;event_vendor&lt;/code&gt;=&amp;#8221;au.com.asnowplowuser&amp;#8221;)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;At the moment, however, all events will have an &lt;code&gt;event_vendor&lt;/code&gt; field that will be &amp;#8220;com.snowplowanalytics&amp;#8221; (using the Java package-style naming convention).&lt;/p&gt;

&lt;h3 id='12_page_url_components'&gt;1.2 Page URL components&lt;/h3&gt;

&lt;p&gt;We have split the &lt;code&gt;page_url&lt;/code&gt; into its six component parts (the unprocessed &lt;code&gt;page_url&lt;/code&gt; field is left unchanged). Having these fields broken out should make it much easier to do page URL-based analyses, such as aggregating data for specific &lt;code&gt;page_url&lt;/code&gt;s (ignoring query strings) or investigating HTTPS traffic.&lt;/p&gt;

&lt;h3 id='13_viewport_fields'&gt;1.3 Viewport fields&lt;/h3&gt;

&lt;p&gt;Each event now tracks the current viewport of the browser - in other words, the viewable area (width x height) current available within the browser.&lt;/p&gt;

&lt;p&gt;This will enable analysts to distinguish browsing behaviour based on viewport size, and see if there are specific events on a customer journey that trigger a user resizing his / her browser. (Which is a useful user-experience indicator.)&lt;/p&gt;

&lt;h3 id='14_document_width_and_height'&gt;1.4 Document width and height&lt;/h3&gt;

&lt;p&gt;We are now tracking the complete width and height of the current document (aka web page) on each event. This tells you the total width and height of the current page, as perceived by the browser. This measures the whole document - i.e. including the non-viewable part of the document.&lt;/p&gt;

&lt;p&gt;This can be used in conjunction with the new viewport fields (above) and page ping offsets (below) to analyse what fraction of a document a user has engaged with, and over what time period.&lt;/p&gt;

&lt;h3 id='15_page_ping_offsets'&gt;1.5 Page ping offsets&lt;/h3&gt;

&lt;p&gt;These four new offset fields are perhaps the most complex new additions. First of all: these fields are only set if you &lt;code&gt;enableActivityTracking()&lt;/code&gt; on your site. In a nutshell, activity tracking:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Silently checks for user activity (mouse movements, scrolling, key presses etc) on a page for a specified time period (e.g. 10 seconds)&lt;/li&gt;

&lt;li&gt;If any user activity was detected in those 10 seconds, the tracker sends a &amp;#8220;page ping&amp;#8221; back to Snowplow. (No user activity, no page ping)&lt;/li&gt;

&lt;li&gt;This is then repeated for each new time period, until the user navigates away from the page&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In this release we are sending four new offset fields along with each page ping event. These offsets track the &lt;strong&gt;minimum&lt;/strong&gt; and &lt;strong&gt;maximum&lt;/strong&gt; horizontal and vertical page offsets scrolled to by the user in the last page ping period. In other words: these four fields tell you how far left/right and up/down the user scrolled during the last ping period.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Simply put: these new offset fields are designed to provide a clear view of how your users scroll around your webpages over time.&lt;/strong&gt; (Especially when combined with the viewport and document width and height fields also listed above.)&lt;/p&gt;

&lt;p&gt;Huge thanks to &lt;a href='https://github.com/kingo55'&gt;Rob Kingston&lt;/a&gt; for providing the original idea and implementation around page ping offsets, and helping us to test our implementation!&lt;/p&gt;

&lt;h3 id='16_document_characterset'&gt;1.6 Document characterset&lt;/h3&gt;

&lt;p&gt;Each event now tracks the document&amp;#8217;s charset where available (not all browsers set this).&lt;/p&gt;
&lt;a name='bug-fixes'&gt;&lt;h2&gt;2. Bug fixes&lt;/h2&gt;&lt;/a&gt;
&lt;p&gt;As well as the new fields introduced above, this release also includes a small set of bug fixes in the JavaScript tracker which are worth noting:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Our &lt;code&gt;logImpression()&lt;/code&gt; method was not working (it was using the wrong argument names) - this has now been fixed.&lt;/li&gt;

&lt;li&gt;The activity tracking (page ping) behaviour was too fragile: if a single monitoring period elapsed with no activity, then all future monitoring would be cancelled. This could easily lead to on-page activity not being recorded. This has now been fixed&lt;/li&gt;
&lt;/ol&gt;
&lt;a name='breaking-changes'&gt;&lt;h2&gt;3. Breaking changes and deprecations&lt;/h2&gt;&lt;/a&gt;
&lt;p&gt;The following table tracks the breaking changes and deprecations in this version. &lt;strong&gt;When upgrading to the latest version of the JavaScript tracker (0.10.0), please update your JavaScript tags as per the instructions below to avoid problems:&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Type of change&lt;/th&gt;&lt;th&gt;Component&lt;/th&gt;&lt;th&gt;Change&lt;/th&gt;&lt;th&gt;Comment&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Breaking change&lt;/td&gt;&lt;td style='text-align: left;'&gt;JavaScript tracker&lt;/td&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;setAccount()&lt;/code&gt; removed&lt;/td&gt;&lt;td style='text-align: left;'&gt;Use &lt;code&gt;setCollectorCf()&lt;/code&gt; instead&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Breaking change&lt;/td&gt;&lt;td style='text-align: left;'&gt;JavaScript tracker&lt;/td&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;setTracker()&lt;/code&gt; removed&lt;/td&gt;&lt;td style='text-align: left;'&gt;Use &lt;code&gt;getTrackerCf()&lt;/code&gt; instead&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Breaking change&lt;/td&gt;&lt;td style='text-align: left;'&gt;JavaScript tracker&lt;/td&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;setHeartBeatTimer()&lt;/code&gt; removed&lt;/td&gt;&lt;td style='text-align: left;'&gt;Use &lt;code&gt;enableActivityTracking()&lt;/code&gt; instead&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Deprecation&lt;/td&gt;&lt;td style='text-align: left;'&gt;JavaScript tracker&lt;/td&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;trackEvent()&lt;/code&gt; deprecated&lt;/td&gt;&lt;td style='text-align: left;'&gt;Use &lt;code&gt;trackStructEvent()&lt;/code&gt; instead&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Data change&lt;/td&gt;&lt;td style='text-align: left;'&gt;S3 &amp;amp; Infobright storage&lt;/td&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;event&lt;/code&gt;=&amp;#8221;custom&amp;#8221; changed&lt;/td&gt;&lt;td style='text-align: left;'&gt;Changed to &lt;code&gt;event&lt;/code&gt;=&amp;#8221;struct&amp;#8221;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;The first three changes are simply cleanup: we are removing tracker methods which we previously deprecated some time ago.&lt;/p&gt;

&lt;p&gt;The last two changes are us starting to re-structure our event tracking - we are making space in our event model to support unstructured events, which will be coming soon. Please check out our previous blog post, &lt;a href='/blog/2013/02/04/help-us-build-out-the-snowplow-event-model/'&gt;Help us build out the Snowplow Event Model&lt;/a&gt; for more background on this.&lt;/p&gt;
&lt;a name='upgrade'&gt;&lt;h2&gt;4. Upgrading&lt;/h2&gt;&lt;/a&gt;
&lt;p&gt;Upgrading is a three-step process:&lt;/p&gt;

&lt;h3 id='41_javascript_tracker'&gt;4.1 JavaScript tracker&lt;/h3&gt;

&lt;p&gt;Please update your website(s) to use the latest version of the JavaScript tracker, which is version 0.10.0. As always, the updated minified tracker is available here:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http(s)://d1fc8wv8zag5ca.cloudfront.net/0.10.0/sp.js&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Don&amp;#8217;t forget to update your Snowplow tags as per the updates in &lt;a href='#breaking-changes'&gt;breaking changes&lt;/a&gt; and deprecations above.&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id='42_etl'&gt;4.2 ETL&lt;/h3&gt;

&lt;p&gt;If you are using EmrEtlRunner, you need to update your configuration file, &lt;code&gt;config.yml&lt;/code&gt;, to use the latest versions of the Hive serde and HiveQL scripts:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:snowplow:
  :serde_version: 0.5.4
  :hive_hiveql_version: 0.5.5
  :non_hive_hiveql_version: 0.0.6&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id='43_infobright'&gt;4.3 Infobright&lt;/h3&gt;

&lt;p&gt;If you are using Infobright Community Edition for analysis, you will need to update your table definition. To make this easier for you, we have created two scripts:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;4-storage/infobright-storage/migrate_004_to_006.sh
4-storage/infobright-storage/migrate_005_to_006.sh&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Choose the appropriate script depending on whether your current events table is &lt;code&gt;events_004&lt;/code&gt; or &lt;code&gt;events_005&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Running this script will create a new table, &lt;code&gt;events_006&lt;/code&gt; (version 0.0.6 of the Infobright table definition) in your &lt;code&gt;snowplow&lt;/code&gt; database, copying across all your data from your existing &lt;code&gt;events&lt;/code&gt; table, which will not be modified in any way.&lt;/p&gt;

&lt;p&gt;Once you have run this, don&amp;#8217;t forget to update your StorageLoader&amp;#8217;s &lt;code&gt;config.yml&lt;/code&gt; to load into the new &lt;code&gt;events_006&lt;/code&gt; table, not your old &lt;code&gt;events&lt;/code&gt; table:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:storage:
  :type: infobright
  :database: snowplow
  :table:    events_006 # NOT &amp;quot;events_004&amp;quot; or &amp;quot;events_005&amp;quot; any more&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Done!&lt;/p&gt;

&lt;h2 id='5_getting_help'&gt;5. Getting help&lt;/h2&gt;

&lt;p&gt;As always, if you do run into any issues or don&amp;#8217;t understand any of the above changes, please &lt;a href='https://github.com/snowplow/snowplow/issues'&gt;raise an issue&lt;/a&gt; or get in touch with us via &lt;a href='https://github.com/snowplow/snowplow/wiki/Talk-to-us'&gt;the usual channels&lt;/a&gt;.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/02/08/writing-hive-udfs-and-serdes</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/02/08/writing-hive-udfs-and-serdes"/>
    <title>Writing Hive UDFs - a tutorial</title>
    <updated>2013-02-08T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;&lt;em&gt;Snowplow&amp;#8217;s own &lt;a href='https://github.com/alexanderdean'&gt;Alexander Dean&lt;/a&gt; was recently asked to write an article for the &lt;a href='http://sdjournal.org/apache-hadoop-ecosystem/?a_aid=bartoszmiedeksza&amp;amp;a_bid=45f0d439'&gt;Software Developer&amp;#8217;s Journal edition on Hadoop&lt;/a&gt; The kind folks at the Software Developer&amp;#8217;s Journal have allowed us to reprint his article in full below.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Alex started writing Hive UDFs as part of the process to write the &lt;a href='https://github.com/snowplow/snowplow/tree/master/3-etl/hive-etl/snowplow-log-deserializers'&gt;Snowplow log deserializer&lt;/a&gt; - the custom SerDe used to parse Snowplow logs generated by the Cloudfront and Clojure collectors so they can be processed in the Snowplow ETL step.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id='article_synopsis'&gt;Article Synopsis&lt;/h2&gt;

&lt;p&gt;In this article you will learn how to write a user-defined function (&amp;#8220;UDF&amp;#8221;) to work with the Apache Hive platform. We will start gently with an introduction to Hive, then move on to developing the UDF and writing tests for it. We will write our UDF in Java, but use Scala&amp;#8217;s SBT as our build tool and write our tests in Scala with Specs2.&lt;/p&gt;

&lt;p&gt;In order to get the most out of this article, you should be comfortable programming in Java. You do not need to have any experience with Apache Hive, HiveQL (the Hive query language) or indeed Hive UDFs - I will introduce all of these concepts from first principles. Experience with Scala is advantageous, but not necessary.&lt;/p&gt;
&lt;!--more--&gt;
&lt;h2 id='introduction'&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Before we start: my name is Alex Dean, and I am the co-founder of Snowplow (http://snowplowanalytics.com/), an open-source web analytics platform built on top of Apache Hadoop and Apache Hive. My experience writing Java code to extend the Hive platform comes from Snowplow, where we built a core piece of our launch platform using a Hive deserializer (https://github.com/snowplow/snowplow/tree/master/3-etl/hive-etl/snowplow-log-deserializers).&lt;/p&gt;

&lt;p&gt;So, what is Apache Hive, and what would you want a Hive UDF for? Hive is a data warehouse system built on top of Hadoop for ad-hoc queries and processing of large datasets. Now an Apache Software Foundation project, Hive was originally developed at Facebook, where analysts and data scientists wanted a SQL-like abstraction over traditional Hadoop MapReduce. As such, the key distinguishing feature of Hive is the SQL-like query language HiveQL. An example HiveQL query might look like this:&lt;/p&gt;

&lt;p&gt;Listing 1: An example HiveQL query&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='mysql'&gt;&lt;span class='k'&gt;SELECT&lt;/span&gt; 
&lt;span class='n'&gt;dt&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='nf'&gt;COUNT&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='k'&gt;DISTINCT&lt;/span&gt; &lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;))&lt;/span&gt;
&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='n'&gt;events&lt;/span&gt;
&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='n'&gt;dt&lt;/span&gt; &lt;span class='p'&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This is actually a standard Snowplow query to calculate the number of unique visitors to a website by day. So what happens when an analyst runs this query in Hive? Simply this:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Hive converts the query into the simplest possible set of MapReduce jobs&lt;/li&gt;

&lt;li&gt;The MapReduce job or jobs is run on the Hadoop platform&lt;/li&gt;

&lt;li&gt;The generated result set is then returned to the user&amp;#8217;s console&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Certainly this is a powerful abstraction over MapReduce jobs, which can be tedious and difficult to write by hand. And HiveQL has a lot of power - there is very little in the ANSI SQL standard which is not available in HiveQL. Nonetheless, sometimes the Hive user will need more power, and for these occasions Hive has three main extension points:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;User-defined functions (&amp;#8220;UDFs&amp;#8221;), which provide a way of extending the functionality of Hive with a function (written in Java) that can be evaluated in HiveQL statements&lt;/li&gt;

&lt;li&gt;Custom serializers and/or deserializers (&amp;#8220;serdes&amp;#8221;), which provide a way of either deserializing a custom file format stored on HDFS to a POJO (plain old Java object), or serializing a POJO to a custom file format (or both)&lt;/li&gt;

&lt;li&gt;Custom mappers/reducers, which allow you to add custom map or reduce steps into your Hive query. These map/reduce steps can be written in any programming language - not just Java&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We will not consider serdes or custom mappers/reducers further in this article - we hope to write further articles on each of these in the future.&lt;/p&gt;

&lt;p&gt;Now that we understand why you might write a UDF for Hive, let&amp;#8217;s crack on and start writing one!&lt;/p&gt;

&lt;h2 id='setting_up_our_project'&gt;Setting up our project&lt;/h2&gt;

&lt;p&gt;We will be writing a relatively simple UDF - one which generates a converts a string in Hive to upper-case. Note that a version of this function is actually built into Hive as the UPPER function - for a full list of built-in UDFs in Hive, please see: https://cwiki.apache.org/Hive/languagemanual-udf.html&lt;/p&gt;

&lt;p&gt;As mentioned previously, we will write our UDF in Java - but we will wrap our Java core in a Scala project (with Scala tests), because at Snowplow we much prefer writing Scala to Java. We will use SBT, the Scala build tool, to configure our project - this is an alternative to Maven or similar; SBT handles mixed Java and Scala projects perfectly well.&lt;/p&gt;

&lt;p&gt;First, let&amp;#8217;s create a directory for our project, and add a file, project.sbt into the project root, which contains:&lt;/p&gt;

&lt;p&gt;Listing 2: Our project.sbt build file&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='scala'&gt;&lt;span class='n'&gt;name&lt;/span&gt; &lt;span class='o'&gt;:=&lt;/span&gt; &lt;span class='s'&gt;&amp;quot;hive-example-udf&amp;quot;&lt;/span&gt;
&lt;span class='n'&gt;version&lt;/span&gt; &lt;span class='o'&gt;:=&lt;/span&gt; &lt;span class='s'&gt;&amp;quot;0.0.1&amp;quot;&lt;/span&gt;
&lt;span class='n'&gt;organization&lt;/span&gt; &lt;span class='o'&gt;:=&lt;/span&gt; &lt;span class='s'&gt;&amp;quot;com.snowplowanalytics&amp;quot;&lt;/span&gt;
&lt;span class='n'&gt;scalaVersion&lt;/span&gt; &lt;span class='o'&gt;:=&lt;/span&gt; &lt;span class='s'&gt;&amp;quot;2.9.2&amp;quot;&lt;/span&gt;
&lt;span class='n'&gt;scalacOptions&lt;/span&gt; &lt;span class='o'&gt;++=&lt;/span&gt; &lt;span class='nc'&gt;Seq&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='s'&gt;&amp;quot;-unchecked&amp;quot;&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='s'&gt;&amp;quot;-deprecation&amp;quot;&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
&lt;span class='n'&gt;resolvers&lt;/span&gt; &lt;span class='o'&gt;+=&lt;/span&gt; &lt;span class='s'&gt;&amp;quot;CDH4&amp;quot;&lt;/span&gt; &lt;span class='n'&gt;at&lt;/span&gt; &lt;span class='s'&gt;&amp;quot;https://repository.cloudera.com/artifactory/cloudera-repos/&amp;quot;&lt;/span&gt;
&lt;span class='n'&gt;libraryDependencies&lt;/span&gt; &lt;span class='o'&gt;+=&lt;/span&gt; &lt;span class='s'&gt;&amp;quot;org.apache.hadoop&amp;quot;&lt;/span&gt; &lt;span class='o'&gt;%&lt;/span&gt;  &lt;span class='s'&gt;&amp;quot;hadoop-core&amp;quot;&lt;/span&gt;        &lt;span class='o'&gt;%&lt;/span&gt; &lt;span class='s'&gt;&amp;quot;0.20.2&amp;quot;&lt;/span&gt;      &lt;span class='o'&gt;%&lt;/span&gt; &lt;span class='s'&gt;&amp;quot;provided&amp;quot;&lt;/span&gt;
&lt;span class='n'&gt;libraryDependencies&lt;/span&gt; &lt;span class='o'&gt;+=&lt;/span&gt; &lt;span class='s'&gt;&amp;quot;org.apache.hive&amp;quot;&lt;/span&gt;   &lt;span class='o'&gt;%&lt;/span&gt;  &lt;span class='s'&gt;&amp;quot;hive-exec&amp;quot;&lt;/span&gt;          &lt;span class='o'&gt;%&lt;/span&gt; &lt;span class='s'&gt;&amp;quot;0.8.1&amp;quot;&lt;/span&gt;       &lt;span class='o'&gt;%&lt;/span&gt; &lt;span class='s'&gt;&amp;quot;provided&amp;quot;&lt;/span&gt;
&lt;span class='n'&gt;libraryDependencies&lt;/span&gt; &lt;span class='o'&gt;+=&lt;/span&gt; &lt;span class='s'&gt;&amp;quot;org.specs2&amp;quot;&lt;/span&gt;        &lt;span class='o'&gt;%%&lt;/span&gt; &lt;span class='s'&gt;&amp;quot;specs2&amp;quot;&lt;/span&gt;             &lt;span class='o'&gt;%&lt;/span&gt; &lt;span class='s'&gt;&amp;quot;1.12.1&amp;quot;&lt;/span&gt;      &lt;span class='o'&gt;%&lt;/span&gt; &lt;span class='s'&gt;&amp;quot;test&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This is a simple project configuration which names and versions our project, and also adds Hadoop, Hive and Specs2 (our testing library) as dependencies. If you do not have SBT already installed, you can find instructions here http://www.scala-sbt.org/release/docs/Getting-Started/Setup.html&lt;/p&gt;

&lt;h2 id='writing_our_udf'&gt;Writing our UDF&lt;/h2&gt;

&lt;p&gt;Done? Onto the code. First let&amp;#8217;s create a folder for it:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='bash'&gt;&lt;span class='nv'&gt;$ &lt;/span&gt;mkdir -p src/main/java/com/snowplowanalytics/hive/udf
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now let&amp;#8217;s add a file into our udf folder called ToUpper.java, containing:&lt;/p&gt;

&lt;p&gt;Listing 3: Our ToUpper.java UDF definition&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='java'&gt;&lt;span class='kn'&gt;package&lt;/span&gt; &lt;span class='n'&gt;com&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='na'&gt;snowplowanalytics&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='na'&gt;hive&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='na'&gt;udf&lt;/span&gt;&lt;span class='o'&gt;;&lt;/span&gt;

&lt;span class='kn'&gt;import&lt;/span&gt; &lt;span class='nn'&gt;org.apache.hadoop.hive.ql.exec.UDF&lt;/span&gt;&lt;span class='o'&gt;;&lt;/span&gt;
&lt;span class='kn'&gt;import&lt;/span&gt; &lt;span class='nn'&gt;org.apache.hadoop.hive.ql.exec.Description&lt;/span&gt;&lt;span class='o'&gt;;&lt;/span&gt;
&lt;span class='kn'&gt;import&lt;/span&gt; &lt;span class='nn'&gt;org.apache.hadoop.io.Text&lt;/span&gt;&lt;span class='o'&gt;;&lt;/span&gt;

&lt;span class='nd'&gt;@Description&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;
	&lt;span class='n'&gt;name&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='s'&gt;&amp;quot;toupper&amp;quot;&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt;
	&lt;span class='n'&gt;value&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='s'&gt;&amp;quot;_FUNC_(str) - Converts a string to uppercase&amp;quot;&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt;
	&lt;span class='n'&gt;extended&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='s'&gt;&amp;quot;Example:\n&amp;quot;&lt;/span&gt; &lt;span class='o'&gt;+&lt;/span&gt;
	&lt;span class='s'&gt;&amp;quot;  &amp;gt; SELECT toupper(author_name) FROM authors a;\n&amp;quot;&lt;/span&gt; &lt;span class='o'&gt;+&lt;/span&gt;
	&lt;span class='s'&gt;&amp;quot;  STEPHEN KING&amp;quot;&lt;/span&gt;
	&lt;span class='o'&gt;)&lt;/span&gt;
&lt;span class='kd'&gt;public&lt;/span&gt; &lt;span class='kd'&gt;class&lt;/span&gt; &lt;span class='nc'&gt;ToUpper&lt;/span&gt; &lt;span class='kd'&gt;extends&lt;/span&gt; &lt;span class='n'&gt;UDF&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;

    &lt;span class='kd'&gt;public&lt;/span&gt; &lt;span class='n'&gt;Text&lt;/span&gt; &lt;span class='nf'&gt;evaluate&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;Text&lt;/span&gt; &lt;span class='n'&gt;s&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
		&lt;span class='n'&gt;Text&lt;/span&gt; &lt;span class='n'&gt;to_value&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='k'&gt;new&lt;/span&gt; &lt;span class='n'&gt;Text&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='s'&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class='o'&gt;);&lt;/span&gt;
		&lt;span class='k'&gt;if&lt;/span&gt; &lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;s&lt;/span&gt; &lt;span class='o'&gt;!=&lt;/span&gt; &lt;span class='kc'&gt;null&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
		    &lt;span class='k'&gt;try&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt; 
				&lt;span class='n'&gt;to_value&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='na'&gt;set&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;s&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='na'&gt;toString&lt;/span&gt;&lt;span class='o'&gt;().&lt;/span&gt;&lt;span class='na'&gt;toUpperCase&lt;/span&gt;&lt;span class='o'&gt;());&lt;/span&gt;
		    &lt;span class='o'&gt;}&lt;/span&gt; &lt;span class='k'&gt;catch&lt;/span&gt; &lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;Exception&lt;/span&gt; &lt;span class='n'&gt;e&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt; &lt;span class='c1'&gt;// Should never happen&lt;/span&gt;
				&lt;span class='n'&gt;to_value&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='k'&gt;new&lt;/span&gt; &lt;span class='n'&gt;Text&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;s&lt;/span&gt;&lt;span class='o'&gt;);&lt;/span&gt;
		    &lt;span class='o'&gt;}&lt;/span&gt;
		&lt;span class='o'&gt;}&lt;/span&gt;
		&lt;span class='k'&gt;return&lt;/span&gt; &lt;span class='n'&gt;to_value&lt;/span&gt;&lt;span class='o'&gt;;&lt;/span&gt;
    &lt;span class='o'&gt;}&lt;/span&gt;
&lt;span class='o'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This file defines our UDF, ToUpper. The package definition and imports should be self-explanatory; the &lt;code&gt;@Description&lt;/code&gt; annotation is a useful Hive-specific annotation to provide usage information for our UDF in the Hive console.&lt;/p&gt;

&lt;p&gt;All user-defined functions extend the Hive UDF class; a UDF sub-class must then implement one or more methods named &amp;#8220;evaluate&amp;#8221; which will be called by Hive. We implement an evaluate method which takes one Hadoop Text (which stores text using UTF8) and returns the same Hadoop Text, but now in upper-case. The only complexity is some exception handling, which we include for safety&amp;#8217;s sake.&lt;/p&gt;

&lt;p&gt;Now let&amp;#8217;s check that this compiles. In the root folder, run SBT like so:&lt;/p&gt;

&lt;p&gt;Listing 4: Compiling in SBT&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='bash'&gt;&lt;span class='nv'&gt;$ &lt;/span&gt;sbt
&amp;gt; compile
&lt;span class='o'&gt;[&lt;/span&gt;success&lt;span class='o'&gt;]&lt;/span&gt; Total &lt;span class='nb'&gt;time&lt;/span&gt;: 0 s, completed 28-Jan-2013 16:41:53
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id='testing_our_udf'&gt;Testing our UDF&lt;/h2&gt;

&lt;p&gt;Okay great, now time to write a test to make sure this is doing what we expect! First we create a folder for it:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='bash'&gt;&lt;span class='nv'&gt;$ &lt;/span&gt;mkdir -p src/test/scala/com/snowplowanalytics/hive/udf
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now let&amp;#8217;s add a file into our udf test folder called ToUpperTest.scala, containing:&lt;/p&gt;

&lt;p&gt;Listing 5: Our ToUpperTest.scala Specs2 test&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='java'&gt;&lt;span class='kn'&gt;package&lt;/span&gt; &lt;span class='n'&gt;com&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='na'&gt;snowplowanalytics&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='na'&gt;hive&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='na'&gt;udf&lt;/span&gt;

&lt;span class='kn'&gt;import&lt;/span&gt; &lt;span class='nn'&gt;org.apache.hadoop.io.Text&lt;/span&gt;

&lt;span class='kn'&gt;import&lt;/span&gt; &lt;span class='nn'&gt;org.specs2._&lt;/span&gt;

&lt;span class='kd'&gt;class&lt;/span&gt; &lt;span class='nc'&gt;ToUpperSpec&lt;/span&gt; &lt;span class='kd'&gt;extends&lt;/span&gt; &lt;span class='n'&gt;mutable&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='na'&gt;Specification&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
  &lt;span class='n'&gt;val&lt;/span&gt; &lt;span class='n'&gt;toUpper&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='k'&gt;new&lt;/span&gt; &lt;span class='n'&gt;ToUpper&lt;/span&gt;

  &lt;span class='s'&gt;&amp;quot;ToUpper#evaluate&amp;quot;&lt;/span&gt; &lt;span class='n'&gt;should&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
    &lt;span class='s'&gt;&amp;quot;return an empty string if passed a null value&amp;quot;&lt;/span&gt; &lt;span class='n'&gt;in&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
      &lt;span class='n'&gt;toUpper&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='na'&gt;evaluate&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='kc'&gt;null&lt;/span&gt;&lt;span class='o'&gt;).&lt;/span&gt;&lt;span class='na'&gt;toString&lt;/span&gt; &lt;span class='n'&gt;mustEqual&lt;/span&gt; &lt;span class='s'&gt;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class='o'&gt;}&lt;/span&gt;

    &lt;span class='s'&gt;&amp;quot;return a capitalised string if passed a mixed-case string&amp;quot;&lt;/span&gt; &lt;span class='n'&gt;in&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
      &lt;span class='n'&gt;toUpper&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='na'&gt;evaluate&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='k'&gt;new&lt;/span&gt; &lt;span class='n'&gt;Text&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='s'&gt;&amp;quot;Stephen King&amp;quot;&lt;/span&gt;&lt;span class='o'&gt;)).&lt;/span&gt;&lt;span class='na'&gt;toString&lt;/span&gt; &lt;span class='n'&gt;mustEqual&lt;/span&gt; &lt;span class='s'&gt;&amp;quot;STEPHEN KING&amp;quot;&lt;/span&gt;
    &lt;span class='o'&gt;}&lt;/span&gt;
  &lt;span class='o'&gt;}&lt;/span&gt;
&lt;span class='o'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This is a Specs2 unit test (http://etorreborre.github.com/specs2/), written in Scala, which checks that ToUpper is performing correctly: we test that an empty string is handled correctly, and then we test that a mixed-case string (&amp;#8220;Stephen King&amp;#8221;) is successfully converted to &amp;#8220;STEPHEN KING&amp;#8221;.&lt;/p&gt;

&lt;p&gt;So let&amp;#8217;s run this next from SBT:&lt;/p&gt;

&lt;p&gt;Listing 6: Testing in SBT&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='text'&gt;&amp;gt; test
[info] Compiling 1 Scala source to /home/alex/Development/Snowplow/hive-example-udf/target/scala-2.9.2/test-classes...
[info] ToUpperSpec
[info] 
[info] ToUpper#evaluate should
[info] + return an empty string if passed a null value
[info] + return a capitalised string if passed a mixed-case string
[info]  
[info]  
[info] Total for specification ToUpperSpec
[info] Finished in 742 ms
[info] 2 examples, 0 failure, 0 error
[info] 
[info] Passed: : Total 2, Failed 0, Errors 0, Passed 2, Skipped 0
[success] Total time: 8 s, completed 28-Jan-2013 17:11:45
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id='building_and_using_our_udf'&gt;Building and using our UDF&lt;/h2&gt;

&lt;p&gt;Our tests passed! Now we&amp;#8217;re ready to use our function &amp;#8220;in anger&amp;#8221; from Hive. First, still from SBT, let&amp;#8217;s build our jarfile:&lt;/p&gt;

&lt;p&gt;Listing 7: Packaging our jar&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='text'&gt;&amp;gt; package
[info] Packaging /home/alex/Development/Snowplow/hive-example-udf/target/scala-2.9.2/hive-example-udf_2.9.2-0.0.1.jar ...
[info] Done packaging.
[success] Total time: 1 s, completed 28-Jan-2013 17:21:02
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now take the jarfile (hive-example-udf_2.9.2-0.0.1.jar) and upload it to our Hive cluster - on Amazon&amp;#8217;s Elastic MapReduce, for example, you could upload it to S3.&lt;/p&gt;

&lt;p&gt;From your Hive console, you can now add our new UDF like so:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='mysql'&gt;&lt;span class='o'&gt;&amp;gt;&lt;/span&gt; &lt;span class='k'&gt;add&lt;/span&gt; &lt;span class='n'&gt;jar&lt;/span&gt; &lt;span class='o'&gt;/&lt;/span&gt;&lt;span class='n'&gt;path&lt;/span&gt;&lt;span class='o'&gt;/&lt;/span&gt;&lt;span class='k'&gt;to&lt;/span&gt;&lt;span class='o'&gt;/&lt;/span&gt;&lt;span class='n'&gt;HiveSwarm&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;jar&lt;/span&gt;&lt;span class='p'&gt;;&lt;/span&gt;
&lt;span class='o'&gt;&amp;gt;&lt;/span&gt; &lt;span class='k'&gt;create&lt;/span&gt; &lt;span class='n'&gt;temporary&lt;/span&gt; &lt;span class='n'&gt;function&lt;/span&gt; &lt;span class='n'&gt;to_upper&lt;/span&gt; &lt;span class='k'&gt;as&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;com.snowplowanalytics.hive.udf.ToUpper&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And then finally you can use our new UDF in your HiveQL queries, something like this:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='mysql'&gt;&lt;span class='o'&gt;&amp;gt;&lt;/span&gt; &lt;span class='k'&gt;SELECT&lt;/span&gt; &lt;span class='nf'&gt;toupper&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;author_name&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='n'&gt;authors&lt;/span&gt; &lt;span class='n'&gt;a&lt;/span&gt;&lt;span class='p'&gt;;&lt;/span&gt;
  &lt;span class='n'&gt;STEPHEN&lt;/span&gt; &lt;span class='n'&gt;KING&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;That completes our article. If you would like to download the example code above as a working project, you can find it on GitHub here: https://github.com/snowplow/hive-example-udf&lt;/p&gt;

&lt;p&gt;I hope to return with further articles about Hive and Hadoop in the future - potentially one on writing a custom serde - an area where we have a lot of experience at Snowplow Analytics.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/02/04/help-us-build-out-the-snowplow-event-model</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/02/04/help-us-build-out-the-snowplow-event-model"/>
    <title>Help us build out the Snowplow Event Model</title>
    <updated>2013-02-04T00:00:00+00:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;At its beating heart, Snowplow is a platform for capturing, storing and analysing event-data, with a real focus on web event data.&lt;/p&gt;

&lt;p&gt;Working out how best to structure the Snowplow event data is key to making Snowplow a success. One of the things that has surprised us, since we started working on Snowplow, is the extent to which our view of the best way to structure that data has changed over time.&lt;/p&gt;

&lt;p&gt;In this blog post, we outline our vision for the Snowplow event data model. We do so to elicit feedback and invite collaboration with the wider Snowplow community. Developing a data model that will work well for &lt;strong&gt;everyone&lt;/strong&gt; will only be possible with input from a broad set of people from a wide range of companies performing analytics on a wide range of businesses. We&amp;#8217;ve been fortunate to work with a wide range of businesses who&amp;#8217;ve helped shape our thinking so far. We hope to talk to many more to help us on our journey to develop the Snowplow Event Model.&lt;/p&gt;
&lt;!--more--&gt;
&lt;h2 id='from_page_views_to_events__a_quick_look_back_at_the_development_of_the_web'&gt;From page views to events - a quick look back at the development of the web&lt;/h2&gt;

&lt;p&gt;Before we get started thinking about the Snowplow event data model, it is helpful to put it in the context of how other web analytics tools model web data, and that in the context of the development of the web.&lt;/p&gt;

&lt;p&gt;When the web started out, it was a network of largely static documents that were hyperlinked to one-another. Over time the documents started updating more rapidly, so looked less static. In addition, the development of Flash and then Javascript meant that web pages became more interactive: websites started to look more like interactive applications and less like documents.&lt;/p&gt;

&lt;p&gt;The web analytics industry was born in the 1990s, when the web was still a network of hyperlinked documents. The primary &amp;#8220;event&amp;#8221; that web analytics programmes were interested a &lt;em&gt;hit&lt;/em&gt;, which referred to a request being made to a web server. (As such, it was more of an &amp;#8220;event&amp;#8221; for the sysadmin than the user navigating the website.) Over time this evolved into the &lt;em&gt;page view&lt;/em&gt; - as loading a web pages with multiple elements (e.g. different images) would result in multiple hits. Web analytics packages excelled at tracking &lt;em&gt;page views&lt;/em&gt;. As online retail took off, they extended to capturing transactions, and most recently, social events (e.g. &lt;em&gt;liking a product&lt;/em&gt;).&lt;/p&gt;

&lt;p&gt;As a user, there are millions of things you can do on the web: from checking your bank balance, to messaging a friend, to researching a holiday, to sharing photos of your children. Web analytics packages, however, still only recognise a very small subset of events. To go beyond tracking &lt;em&gt;page views&lt;/em&gt; and &lt;em&gt;transactions&lt;/em&gt;, web analysts have to use custom event tracking (in Google Analytics), or an unholy combination of eVars and sProps (in SiteCatalyst).&lt;/p&gt;

&lt;p&gt;We want to do better with Snowplow. We want to identfy a broad set of events that are useful to a wide range of web analysts across different companies and products, and recognise these in the Snowplow Event Model as first class citizens. We want to design the data structures for these events so that there are named fields to capture the dimensions and metrics for those events that meet the needs of 80% of Snowplow users, and a set of configurable dimensions and metrics to meet the needs of the remaining 20%. Similarly, we recognise that those &amp;#8220;1st class&amp;#8221; events might only meet the needs of 80% of the events that people need to track online, and so we will still need generic &amp;#8220;custom events&amp;#8221; for users to configure to track the rest.&lt;/p&gt;

&lt;p&gt;By writing this blog post, we hope to entice readers like you to contribute to that Event Model.&lt;/p&gt;

&lt;h2 id='why_bother_with_an_event_model_at_all'&gt;Why bother with an Event Model at all?&lt;/h2&gt;

&lt;p&gt;Some of the people we have talked to about the Snowplow Event Model have not been convinced of the need to develop one. These people, who are typically very familiar with NoSQL datastores like Mongo, Riak and Cassandra, sometimes argue that we can do away with a formal model all together and simply stuff a JSON with whatever dimensions and metrics suit, when we come to store the data associated with a specific event.&lt;/p&gt;

&lt;p&gt;NoSQL data stores are attractive because they enable users to store data without worrying about a schema. However, that does not mean we can forget about schemas all together: we still need a schema when it comes to querying the data, in order to drive our analysis. Performing even simple OLAP analysis on data in NoSQL stores is significantly harder than on structured data in columnar databases, because we have to work out a schema as part of the analysis. Not only that: but we have to check individual event-level data to test if the dimensions and metrics we&amp;#8217;re exploring using our OLAP analysis are correctly stored for every event we want to explore, and potentially map different fields together to include all the events that we would like. (If this is even possible.) This makes analysis much more involved and complex.&lt;/p&gt;

&lt;p&gt;Sometimes, this complexity is worth it: if our data structures are evolving so fast that any schema we develop today will be redundant tomorrow - then better to collect the data that&amp;#8217;s available today and work out how to query it another day, then over complicate our data collection by forcing the data into a schema it doesn&amp;#8217;t really fit.&lt;/p&gt;

&lt;p&gt;That is not the situation that we are in when it comes to web data, however. With a bit of thought, it is not too difficult to identify a set of events that are meaningful for a wide range of people, and a set of dimensions and metrics that are relevant for each event type. By standardising these in a Event Model, we can develop a standard set of analyses that anyone collecting data which adheres to the model can apply. As an open source community committed to driving innovation in web event analytics, this will make it easier to work collaboratively to develop new approaches to mining web data to learn new and valuable insights.&lt;/p&gt;

&lt;h2 id='the_snowplow_event_model_first_class_events_identified_so_far'&gt;The Snowplow Event Model: first class events identified so far&lt;/h2&gt;

&lt;p&gt;So far, we have identified the following events as ones we wish to identify as first class citizens. (Some of these are already incorporated in Snowplow as first class citizens, others need to be added.)&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Page views&lt;/li&gt;

&lt;li&gt;Page pings (i.e. a user reading through content on a page)&lt;/li&gt;

&lt;li&gt;Link clicks&lt;/li&gt;

&lt;li&gt;Ad impressions&lt;/li&gt;

&lt;li&gt;Online transactions&lt;/li&gt;

&lt;li&gt;Social events&lt;/li&gt;

&lt;li&gt;Item views (e.g. viewing a product on a retailer site, or viewing an article on a news site)&lt;/li&gt;

&lt;li&gt;Errors (e.g. an application returning an error)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For each of these events, we expect there to be some specific dimensions and metrics that are likely to be captured. In addition, we need to make it possible for users with particular needs to record their own custom dimensions and metrics associated with those specific events. We have detailed the event-specific fields for each of the above 1st class events on the &lt;a href='https://github.com/snowplow/snowplow/wiki/canonical-event-model'&gt;Canonical Event Model&lt;/a&gt; page on the &lt;a href='https://github.com/snowplow/snowplow/wiki'&gt;wiki&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Are there other events that we should add to the above list? Are their fields we should add to any of the specific events listed above? Let us know if so :-).&lt;/p&gt;

&lt;p&gt;In addition to the events explicitly recognised by the Event Model, there are likely to be many events that need to be tracked that are not included in the above list. For most of these, we hope that Snowplow users will store them as &lt;a href='https://github.com/snowplow/snowplow/wiki/canonical-event-model#wiki-customstruct'&gt;custom structured events&lt;/a&gt;. Where this is not possible, we plan to enable capturing of &lt;a href='https://github.com/snowplow/snowplow/wiki/canonical-event-model#wiki-customunstruct'&gt;custom unstructured events&lt;/a&gt;, so our friends who like their NoSQL technologies can create whatever JSONs they like to store their event data in.&lt;/p&gt;

&lt;h2 id='technical_implications_of_expanding_out_the_event_model'&gt;Technical implications of expanding out the event model&lt;/h2&gt;

&lt;p&gt;Currently, Snowplow events data is stored in a single &amp;#8216;fat&amp;#8217; table in either S3 or Infobright. As we build out the number of events that are explicitly included in the event model, along with their associated fields, the table will have to get wider to accommodate those new fields. Clearly, there are implications to doing so - especially as a single row of data, which represents a single event, will only have a subset of those fields populated. (Those that are relevant for the specific event.)&lt;/p&gt;

&lt;p&gt;This is one of the reasons we plan to the storage format of data stored in S3 from the current flat-file structure into &lt;a href='http://avro.apache.org/'&gt;Avro&lt;/a&gt;. There are a number of other benefits associated with migrating to Avro - these will be explored in a forthcoming blog post.&lt;/p&gt;

&lt;p&gt;We also plan to make the StorageLoader that loads data into Infobright configurable, so that it only loads fields related to events that the particular business is interested in. If, for example, you do not serve ads to your users, than you will not track ad impressions served. It therefore makes no sense to devote 5 or 6 columns in Infobright to fields which only relate to ad impression tracking like &lt;code&gt;campaign_id&lt;/code&gt;, &lt;code&gt;advertiser_id&lt;/code&gt; etc. Upgrading the StorageLoader so that it understands that is a priority moving forwards.&lt;/p&gt;

&lt;h2 id='help_us_build_out_the_event_model'&gt;Help us build out the Event Model&lt;/h2&gt;

&lt;p&gt;A lot about big data is sexy. Unfortunately, data modelling is not. Nonetheless, getting it right will be a huge benefit to the whole Snowplow community and by extension the wider web analytics community. Help us to get it right - by:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Posting ideas and feedback on this blog post&lt;/li&gt;

&lt;li&gt;Raising ideas / issues on &lt;a href='https://github.com/snowplow/snowplow/issues'&gt;Github&lt;/a&gt;. (Like the &lt;a href='https://github.com/snowplow/snowplow/issues/113'&gt;original suggestion&lt;/a&gt; from &lt;a href='https://github.com/kingo55'&gt;Robert Kingston&lt;/a&gt; to track &lt;a href='https://github.com/snowplow/snowplow/issues/113'&gt;item views&lt;/a&gt;)&lt;/li&gt;

&lt;li&gt;&lt;a href='/about/index.html'&gt;Get in touch with us directly&lt;/a&gt; to share your thoughts and ideas&lt;/li&gt;
&lt;/ul&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/01/29/snowplow-0.7.2-released</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/01/29/snowplow-0.7.2-released"/>
    <title>Snowplow 0.7.2 released, with the new no-JavaScript tracker</title>
    <updated>2013-01-29T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We&amp;#8217;re excited to announce the release of Snowplow version &lt;strong&gt;0.7.2&lt;/strong&gt;. As well as a couple of bug fixes, this release includes our second Snowplow tracker - the &lt;a href='/no-js-tracker.html'&gt;No-JS Tracker&lt;/a&gt;, to be used in web environments where a JavaScript-based tracker is not an option.&lt;/p&gt;

&lt;p&gt;One of the bug fixes is particularly important: we are recommending that &lt;strong&gt;all users of the Clojure-based Collector upgrade&lt;/strong&gt; to the new version (0.2.0) due to a serious bug in the way that event timestamps were recorded.&lt;/p&gt;

&lt;p&gt;But first let&amp;#8217;s look at the No-JS Tracker, and then talk about the other fixes:&lt;/p&gt;

&lt;h2 id='introducing_the_nojavascript_tracker'&gt;Introducing the No-JavaScript Tracker&lt;/h2&gt;

&lt;p&gt;The &lt;a href='/no-js-tracker.html'&gt;No-JS Tracker&lt;/a&gt; (or &amp;#8216;pixel tracker&amp;#8217;) can be used to log web events in environments that do not support Javascript. Examples of events include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Views an HTML email&lt;/li&gt;

&lt;li&gt;Views of product listing on a 3rd party marketplace&lt;/li&gt;

&lt;li&gt;Views a page on a 3rd party hosting site&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Our &lt;a href='/no-js-tracker.html'&gt;No-JS Tracking tag wizard&lt;/a&gt; makes it easier to generate pure-HTML tracking tags. Were you to embed these in email marketing messages, for example, you would be able to compare the behaviour of users on your website who had opened specific messages with those who had not. The &lt;a href='/no-js-tracker.html'&gt;No-JS Tracker&lt;/a&gt; enables you to track a broader set of user events in Snowplow, so providing greater coverage of your users&amp;#8217; journeys. For more information on the &lt;a href='/no-js-tracker.html'&gt;No-JS tracker&lt;/a&gt; see the &lt;a href='/blog/2013/01/29/introducing-the-no-js-tracker/'&gt;blog post&lt;/a&gt;.&lt;/p&gt;
&lt;!--more--&gt;
&lt;h2 id='important_bug_fixes'&gt;Important bug fixes&lt;/h2&gt;

&lt;p&gt;We have fixed an important issue with the logging on the Clojure Collector (&lt;a href='https://github.com/snowplow/snowplow/issues/146'&gt;issue 146&lt;/a&gt;). The previous version was logging all event dates using a 12-hour clock - meaning that it was impossible to tell if an event happened in the morning or evening.&lt;/p&gt;

&lt;p&gt;As a result of this, we &lt;strong&gt;strongly recommend&lt;/strong&gt; that &lt;strong&gt;all&lt;/strong&gt; users of the Clojure-based Collector upgrade to the new version, 0.2.0. As always, you can find this version available from our &lt;a href='https://github.com/snowplow/snowplow/wiki/Hosted-assets'&gt;Hosted assets page&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Our second bug (&lt;a href='https://github.com/snowplow/snowplow/pull/147'&gt;issue 147&lt;/a&gt;) was spotted and fixed by &lt;a href='https://github.com/ngsmrk'&gt;Angus Mark&lt;/a&gt; from &lt;a href='http://www.simplybusiness.co.uk/'&gt;Simply Business&lt;/a&gt; - many thanks Angus! There was a bug in the JavaScript tracker where the secure flag was not being correctly set on Snowplow cookies transmitted over HTTPS.&lt;/p&gt;

&lt;p&gt;We don&amp;#8217;t believe that this bug was breaking any functionality, but you can upgrade to the new version 0.9.1 of the tracker at this URL:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http(s)://d1fc8wv8zag5ca.cloudfront.net/0.9.1/sp.js&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And that&amp;#8217;s it! As always, if you do run into any issues, please &lt;a href='https://github.com/snowplow/snowplow/issues'&gt;raise an issue&lt;/a&gt; or get in touch with us via &lt;a href='https://github.com/snowplow/snowplow/wiki/Talk-to-us'&gt;the usual channels&lt;/a&gt;.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/01/29/introducing-the-no-js-tracker</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/01/29/introducing-the-no-js-tracker"/>
    <title>Introducing the No-Javascript pixel tracker</title>
    <updated>2013-01-29T00:00:00+00:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;The &lt;a href='/no-js-tracker.html'&gt;No-Javascript tracker&lt;/a&gt; (pixel tracker) enables companies running Snowplow to track users in environments that do not support Javascript. In this blog post we will cover:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href='/blog/2013/01/29/introducing-the-no-js-tracker#why'&gt;The purpose of the No-Javascript tracker (pixel tracker)&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/01/29/introducing-the-no-js-tracker#mechanics'&gt;How it works&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/01/29/introducing-the-no-js-tracker#collector-considerations'&gt;Considerations when using the No-JS tracker (pixel tracker) with the Clojure collector in particular&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/01/29/introducing-the-no-js-tracker#roadmap'&gt;Next steps on the Snowplow tracker roadmap&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;a name='why'&gt;&lt;h2&gt;What is the purpose of the No-Javascript tracker (pixel tracker)?&lt;/h2&gt; &lt;/a&gt;
&lt;p&gt;Our aim with Snowplow has been to enables companies to track user events across &lt;strong&gt;all&lt;/strong&gt; platforms and devices. That means enabling tracking offline events, as well as online events, and mobile events, as well as web events.&lt;/p&gt;

&lt;p&gt;There is a whole class of web event that Snowplow users may want to capture, but which are not possible to track using our standard Javascript tracker because they are environments that do not support Javascript. This includes:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Views of HTML emails&lt;/li&gt;

&lt;li&gt;Views of ecommerce products on 3rd party marketplaces&lt;/li&gt;

&lt;li&gt;Views of pages on 3rd party hosting sites e.g. Github&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In these cases, you can use the &lt;a href='/no-js-tracker.html'&gt;No-Javascript tracker&lt;/a&gt; (pixel tracker) to track events directly into your Snowplow stack. Doing so enables you to analyse complete customer journeys: tying together data on the emails a user has opened with their subsequent web browsing behaviour, for example.&lt;/p&gt;
&lt;!--more--&gt;&lt;a name='mechanics'&gt;&lt;h2&gt;How it works&lt;/h2&gt;&lt;/a&gt;
&lt;p&gt;The standard Javascript tracker&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Uses a set of Javascript functions to determine key data elements about an event e.g. the &lt;code&gt;page_url&lt;/code&gt; that the event occurs on or the &lt;code&gt;page_title&lt;/code&gt;&lt;/li&gt;

&lt;li&gt;Appends those data points as key value parameters on a query string&lt;/li&gt;

&lt;li&gt;Makes a GET request to your collector including the above querystring, so that the data relevant data is passed into Snowplow&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The key difference with the &lt;a href='/no-js-tracker.html'&gt;No-Javascript tracker&lt;/a&gt; (pixel tracker) is that it is not possible ot use Javascript functions to determine data points like &lt;code&gt;page_url&lt;/code&gt;. Instead, you have to hardcode those values into the request string, and append those values onto an image request for the Snowplow tracking pixel.&lt;/p&gt;

&lt;p&gt;As a result, the range of data captured by the No-Javascript tracker is smaller than the Javascript tracker. (For example, no browser features are identified and passed into Snowplow.) Nevertheless, it is still a useful data set, and can be used to return data on:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The number of unique visitors to a web page&lt;/li&gt;

&lt;li&gt;The number of events / page views (e.g. the number of times an email was opened)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id='anatomy_of_a_nojs_tracking_tag'&gt;Anatomy of a No-JS tracking tag&lt;/h3&gt;

&lt;p&gt;The standard No-JS tracking tag looks something like this:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='html'&gt;&lt;span class='c'&gt;&amp;lt;!--Snowplow start plowing--&amp;gt;&lt;/span&gt;
&lt;span class='nt'&gt;&amp;lt;img&lt;/span&gt; &lt;span class='na'&gt;src=&lt;/span&gt;&lt;span class='s'&gt;&amp;quot;http://collector.snplow.com/i?&amp;amp;e=pv&amp;amp;page=Root%20README&amp;amp;url=http%3A%2F%2Fgithub.com%2Fsnowplow%2Fsnowplow&amp;amp;aid=snowplow&amp;amp;p=web&amp;amp;tv=no-js-0.1.0&amp;quot;&lt;/span&gt; &lt;span class='nt'&gt;/&amp;gt;&lt;/span&gt;
&lt;span class='c'&gt;&amp;lt;!--Snowplow stop plowing--&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;There are several things to note about the tag:&lt;/p&gt;

&lt;h4 id='1_it_is_a_plain_html_image_tag'&gt;1. It is a plain HTML image tag&lt;/h4&gt;

&lt;p&gt;Given the tag uses no Javascript, we should not be surprised that it is just a simple HTML &lt;code&gt;&amp;lt;img src...&amp;gt;&lt;/code&gt; tag.&lt;/p&gt;

&lt;h4 id='2_only_a_handful_of_parameters_is_passed_into_the_collector'&gt;2. Only a handful of parameters is passed into the collector&lt;/h4&gt;

&lt;p&gt;The data points passed are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Event&lt;/code&gt; = &lt;code&gt;Pageview&lt;/code&gt;&lt;/li&gt;

&lt;li&gt;&lt;code&gt;Page title&lt;/code&gt; = &lt;code&gt;Root README&lt;/code&gt;&lt;/li&gt;

&lt;li&gt;&lt;code&gt;Page URL&lt;/code&gt; = &lt;code&gt;https://github.com/snowplow/snowplow&lt;/code&gt;&lt;/li&gt;

&lt;li&gt;&lt;code&gt;Application ID&lt;/code&gt; = &lt;code&gt;snowplow&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id='3_the_parameters_are_hardcoded'&gt;3. The parameters are hard-coded&lt;/h4&gt;

&lt;p&gt;As a result, is it necessary to generate a unique tag for each individual web page / email newsletter that you want to track. To make it easier to generate the tag, we have &lt;a href='/no-js-tracker.html'&gt;created a wizard&lt;/a&gt;&lt;/p&gt;
&lt;a name='collector-considerations'&gt;&lt;h2&gt;Considerations when using the No-JS tracker with the Clojure collector in particular&lt;/h2&gt;&lt;/a&gt;
&lt;p&gt;The &lt;a href='/no-js-tracker.html'&gt;No-Javascript tracker&lt;/a&gt; works with &lt;strong&gt;both&lt;/strong&gt; the &lt;a href='https://github.com/snowplow/snowplow/wiki/setting-up-the-cloudfront-collector'&gt;Cloudfront collector&lt;/a&gt; and the cross-domain &lt;a href='https://github.com/snowplow/snowplow/wiki/setting-up-the-clojure-collector'&gt;Clojure collector&lt;/a&gt;. However, there is an important difference between the way it works with each collector, that has implications for:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;What user data is captured&lt;/li&gt;

&lt;li&gt;Which services you should use the &lt;a href='/no-js-tracker.html'&gt;No-JS tracker&lt;/a&gt; with&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;When the &lt;a href='/no-js-tracker.html'&gt;No-JS tracker&lt;/a&gt; is used with the Cloudfront collector, the &lt;strong&gt;only&lt;/strong&gt; data captured is:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The name / value pairs stored on the query string i.e. the &lt;code&gt;event type&lt;/code&gt;, &lt;code&gt;page_url&lt;/code&gt; and &lt;code&gt;page_title&lt;/code&gt;&lt;/li&gt;

&lt;li&gt;The data captured as standard by the Cloudfront collector i.e. the &lt;code&gt;useragent&lt;/code&gt; string and the &lt;code&gt;date&lt;/code&gt; / &lt;code&gt;time&lt;/code&gt; of the event&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This limits the scope of the analysis that can be performed with the data: if for example we&amp;#8217;re using the No-JS tracker to track views of a README page on a Github repo, we can see how many times the page was viewed but &lt;strong&gt;not&lt;/strong&gt; how many unique users viewed the page, because no &lt;code&gt;user_id&lt;/code&gt; has been set or stored.&lt;/p&gt;

&lt;p&gt;In contrast, when using the Clojure collector with the &lt;a href='/no-js-tracker.html'&gt;No-JS tracker&lt;/a&gt;, a &lt;code&gt;user_id&lt;/code&gt; is set server-side, and saved to a cookie on the user browser. This provides better data for analytics: you can now analyse at the number of unique visitors to a web page.&lt;/p&gt;

&lt;p&gt;However, you need to make sure that you are allowed to drop a cookie on a user, on a web page owned and managed by a partner or 3rd party service provider. It is &lt;strong&gt;your&lt;/strong&gt; responsibility to ensure that you only drop cookies on web pages where the owners of the web page / service provider are happy for you to do so. There are many examples of providers who do not: for example &lt;a href='http://pages.ebay.com/help/policies/listing-javascript.html'&gt;eBay explicitly does not allow you to drop cookies on your listings pages&lt;/a&gt;. Snowplow takes &lt;strong&gt;no&lt;/strong&gt; responsibility for your use of the &lt;a href='/no-js-tracker.html'&gt;No-JS tracker&lt;/a&gt;. It is your responsibility to ensure that you abide by the terms and conditions of any 3rd party services and hosting companies you employ this tracking technology on, and we urge extreme caution when deploying the &lt;a href='/no-js-tracker.html'&gt;No-Javascript tracker&lt;/a&gt; in conjunction wiht the &lt;a href='https://github.com/snowplow/snowplow/wiki/setting-up-the-clojure-collector'&gt;Clojure collector&lt;/a&gt; on sites owned and operated by 3rd parties.&lt;/p&gt;
&lt;a name='roadmap'&gt;&lt;h2&gt;Next steps on the Snowplow tracker roadmap&lt;/h2&gt;&lt;/a&gt;
&lt;p&gt;The &lt;a href='/no-js-tracker.html'&gt;No-JS tracker&lt;/a&gt; is only our second tracker: to fulfil our vision of supporting event-data collection across many more platforms, we need to launch a wide range of new trackers.&lt;/p&gt;

&lt;p&gt;We are getting close to launching an &lt;a href='https://github.com/snowplow/snowplow-arduino-tracker'&gt;Arduino tracker&lt;/a&gt; for Snowplow, which will enable data collection from physical events into Snowplow. As you might expect, mobile trackers (especially for Android and iOS) are high priorities oadmap, alongside other software trackers (e.g. Windows 8). It will take a lot of work (and trackers) to fulfil our vision of enabling data collection across any platform in Snowplow, but we are getting there steadily.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/01/22/snowplow-0.7.1-released</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/01/22/snowplow-0.7.1-released"/>
    <title>Snowplow 0.7.1 released, with easier-to-run Ruby apps</title>
    <updated>2013-01-22T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We&amp;#8217;re happy to announce the release of Snowplow version &lt;strong&gt;0.7.1&lt;/strong&gt;. This release is designed to make it much easier to install and run the two Snowplow Ruby applications:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href='https://github.com/snowplow/snowplow/tree/master/3-etl/emr-etl-runner'&gt;EmrEtlRunner&lt;/a&gt; - which runs the Snowplow ETL job&lt;/li&gt;

&lt;li&gt;&lt;a href='https://github.com/snowplow/snowplow/tree/master/4-storage/storage-loader'&gt;StorageLoader&lt;/a&gt; - which loads Snowplow events into Infobright&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;From the feedback we received, setting up and running these two Ruby apps was the most challenging (and error-prone) part of the Snowplow experience. Many thanks to all of those in the community who reported bugs with our original approach and suggested fixes!&lt;/p&gt;

&lt;p&gt;To streamline this process and reduce the chances of problems occurring, we have updated both Ruby apps to work in a &lt;a href='https://rvm.io/integration/bundler/'&gt;RVM+Bundler&lt;/a&gt; environment, inline with Ruby community best practice. Specifically, we have:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Created a simple &lt;a href='https://github.com/snowplow/snowplow/wiki/Ruby-and-RVM-setup'&gt;guide to setting up Ruby and RVM&lt;/a&gt; ready for Snowplow&lt;/li&gt;

&lt;li&gt;Updated both of our Ruby apps to use RVM and Bundler&lt;/li&gt;

&lt;li&gt;Updated our cronjob shell scripts to work with RVM and Bundler&lt;/li&gt;

&lt;li&gt;Updated the setup guides (&lt;a href='https://github.com/snowplow/snowplow/wiki/EmrEtlRunner-setup'&gt;EmrEtlRunner&lt;/a&gt;; &lt;a href='https://github.com/snowplow/snowplow/wiki/StorageLoader-setup'&gt;StorageLoader&lt;/a&gt;) for both apps to follow RVM and Bundler best practice&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This release bumps EmrEtlRunner to version 0.0.8 and StorageLoader to version 0.0.4.&lt;/p&gt;

&lt;p&gt;And that&amp;#8217;s it! Hopefully this release fixes all of the Ruby-related issues encountered by Snowplow users - but of course there might still be a couple of teething issues. If you spot anything that still doesn&amp;#8217;t seem right, do please &lt;a href='https://github.com/snowplow/snowplow/issues'&gt;raise an issue&lt;/a&gt; or get in touch with us via &lt;a href='https://github.com/snowplow/snowplow/wiki/Talk-to-us'&gt;the usual channels&lt;/a&gt;.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/01/21/working-out-what-data-to-pass-into-your-tag-manager</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/01/21/working-out-what-data-to-pass-into-your-tag-manager"/>
    <title>What data should you be passing into your tag manager?</title>
    <updated>2013-01-21T00:00:00+00:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;Since the launch of &lt;a href='https://www.google.com/tagmanager/'&gt;Google Tag Manager&lt;/a&gt;, a plethora of blog posts have been written on the value of tag management solutions. What has been left out of the discussion is practical advice on how to setup your tag management solution (be it &lt;a href='https://www.google.com/tagmanager/'&gt;GTM&lt;/a&gt; or &lt;a href='http://www.opentag.qubitproducts.com/'&gt;OpenTag&lt;/a&gt; or one of the paid solutions), and, crucially, what data you should be passing into your tag manager. In this post, we will outline a methodology for identifying all the relevant data you should be passing in, and bringing that methodology to life with a real-world example.&lt;/p&gt;
&lt;img src='/static/img/tag-management/tag-management-schematic.gif' width='320' /&gt;
&lt;h2 id='why_is_it_important_to_define_at_implementation_time_what_data_to_pass_to_your_tag_manager'&gt;Why is it important to define, at implementation time, what data to pass to your tag manager?&lt;/h2&gt;

&lt;p&gt;One of the things we hear a lot from proponents of tag management (especially from web analyts) is that they make it easy to capture data from web pages. Indeed, the &lt;em&gt;best&lt;/em&gt; solutions enable analysts with no development knowledge to identify and capture new data points, to pass onto their web analytics program, without any programming knowledge, using snazzy drag-and-drop UIs.&lt;/p&gt;
&lt;!--more--&gt;
&lt;p&gt;We think this is rather dangerous. We&amp;#8217;re much more excited about the way that tag management solutions enable webmasters to explicitly pass data into their tag management solutions using constructs like the &lt;a href='https://developers.google.com/tag-manager/reference'&gt;&lt;code&gt;dataLayer&lt;/code&gt;&lt;/a&gt; in GTM and the &lt;a href='https://github.com/QubitProducts/UniversalVariable'&gt;&lt;code&gt;Universal Variable&lt;/code&gt;&lt;/a&gt; in OpenTag. The nice thing about this approach is that the infrastructure for managing the flow of data into your analytics infrastructure is decoupled from the infrastructure delivering the end-user experience. It means that web masters are free to improve websites, able to modify elements of web pages without breaing any data transfer processes. It forces companies to think through what data they should be capturing, and document it. It makes it easy for analysts and data scientists, down the line, to audit what data is being collected and how.&lt;/p&gt;

&lt;p&gt;The trouble with insisting on formally passing data to your tag management system using things like the &lt;a href='https://developers.google.com/tag-manager/reference'&gt;&lt;code&gt;dataLayer&lt;/code&gt;&lt;/a&gt; or &lt;a href='https://github.com/QubitProducts/UniversalVariable'&gt;&lt;code&gt;Universal Variable&lt;/code&gt;&lt;/a&gt; is that it makes the process of implementing a tag management system more complicated: because you have to identify all the data points you want to pass to your web analytics (and advertising) systems and often develop a data model for transferring them to your tag management system, so that it can pass them on via the tags it fires.&lt;/p&gt;

&lt;p&gt;For Snowplow users, the challenge is more acute. Whereas other analytics systems recommend that you only pass data into them that you know how to use / evaluate, we recommend that Snowplow users pass in &lt;em&gt;all&lt;/em&gt; the data associated with the events on a user journey so that analysts have a &lt;strong&gt;complete&lt;/strong&gt; picture of a user&amp;#8217;s journey. Then it is up to the analyst to decide whether or not specific bits of data are valuable based on what he / she does with that data (rather than prejudging it). So for Snowplow users who are setting up a tag management system, the challenge is to identify, upfront &lt;strong&gt;all&lt;/strong&gt; the data points to pass into the tag management system, so that they can be passed on to Snowplow via the Snowplow tracking tags. Simple, right?&lt;/p&gt;

&lt;h2 id='what_data_do_we_want_to_pass_into_snowplow'&gt;What data do we want to pass into Snowplow?&lt;/h2&gt;

&lt;p&gt;Broadly speaking, there are types of data that we want to process in Snowplow: event data and page-level data.&lt;/p&gt;

&lt;h3 id='event_data'&gt;Event data&lt;/h3&gt;

&lt;p&gt;At its heart, Snowplow is a tool for capturing, storing and analysing event-stream data, with a focus on web event data. We aim to capture all events that occur on an individual&amp;#8217;s customer journey. To give a random assortment of examples of the types of events we might include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Add item to basket&lt;/li&gt;

&lt;li&gt;Like a post&lt;/li&gt;

&lt;li&gt;Invite someone to be friends&lt;/li&gt;

&lt;li&gt;Watch a video&lt;/li&gt;

&lt;li&gt;Review a product&lt;/li&gt;

&lt;li&gt;Log in to an application&lt;/li&gt;

&lt;li&gt;Create and display a graph&lt;/li&gt;

&lt;li&gt;Send a message&lt;/li&gt;

&lt;li&gt;Create a listing&lt;/li&gt;

&lt;li&gt;Update a status&lt;/li&gt;

&lt;li&gt;Ask a question&lt;/li&gt;

&lt;li&gt;Book a flight&lt;/li&gt;

&lt;li&gt;Donate to a charity&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As part of setting up a tag management solution, it is important to identify all the possible events that can occur to a user on his / her journey through our website, and what data we want to capture for each of those event types.&lt;/p&gt;

&lt;h3 id='web_page_entity_data'&gt;Web page entity data&lt;/h3&gt;

&lt;p&gt;As the web evolves, websites look less-and-less like hyperlinked documents and more-and-more like interactive applications. A larger fraction of interesting events on a customer journeys are powered by AJAX events, and fewer are enabled by web page loads.&lt;/p&gt;

&lt;p&gt;In spite of this evolution, web page loads are still very important events in a user journey. Broadly speaking, we capture data that occur thanks to AJAX events using the &lt;a href='https://github.com/snowplow/snowplow/wiki/javascript-tracker#wiki-events'&gt;Snowplow event tracking method&lt;/a&gt; except for specific events that have their own specific methods e.g. &lt;a href='https://github.com/snowplow/snowplow/wiki/javascript-tracker#wiki-ecommerce'&gt;tracking ecommerce transaction&lt;/a&gt; or &lt;a href='https://github.com/snowplow/snowplow/wiki/javascript-tracker#wiki-adimps'&gt;ad impression tracking&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;To capture the broad swathe of events that result in a web page load, we use the &lt;a href='https://github.com/snowplow/snowplow/wiki/javascript-tracker#wiki-page'&gt;page tracker method&lt;/a&gt;. However, performing an analysis on the journey a user has taken based on the URLs and page titles of the pages they have visited is not that informative: we really want to store what entities were displayed on those web pages, so we can analyse what the user was shown, what entities they engaged with and which they did not. To take a simple example, we might want to compare conversion rate for a retailer by product, to see which products &amp;#8216;convert best&amp;#8217; and why. In order to do this, we need to pass onto Snowplow exactly what products were displayed on the web pages they visited, and potentially pass in additional information like what type of listing they were shown.&lt;/p&gt;

&lt;p&gt;To give an example of the types of entities we might identify:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Products&lt;/li&gt;

&lt;li&gt;Articles / blog posts&lt;/li&gt;

&lt;li&gt;Videos&lt;/li&gt;

&lt;li&gt;Adverts&lt;/li&gt;

&lt;li&gt;People / connections&lt;/li&gt;

&lt;li&gt;Jobs&lt;/li&gt;

&lt;li&gt;Flights&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As well as identifying all the events that can occur on a user journey, then, we &lt;em&gt;also&lt;/em&gt; need to identify &lt;strong&gt;all&lt;/strong&gt; the key elements that make up each web page, and pass them to our tag manager on page load, as part of a tag management implementation.&lt;/p&gt;

&lt;h2 id='summarising_our_method_for_identifying_all_the_data_points_to_pass_into_the_tag_manager'&gt;Summarising our method for identifying all the data points to pass into the tag manager&lt;/h2&gt;

&lt;p&gt;We are now in a position to pull the above information together and summarise our suggested approach:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Identify all the relevant events that can occur to a user navigating your website app. Catalogue each.&lt;/li&gt;

&lt;li&gt;For each event type, document what data should/could be captured&lt;/li&gt;

&lt;li&gt;Comb through each web page that makes up your website, and identify the relevant entities that that make up web pages.&lt;/li&gt;

&lt;li&gt;For each entity, document what data you want to capture&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id='bringing_the_method_to_life_a_realworld_example'&gt;Bringing the method to life: a real-world example&lt;/h2&gt;

&lt;p&gt;We applied the above methodology to &lt;a href='http://www.psychicbazaar.com/index.php'&gt;Psychic Bazaar&lt;/a&gt;, an online retailer in the esoteric space. The good folks at Psychic Bazaar have kindly allowed us to share the implementation guide, so you can see the approach in action. Psychic Bazaar is in the process of implementing Google Tag Manager. However, the approach outlined is tag manager agnostic: if they implemented OpenTag instead, then references to the &lt;code&gt;dataLayer&lt;/code&gt; would be replaced to references to the &lt;code&gt;Universal Variable&lt;/code&gt; - the actual data and structure of the data would remain unchanged.&lt;/p&gt;
&lt;a href='/static/pdf/google-tag-manager-implementation-specification-for-psychic-bazaar.pdf'&gt;&lt;img src='/static/img/tag-management/gtm-spec-title-page.JPG' /&gt;&lt;/a&gt;
&lt;h2 id='want_help_implementing_a_tag_management_solution'&gt;Want help implementing a tag management solution?&lt;/h2&gt;

&lt;p&gt;The Snowplow &lt;a href='/services/index.html'&gt;Professional Services team&lt;/a&gt; can produce implementation guides like &lt;a href='/static/pdf/google-tag-manager-implementation-specification-for-psychic-bazaar.pdf'&gt;the one for Psychic Bazaar&lt;/a&gt;. If you are implementing a tag management solution, either as part of a Snowplow implementation or not, and and would like assistance, then &lt;a href='/about/index.html'&gt;get in touch&lt;/a&gt;.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/01/20/snowplow-hits-202-stars</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/01/20/snowplow-hits-202-stars"/>
    <title>Snowplow reaches 202 stars on GitHub</title>
    <updated>2013-01-20T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;As of this weekend, the Snowplow repository on GitHub now has over 200 stars! We&amp;#8217;re hugely excited to reach this milestone - this makes us:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The 3rd most-watched analytics project on GitHub, after &lt;a href='https://github.com/mnutt/hummingbird'&gt;Hummingbird&lt;/a&gt; (real-time analytics) and &lt;a href='https://github.com/Countly/countly-server'&gt;Countly&lt;/a&gt; (mobile analytics)&lt;/li&gt;

&lt;li&gt;The &lt;a href='https://github.com/languages/Scala/most_watched?page=3'&gt;58th most-watched&lt;/a&gt; Scala project on GitHub&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Many thanks to everyone in the Snowplow community and on GitHub for their support and interest!&lt;/p&gt;

&lt;p&gt;We thought it might be interesting to share the &lt;a href='http://jrvis.com/red-dwarf/?user=snowplow&amp;amp;repo=snowplow'&gt;Red Dwarf heatmap&lt;/a&gt; of where our 202 GitHub stars are located across the world:&lt;/p&gt;

&lt;p&gt;&lt;img alt='heatmap' src='/static/img/blog/2013/01/snowplow-stars-at-202.png' /&gt;&lt;/p&gt;

&lt;p&gt;Beyond the &amp;#8220;hot spots&amp;#8221; in the US and Europe, it&amp;#8217;s encouraging to see growing interest in Snowplow from South America, South/East Asia and Australia/New Zealand. We look forward to checking back on the heatmap when we have some more stars!&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/01/18/using-snowplow-with-qubit-opentag</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/01/18/using-snowplow-with-qubit-opentag"/>
    <title>Implementing Snowplow with QuBit's OpenTag</title>
    <updated>2013-01-18T00:00:00+00:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;This is a short blog post to highlight a new section on the &lt;a href='https://github.com/snowplow/snowplow/wiki/Snowplow-setup-guide'&gt;Snowplow setup guide&lt;/a&gt; covering &lt;a href='https://github.com/snowplow/snowplow/wiki/Integrating%20Javascript%20tags%20with%20QuBit%20OpenTag'&gt;how to integrate Snowplow with QuBit&amp;#8217;s OpenTag tag management system&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In November last year, we started playing with tag management systems: testing Snowplow with Google Tag Manager, and documented how to setup Snowplow with GTM on the &lt;a href='https://github.com/snowplow/snowplow/wiki/Integrating-javascript-tags-with-Google-Tag-Manager'&gt;Snowplow setup guide&lt;/a&gt;. We were impressed on a number of fronts, but thought that the much more thought need to be put into what data was passed into the tag management system than people typically admit. (We documented our thoughts, at the time, on &lt;a href='/blog/2012/11/16/integrating-snowplow-with-google-tag-manager/'&gt;this blog post&lt;/a&gt;.)&lt;/p&gt;

&lt;p&gt;Since then, we&amp;#8217;ve recommended that &lt;strong&gt;all&lt;/strong&gt; new Snowplow users setup a tag management system, prior to integrating Snowplow on their website, if they have not already done so. The benefits of doing so are well documented elsewhere. For Snowplow users, there are two big benefits in particular, that we will flag:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Going through the exercise of implementing a tag management solution forces companies to take a rigorous look at the data they pass from their website into the tag manager, especially when declaring the data explicitly using things like the &lt;code&gt;dataLayer&lt;/code&gt; (in GTM) or the &lt;code&gt;Universal Variable&lt;/code&gt; in OpenTag. This makes it easier for analysts, down the line, to understand &lt;em&gt;what&lt;/em&gt; data has been passed into their web analytics system, and how that data has been generated: key bits of information that can often get lost months after web analytics platforms like Snowplow have been implemented. In addition, it makes the analytics as a whole more robust, as the generation of data is decoupled from the generation of other elements of web pages, which means web developers can continue to improve site functionality, safe in the knowledge they wont break anything on the analytics side.&lt;/li&gt;

&lt;li&gt;A selfish reason, perhaps, but having our customers use a tag management platforms gives us the freedom to improve Snowplow tracking tags where we see the opportunity, safe in the knowledge that we&amp;#8217;re not causing our clients too much difficulty to upgrade their tags.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img alt='qubit-opentag-logo' src='/static/img/blog/2013/01/qubit-opentag.png' /&gt;&lt;/p&gt;

&lt;p&gt;We were eager, having integrated Snowplow with GTM, to integrate it with an open source tag management system. After all, a big selling point of Snowplow is that it is open source: enabling companies to setup, own and manage their own data infrastructure, without relying on a third party to mediate their access to their own data. We were therefore delighted that QuBit has developed an open source tag management system, &lt;a href='http://www.opentag.qubitproducts.com/'&gt;OpenTag&lt;/a&gt;, and that we have finally documented &lt;a href='https://github.com/snowplow/snowplow/wiki/Integrating%20Javascript%20tags%20with%20QuBit%20OpenTag'&gt;how to integrate it with Snowplow&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;There is a lot we like about QuBit&amp;#8217;s OpenTag:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Open source&lt;/strong&gt;. You can view the &lt;a href='https://github.com/QubitProducts/OpenTag/blob/master/OpenTag.js'&gt;OpenTag.js&lt;/a&gt; on Github to get a handle on exactly how OpenTag works&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Low cost hosted service&lt;/strong&gt;. QuBit offers free hosting for sites with less than 1M page views per month, and $99 per 10M page views thereafter&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Easy-to-implement host-yourself option&lt;/strong&gt;. You can use OpenTag&amp;#8217;s web UI to configure all your tags for free and publish the results to a Javascript file that you can then host on your own CDN (e.g. Amazon Cloudfront). By not using Qubit to host the configured javascript file with all your different tags, you are not locked into QuBit as a vendor. In addition, the cost of managing your tags across large sites, content networks and ad networks is kept at a bare minimum.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In order to make it easier for new Snowplow users to implement OpenTag alongside Snowplow, and existing OpenTag users to implement Snowplow, we&amp;#8217;ve documented how to setup Snowplow in OpenTag on our &lt;a href='https://github.com/snowplow/snowplow/wiki/Integrating%20Javascript%20tags%20with%20QuBit%20OpenTag'&gt;setup guide&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Keep plowing!&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/01/16/scala-maxmind-geoip-released</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/01/16/scala-maxmind-geoip-released"/>
    <title>Scala MaxMind GeoIP library released</title>
    <updated>2013-01-16T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;A short blog post this, to announce the release of &lt;a href='https://github.com/snowplow/scala-maxmind-geoip'&gt;&lt;strong&gt;Scala MaxMind GeoIP&lt;/strong&gt;&lt;/a&gt;, our Scala wrapper for the MaxMind &lt;a href='http://www.maxmind.com/download/geoip/api/java/'&gt;Java Geo-IP&lt;/a&gt; library.&lt;/p&gt;

&lt;p&gt;We have extracted Scala MaxMind GeoIP from our current (ongoing) work porting our ETL process from Apache Hive to &lt;a href='https://github.com/twitter/scalding'&gt;Scalding&lt;/a&gt;. We extracted this as a separate library for two main reasons:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Being good open-source citizens&lt;/strong&gt; - as with our &lt;a href='https://github.com/snowplow/referer-parser'&gt;referer-parser&lt;/a&gt; library, we believe this library willl be useful to the wider community of software developers, not just Snowplow users&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Keeping Snowplow&amp;#8217;s footprint small&lt;/strong&gt; - at Snowplow we believe very strongly in building modular, loosely-coupled software. Massive monolithic systems that &amp;#8216;do everything&amp;#8217; are a nightmare to test, maintain and extend - so we prefer to build small, standalone components and libraries which we (and the community) can then compose into larger pipelines and processes&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;On to the library: for Scala developers, the main benefits of using &lt;a href='https://github.com/snowplow/scala-maxmind-geoip'&gt;scala-maxmind-geoip&lt;/a&gt; over the MaxMind Java library are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Easier to setup/test&lt;/strong&gt; - the SBT project definition automatically pulls down the latest MaxMind Java code and &lt;code&gt;GeoLiteCity.dat&lt;/code&gt;&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Better type safety&lt;/strong&gt; - the MaxMind Java library is somewhat null-happy. This library uses Option boxing wherever possible&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Better performance&lt;/strong&gt; - as well as or instead of using MaxMind&amp;#8217;s own caching (&lt;code&gt;GEOIP_MEMORY_CACHE&lt;/code&gt;), you can also configure an LRU (Least Recently Used) cache of variable size&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;That&amp;#8217;s it! And if you have any problems with this Scala library for MaxMind GeoIP lookups, please &lt;a href='https://github.com/snowplow/snowplow/issues'&gt;raise an issue&lt;/a&gt; or get in touch with us via &lt;a href='https://github.com/snowplow/snowplow/wiki/Talk-to-us'&gt;the usual channels&lt;/a&gt;.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/01/09/from-etl-to-enrichment</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/01/09/from-etl-to-enrichment"/>
    <title>The Snowplow development roadmap for the ETL step - from ETL to enrichment</title>
    <updated>2013-01-09T00:00:00+00:00</updated>
    <author>
      <name>yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;In this blog post, we outline our plans to develop the &lt;a href='https://github.com/snowplow/snowplow/wiki/etl'&gt;ETL&lt;/a&gt; (&amp;#8220;extract, transform and load&amp;#8221;) part of the Snowplow stack. Although in many respects the least sexy element of the stack, it is critical to Snowplow, and we intend to re-architect the ETL step in quite significant ways. In this post, we discuss our plans and the rationale behind them, in the hope to get:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Feedback from the community on them&lt;/li&gt;

&lt;li&gt;Ideas for alternative approaches or new features&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We will cover:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href='/blog/2013/01/09/from-etl-to-enrichment/#purpose'&gt;Recap: the point of the ETL step&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/01/09/from-etl-to-enrichment/#limitations'&gt;Limitations with the current, Hive-based ETL process&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/01/09/from-etl-to-enrichment/#enrichment'&gt;From ETL to enrichment&lt;/a&gt;: what we want the ETL step to achieve&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/01/09/from-etl-to-enrichment/#speed'&gt;Towards a real-time ETL&lt;/a&gt;: speeding things up&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/01/09/from-etl-to-enrichment/#scalding'&gt;Moving to Cascading / Scalding&lt;/a&gt;: what we plan to do&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/01/09/from-etl-to-enrichment/#benefits'&gt;Benefits of this approach&lt;/a&gt;: both in the short and long term&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To get the conversation started, a conceptual map of the new ETL process is shown below. You will probably want to click on it to see a blown-up PDF version, as it is rather large:&lt;/p&gt;
&lt;p&gt;&lt;a href='/static/pdf/snowplow-scalding-etl-specification.pdf'&gt;&lt;img src='/static/img/blog/2013/01/scalding-etl-spec.gif' /&gt;&lt;/a&gt;&lt;/p&gt;&lt;!--more--&gt;&lt;a name='purpose'&gt;&lt;h2&gt;Recap: the point of the ETL step&lt;/h2&gt;&lt;/a&gt;
&lt;p&gt;The primary purpose of the ETL step is to parse the logs generated by the Snowplow collector(s) and push the data stored into one or more storage facilities (e.g. S3, Infobright) where it can be accessed by analytic tools. However, there are two complexities that have to be dealt with:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Checking data quality and resolving any issues&lt;/strong&gt;. Sometimes, the Snowplow tracker has not been correctly configured; sometimes, there may even be a bug in a tracker or collector, which means that the log files contain errors. In an ideal world, the ETL step should validate the lines of data in the logs, push data through to storage when the data is good quality, and initiate a process for handling malformed data in the unfortunate cases when it is not. (Note: most web analytics programmes do not support this, so if you haven&amp;#8217;t set your tracking up properly and haven&amp;#8217;t been logging data correctly for a couple of months - tough - there&amp;#8217;s no way of fixing it.) By flagging malformed data quickly, the ETL step should also provide the ops team with a good guide to review the tracker and collector setup, and correct any mistakes.&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Supporting multiple storage options&lt;/strong&gt;. We want Snowplow to support the widest range of analytics: encompassing &lt;a href='/analytics/tools-and-techniques/converting-snowplow-data-into-a-format-suitable-for-olap.html'&gt;OLAP style aggregations&lt;/a&gt; slicing and dicing of data, &lt;a href='http://mahout.apache.org/'&gt;Mahout-like machine learning&lt;/a&gt; and &lt;a href='https://github.com/skydb'&gt;Sky-like&lt;/a&gt; event stream analytics. The ETL step has to be powerful enough to push data into multiple locations in an efficient manner, and support pushing different cuts and structures of the data into each of those different storage options as required.&lt;/li&gt;
&lt;/ol&gt;
&lt;a name='limitations'&gt;&lt;h2&gt;Limitations with the current, Hive-based ETL process&lt;/h2&gt;&lt;/a&gt;
&lt;p&gt;The current ETL process is based on Hive, which processes Cloudfront-formatted log files containing querystrings matching the &lt;a href='https://github.com/snowplow/snowplow/wiki/snowplow-tracker-protocol'&gt;Snowplow tracker protocol&lt;/a&gt; using a &lt;a href='https://github.com/snowplow/snowplow/tree/master/3-etl/hive-etl/snowplow-log-deserializers'&gt;custom deserializer&lt;/a&gt;. This was a good option to build an initial prototype of the ETL step: it enabled us to query data in the raw logs directly, and made it relatively straightforward to transfer the data from the Snowplow log format into a more standard format suitable for faster querying in Hive or importing into Infobright.&lt;/p&gt;

&lt;p&gt;However, there are a number of limitations to the Hive-based ETL process:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;It makes error handling very difficult&lt;/strong&gt;. Either a row is processed, or it is not. There&amp;#8217;s no option to build more sophisticated data processing pipelines including flows to divert malformed data, spot the source of the data quality problem and address it.&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;It is a tightly-coupled process&lt;/strong&gt;: all the parsing on the entire row is performed by the custom deserializer. If something goes wrong, it is hard to debug what went wrong. If we want to extend part of the ETL process, we have to go in and upgrade the deserializer or the HiveQL wrapper scripts. As the conceptual map of our proposed ETL shown at the top of this post demonstrates, our ideal ETL process consists of multiple steps. These should be decoupled for robustness and ease of extension.&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;It is hard to extend the ETL process to build enrichments of the data&lt;/strong&gt;. (See the &lt;a href='#enrichments'&gt;next section&lt;/a&gt;.)&lt;/li&gt;
&lt;/ol&gt;
&lt;a name='enrichment'&gt;&lt;h2&gt;From ETL to enrichment: what we want to achieve&lt;/h2&gt; &lt;/a&gt;
&lt;p&gt;The initial purpose of the ETL step was quite narrow: to move data generated by the collectors into the different storage options for analytics. Since then, we have realised that there are a number of important enrichments that can be performed on the data, that are best done as part of the ETL step, so that they are available when the data comes to be analysed. Examples include:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Inferring location from &lt;code&gt;user_ipaddress&lt;/code&gt; e.g. using &lt;a href='http://www.maxmind.com/en/geolocation_landing'&gt;Maxmind&lt;/a&gt; or &lt;a href='http://www.digitalelement.com/our_technology/our_technology.html'&gt;Digital Element&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;Inferring marketing parameters (source, medium, keywords) by processing referrer url and query strings using &lt;a href='https://github.com/snowplow/referer-parser'&gt;referer-parser&lt;/a&gt;. This would include identifying search engine originated traffic and social network originated traffic, for example&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In addition, decomposing some of the fields into constituent elements can make analysis easier: for example, breaking up &lt;code&gt;page_url&lt;/code&gt; and &lt;code&gt;referrer_url&lt;/code&gt; into host, domain, path and query string, can enable us to easily group visits by referer domain or path, depending on granularity of analysis we&amp;#8217;re performing.&lt;/p&gt;
&lt;a name='speed'&gt;&lt;h2&gt;Towards a real-time ETL process: speeding things up&lt;/h2&gt;&lt;/a&gt;
&lt;p&gt;The majority of Snowplow users run their ETL process daily, so that yesterday&amp;#8217;s data is available today.&lt;/p&gt;

&lt;p&gt;We need to move the whole Snowplow stack so that data is available for analytics faster. Doing so will be welcomed by analysts crunching Snowplow data, but perhaps more significantly, it will open up the possibility of building real-time response engines based on Snowplow data: these might include things like retargeting users who&amp;#8217;ve performed specific actions with display ads or emails, or personalising the content shown to a user based on their recent browsing history, on the fly.&lt;/p&gt;

&lt;p&gt;There is limited scope to speed up the current Hive-based ETL process. However, there are lots of interesting opportunities that arise if we consider an alternative archtiecture, especially one that moves us closer to a stream-based data processing model.&lt;/p&gt;
&lt;a name='scalding'&gt;&lt;h2&gt;Moving to Cascading / Scalding: how we plan to rearchitect the ETL process&lt;/h2&gt;&lt;/a&gt;
&lt;p&gt;We intend to replace the current Hive-based ETL process with one based on the Scala library that runs on top of &lt;a href='http://www.cascading.org/'&gt;Cascading&lt;/a&gt;, known as &lt;a href='https://github.com/twitter/scalding'&gt;Scalding&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Cascading is an application framework specifically designed to build robust data pipelines using Hadoop. We intend to use it to build the pipeline &lt;a href='/static/pdf/snowplow-scalding-etl-specification.pdf'&gt;sketched above&lt;/a&gt;.&lt;/p&gt;
&lt;a name='benefits'&gt;&lt;h2&gt;Benefits of this approach: both in the short and long term&lt;/h2&gt;&lt;/a&gt;
&lt;p&gt;By rearchitecting the ETL using Scalding / Cascading, we hope to realise the following benefits in the short-term:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Deliver enrichments on the data: in particular, classify visits based on referer, and locate users via geo-ip&lt;/li&gt;

&lt;li&gt;Improved handling of malformed data: making it easier to spot bugs in Snowplow, mistakes in tracker or collector setup, and the ability to fix and reprocess malformed data&lt;/li&gt;

&lt;li&gt;Make it easier to run the ETL process more frequently, so that Snowplow data is more up-to-date&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In the long term there are a number of important benefits we hope moving to Scalding will help us realise:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Expand the ETL to output data in a format suitable for OLAP reporting&lt;/strong&gt;. Currently, users who want to use OLAP tools e.g. Tableau, Pentaho or Microstrategy, to report on Snowplow data, need to &lt;a href='/analytics/tools-and-techniques/converting-snowplow-data-into-a-format-suitable-for-olap.html'&gt;transform that data&lt;/a&gt; prior to running those tools on top of it. We want to build out the ETL process to output two versions of the data: the raw event field (as it currently does) and a cube-formatted version that can be used directly with these tools. Delivering this with the current Hive-based process would be incredibly difficult.&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Move towards a real-time engine&lt;/strong&gt;. In order to deliver data in real-time, Snowplow ETL would need to move from a Hadoop, batch-based process into a stream-based process, likely using &lt;a href='http://storm-project.net/'&gt;Storm&lt;/a&gt;. Porting the data pipeline from Cascading to Storm should be significantly easier than porting it from Hive to Storm: as such, Cascading provides a useful stepping stone on our journey to deliver real time event-level analytics.&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Make it easier to support a wider range of collector log formats&lt;/strong&gt;. Because the ETL process is decoupled, handling a different log file format means only updating the first processing step in the data pipeline that parses the raw collector logs. That means building out the ETL to support other collectors (e.g. &lt;a href='/blog/2012/08/13/introducing-snow-cannon-a-node-js-collector-for-snowplow/'&gt;SnowCannon&lt;/a&gt;) should be much simpler.&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Make it easier to support a growing range of event types&lt;/strong&gt;. As should be clear from the &lt;a href='/static/pdf/snowplow-scalding-etl-specification.pdf'&gt;data pipeline flowchart&lt;/a&gt;, seven event types are currently supported, each with their own set of fields. (Page views, page pings, link clicks, custom events, ad impressions, transaction events and transaction items.) That list is only likely to grow over time. By clearly differentiating each of them in the data pipeline, a Scalding-based ETL process should be easier to extend to support a greater range of events.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id='we_want_your_feedback'&gt;We want your feedback&lt;/h2&gt;

&lt;p&gt;We&amp;#8217;ve been very lucky to have community members contribute an enormous number of fantastic ideas and code that we&amp;#8217;ve been able to incorporate into Snowplow. We&amp;#8217;ve shared our roadmap for the ETL step and our rationale for that roadmap to see what you think. Does our approach sound sensible? What should we do differently? What can we add to it to make it more robust and valuable?&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/01/08/using-chartio-to-visualise-and-interrogate-snowplow-data</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/01/08/using-chartio-to-visualise-and-interrogate-snowplow-data"/>
    <title>Using ChartIO to visualise and interrogate Snowplow data</title>
    <updated>2013-01-08T00:00:00+00:00</updated>
    <author>
      <name>yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;In the last couple of weeks, we have been experimenting with &lt;a href='http://chartio.com/'&gt;ChartIO&lt;/a&gt; - a hosted BI tool for visualising data and creating dashboards. So far, we are very impressed - ChartIO is an excellent analytics tool to use to interrogate and visualise Snowplow data. Given the number of requests we get from Snowplow users to recommend tools to assist with analytics on Snowplow data, we thought it well worth sharing why ChartIO is so good, and give some examples of analyses on Snowplow data using ChartIO.&lt;/p&gt;

&lt;p&gt;&lt;img alt='chartio-pic-0' src='/static/img/blog/2013/01/chartio-0.png' /&gt;&lt;/p&gt;

&lt;p&gt;In this post we cover:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href='/blog/2013/01/08/using-chartio-to-visualise-and-interrogate-snowplow-data#why'&gt;Why is ChartIO so good?&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/01/08/using-chartio-to-visualise-and-interrogate-snowplow-data#setup'&gt;Setting up ChartIO to work with Snowplow&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/01/08/using-chartio-to-visualise-and-interrogate-snowplow-data#engagement'&gt;Tutorial: using ChartIO to unpick the drivers of engagement with a site&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a name='why'&gt;Why is ChartIO so good?&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;ChartIO is great for two reasons:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Fast&lt;/strong&gt;. ChartIO is quick to setup. (Because it is a hosted product, with a very nice script for establishing an SSH connection between your database and the ChartIO web application.) At the same time, it is very quick, once a data connection is established, to create new graphs and charts and embed them in dashboards.&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Easy&lt;/strong&gt;. ChartIO is easy to use. This is partly because the UI is really nice. (Lots of drag and drop, easy-to-follow workflow.) But it is also because ChartIO is very simple: it lacks a lot of the complexity of more traditional BI tools like Microstrategy and Pentaho. It is a lot simpler even than more recent innovations in the space like Tableau. Whilst this means it is a bit less powerful, the upside is the tool is a lot easier to use than comparable tools.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;ChartIO has one enormous advantage that makes it especially well suited to querying Snowplow data: it does not require the data to be in a specific format before it will let users chart / graph it. That compares with the vast majority of tools (including Tableau, Qlikview, Pentaho and Microstrategy) that all require that any data is structured in a format suitable for &lt;a href='/analytics/tools-and-techniques/converting-snowplow-data-into-a-format-suitable-for-olap.html'&gt;OLAP analysis&lt;/a&gt; before they can be used. (We covered how to convert Snowplow data into that format in the &lt;a href='/analytics/tools-and-techniques/converting-snowplow-data-into-a-format-suitable-for-olap.html'&gt;analytics cookbook&lt;/a&gt;.) ChartIO &lt;strong&gt;does&lt;/strong&gt; work better with data that is formatted in this way, but it still works beautifully with the data as is. As a result, &lt;strong&gt;ChartIO is, we believe, the easiest way to build graphs and dashboards on top of Snowplow data&lt;/strong&gt;.&lt;/p&gt;
&lt;!--more--&gt;&lt;h2&gt;&lt;a name='setup'&gt;Setting up ChartIO to work with Snowplow&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;You can get started with ChartIO by signing up to a free 30 day trial. Connecting it to Snowplow data is straightforward: full instructions can be found &lt;a href='https://github.com/snowplow/snowplow/wiki/ChartIO-setup'&gt;on the setup guide&lt;/a&gt;, including how to create your first graph using Snowplow data in ChartIO.&lt;/p&gt;
&lt;h2&gt;&lt;a name='engagement'&gt;Tutorial: using ChartIO to unpick the drivers of engagement with a site&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id='before_we_get_started_how_will_we_measure_engagement'&gt;Before we get started: how will we measure engagement?&lt;/h3&gt;

&lt;p&gt;As we discuss in detail in the &lt;a href='/analytics/customer-analytics/user-engagement.html'&gt;analytics cookbook&lt;/a&gt;, there are many possible ways to measure engagement, and Snowplow supports all of them. We need to pick one or two to use in this tutorial, although it would be possible to perform the analyses described with any measure that suits your business.&lt;/p&gt;

&lt;p&gt;For this tutorial we&amp;#8217;re going to use data from &lt;a href='http://www.psychicbazaar.com/'&gt;Psychic Bazaar&lt;/a&gt;, an online retailer of esoteric products. For an online retailer, whether a visitors makes a purchase is generally more interesting than whether they &amp;#8216;engage&amp;#8217; in vaguer terms. So we will use conversion rate as our first measure of engagement. However, to keep our tutorial interesting to people who want to perform the analysis on non-retail sites, we will also look at number of page views over a period of time as a measure of engagement.&lt;/p&gt;

&lt;h3 id='establishing_the_baseline_measuring_engagement_over_time'&gt;Establishing the baseline: measuring engagement over time&lt;/h3&gt;

&lt;p&gt;Lets start by looking out how engagement has changed over time on Psychic Bazaar. Let&amp;#8217;s create a new dashboard to explore this issue in particular. Log into ChartIO and click on the &lt;strong&gt;+Dashboard&lt;/strong&gt; link on the left hand menu to create a new dashboard.&lt;/p&gt;

&lt;p&gt;&lt;img alt='chartio-pic-1' src='/static/img/blog/2013/01/chartio-1.png' /&gt;&lt;/p&gt;

&lt;p&gt;Give the dashboard a suitable name and description and then click the relevant button to craete it. Now we need to add a chart to it. Click on the &lt;strong&gt;+Chart&lt;/strong&gt; link on the right hand menu. The Chart Creator opens in &lt;strong&gt;interactive mode&lt;/strong&gt;, with your database on the top left, a list of tables under it (including the Snowplow events table) and under the table, a list of fields split by which ChartIO believes is a measure and dimension.&lt;/p&gt;

&lt;p&gt;&lt;img alt='chartio-pic-2' src='/static/img/blog/2013/01/chartio-2.png' /&gt;&lt;/p&gt;

&lt;p&gt;In interactive mode, ChartIO lets you drag and drop measures into the &lt;strong&gt;Measures&lt;/strong&gt;, &lt;strong&gt;Dimensions&lt;/strong&gt; and &lt;strong&gt;Filters&lt;/strong&gt; dialogue box to generate graphs. We&amp;#8217;re not going to do that, though, because we want to be explicit about how ChartIO uses Snowplow data. So we&amp;#8217;re going to use &lt;strong&gt;Query mode&lt;/strong&gt; by clicking on the &lt;strong&gt;Query mode&lt;/strong&gt; hyperlink on the top left of the &lt;strong&gt;Layer 1&lt;/strong&gt; box. This enables us to enter a SQL query directly. ChartIO will graph the results:&lt;/p&gt;

&lt;p&gt;&lt;img alt='chartio-pic-3' src='/static/img/blog/2013/01/chartio-3.png' /&gt;&lt;/p&gt;

&lt;p&gt;Now we&amp;#8217;re ready to graph engagement levels over time. Let&amp;#8217;s start with our first measure of engagement: conversion levels. We want to look at what % of users who visit our site each month that perform a transaction. To do this, we first need to identify users who have performed a transaction each month, using the following query:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='mysql'&gt;&lt;span class='k'&gt;SELECT&lt;/span&gt;
&lt;span class='nf'&gt;MONTH&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;dt&lt;/span&gt;&lt;span class='p'&gt;),&lt;/span&gt;
&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='mi'&gt;1&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;buyer&lt;/span&gt;
&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='n'&gt;events_005&lt;/span&gt;
&lt;span class='k'&gt;WHERE&lt;/span&gt; &lt;span class='n'&gt;event&lt;/span&gt;&lt;span class='o'&gt;=&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;transaction&amp;#39;&lt;/span&gt;
&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='n'&gt;dt&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;user_id&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now we join this table with the events table to list all the users who have visited each month and identify which of them has bought:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='mysql'&gt;&lt;span class='k'&gt;SELECT&lt;/span&gt;
&lt;span class='n'&gt;visitors&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='n'&gt;visitors&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='n'&gt;buyer&lt;/span&gt;
&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='p'&gt;(&lt;/span&gt;
	&lt;span class='k'&gt;SELECT&lt;/span&gt;
	&lt;span class='nf'&gt;MONTH&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;dt&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='n'&gt;user_id&lt;/span&gt;
	&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='n'&gt;events_005&lt;/span&gt;
	&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='n'&gt;visitors&lt;/span&gt;
&lt;span class='k'&gt;LEFT&lt;/span&gt; &lt;span class='k'&gt;JOIN&lt;/span&gt; &lt;span class='p'&gt;(&lt;/span&gt;
	&lt;span class='k'&gt;SELECT&lt;/span&gt;
	&lt;span class='nf'&gt;MONTH&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;dt&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='mi'&gt;1&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;buyer&lt;/span&gt;
	&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='n'&gt;events_005&lt;/span&gt;
	&lt;span class='k'&gt;WHERE&lt;/span&gt; &lt;span class='n'&gt;event&lt;/span&gt;&lt;span class='o'&gt;=&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;transaction&amp;#39;&lt;/span&gt;
	&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;user_id&lt;/span&gt; &lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='n'&gt;buyers&lt;/span&gt;
&lt;span class='k'&gt;ON&lt;/span&gt; &lt;span class='n'&gt;visitors&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='ss'&gt;`month`&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='n'&gt;buyers&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='ss'&gt;`month`&lt;/span&gt;
&lt;span class='k'&gt;AND&lt;/span&gt; &lt;span class='n'&gt;visitors&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='n'&gt;buyers&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt;
&lt;span class='k'&gt;ORDER&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='n'&gt;visitors&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;visitors&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now we can aggregate over the results of the above query, calculating the conversion rate by dividing the number of buyers by the total number of visitors:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='mysql'&gt;&lt;span class='k'&gt;SELECT&lt;/span&gt;
&lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='ss'&gt;`converted_visitors`&lt;/span&gt; &lt;span class='o'&gt;/&lt;/span&gt; &lt;span class='n'&gt;visitors&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;conversion_rate&lt;/span&gt;
&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='p'&gt;(&lt;/span&gt;
	&lt;span class='k'&gt;SELECT&lt;/span&gt;
	&lt;span class='n'&gt;visitors&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='n'&gt;visitors&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='n'&gt;buyer&lt;/span&gt;
	&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='p'&gt;(&lt;/span&gt;
		&lt;span class='k'&gt;SELECT&lt;/span&gt;
		&lt;span class='nf'&gt;MONTH&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;dt&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
		&lt;span class='n'&gt;user_id&lt;/span&gt;
		&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='n'&gt;events_005&lt;/span&gt;
		&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='n'&gt;visitors&lt;/span&gt;
	&lt;span class='k'&gt;LEFT&lt;/span&gt; &lt;span class='k'&gt;JOIN&lt;/span&gt; &lt;span class='p'&gt;(&lt;/span&gt;
		&lt;span class='k'&gt;SELECT&lt;/span&gt;
		&lt;span class='nf'&gt;MONTH&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;dt&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
		&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
		&lt;span class='mi'&gt;1&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;buyer&lt;/span&gt;
		&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='n'&gt;events_005&lt;/span&gt;
		&lt;span class='k'&gt;WHERE&lt;/span&gt; &lt;span class='n'&gt;event&lt;/span&gt;&lt;span class='o'&gt;=&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;transaction&amp;#39;&lt;/span&gt;
		&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;user_id&lt;/span&gt; &lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='n'&gt;buyers&lt;/span&gt;
	&lt;span class='k'&gt;ON&lt;/span&gt; &lt;span class='n'&gt;visitors&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='ss'&gt;`month`&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='n'&gt;buyers&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='ss'&gt;`month`&lt;/span&gt;
	&lt;span class='k'&gt;AND&lt;/span&gt; &lt;span class='n'&gt;visitors&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='n'&gt;buyers&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt;
	&lt;span class='k'&gt;ORDER&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='n'&gt;visitors&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;visitors&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt;
&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='n'&gt;e&lt;/span&gt;
&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Pop the above query in the ChartIO query box:&lt;/p&gt;

&lt;p&gt;&lt;img alt='chartio-pic-4' src='/static/img/blog/2013/01/chartio-4.png' /&gt;&lt;/p&gt;

&lt;p&gt;and click the &lt;strong&gt;Chart Query&lt;/strong&gt; button below. ChartIO will respond with a table of data. We can graph the data by clicking on any of the graph icons above the data table. Choosing the line graph, I get:&lt;/p&gt;

&lt;p&gt;&lt;img alt='chartio-pic-6' src='/static/img/blog/2013/01/chartio-6.png' /&gt;&lt;/p&gt;

&lt;p&gt;We can then rename the graph (by clicking the &lt;strong&gt;edit&lt;/strong&gt; hyperlink that appears when you hover over &lt;strong&gt;Chart Title&lt;/strong&gt;) and save the graph to our dashboard by clicking &lt;strong&gt;Save to Exploring engagement&lt;/strong&gt; button. ChartIO lets us resize and position the graph on the dashboard:&lt;/p&gt;

&lt;p&gt;&lt;img alt='chartio-pic-7' src='/static/img/blog/2013/01/chartio-7.png' /&gt;&lt;/p&gt;

&lt;p&gt;Great! We can see conversion rates were reasonably stable between September and November of the year, but peaked at the end of the year at a height they were previously in June. The figure for September seems suspiciously high - we&amp;#8217;ll drill into this in more detail in a bit. Next we will plot our alternative measure of engagement over time: the number of pageviews per user per month, and see how that has changed over time.&lt;/p&gt;

&lt;p&gt;Calculating the number of pageviews per user per month is straightforward in Snowplow - we can use the following query:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='mysql'&gt;&lt;span class='k'&gt;SELECT&lt;/span&gt;
&lt;span class='nf'&gt;MONTH&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;dt&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='nf'&gt;count&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='o'&gt;*&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;page_views&lt;/span&gt;
&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='n'&gt;events_005&lt;/span&gt;
&lt;span class='k'&gt;WHERE&lt;/span&gt; &lt;span class='n'&gt;event&lt;/span&gt;&lt;span class='o'&gt;=&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;page_view&amp;#39;&lt;/span&gt;
&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;user_id&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now we want to aggregate users by the number of pageviews each has done by month:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='mysql'&gt;&lt;span class='k'&gt;SELECT&lt;/span&gt;
&lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='n'&gt;page_views&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='nf'&gt;count&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='k'&gt;distinct&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;))&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;uniques&lt;/span&gt;
&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='p'&gt;(&lt;/span&gt;
	&lt;span class='k'&gt;SELECT&lt;/span&gt;
	&lt;span class='nf'&gt;MONTH&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;dt&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='nf'&gt;count&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='o'&gt;*&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;page_views&lt;/span&gt;
	&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='n'&gt;events_005&lt;/span&gt;
	&lt;span class='k'&gt;WHERE&lt;/span&gt; &lt;span class='n'&gt;event&lt;/span&gt;&lt;span class='o'&gt;=&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;page_view&amp;#39;&lt;/span&gt;
	&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;user_id&lt;/span&gt;
&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='n'&gt;t&lt;/span&gt;
&lt;span class='k'&gt;ORDER&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt; &lt;span class='k'&gt;ASC&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;page_views&lt;/span&gt; &lt;span class='k'&gt;DESC&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Lastly we want to bucket values of page views e.g. into 1, 2-5, 6-10, 11-25 and 25+. We can introduce a bucketing into our previous query:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='mysql'&gt;&lt;span class='k'&gt;SELECT&lt;/span&gt;
&lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='n'&gt;page_views&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='k'&gt;CASE&lt;/span&gt;
	&lt;span class='k'&gt;WHEN&lt;/span&gt; &lt;span class='n'&gt;page_views&lt;/span&gt; &lt;span class='o'&gt;&amp;gt;&lt;/span&gt; &lt;span class='mi'&gt;25&lt;/span&gt; &lt;span class='k'&gt;THEN&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;25+&amp;#39;&lt;/span&gt;
	&lt;span class='k'&gt;WHEN&lt;/span&gt; &lt;span class='n'&gt;page_views&lt;/span&gt; &lt;span class='o'&gt;&amp;gt;&lt;/span&gt; &lt;span class='mi'&gt;10&lt;/span&gt; &lt;span class='k'&gt;THEN&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;11-25&amp;#39;&lt;/span&gt;
	&lt;span class='k'&gt;WHEN&lt;/span&gt; &lt;span class='n'&gt;page_views&lt;/span&gt; &lt;span class='o'&gt;&amp;gt;&lt;/span&gt;  &lt;span class='mi'&gt;5&lt;/span&gt; &lt;span class='k'&gt;THEN&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;6-10&amp;#39;&lt;/span&gt;
	&lt;span class='k'&gt;WHEN&lt;/span&gt; &lt;span class='n'&gt;page_views&lt;/span&gt; &lt;span class='o'&gt;&amp;gt;&lt;/span&gt;  &lt;span class='mi'&gt;1&lt;/span&gt; &lt;span class='k'&gt;THEN&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;2-5&amp;#39;&lt;/span&gt;
	&lt;span class='k'&gt;ELSE&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;1&amp;#39;&lt;/span&gt;
&lt;span class='n'&gt;END&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;bucket&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; 
&lt;span class='nf'&gt;count&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='k'&gt;distinct&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;))&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;uniques&lt;/span&gt;
&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='p'&gt;(&lt;/span&gt;
	&lt;span class='k'&gt;SELECT&lt;/span&gt;
	&lt;span class='nf'&gt;MONTH&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;dt&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='nf'&gt;count&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='o'&gt;*&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;page_views&lt;/span&gt;
	&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='n'&gt;events_005&lt;/span&gt;
	&lt;span class='k'&gt;WHERE&lt;/span&gt; &lt;span class='n'&gt;event&lt;/span&gt;&lt;span class='o'&gt;=&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;page_view&amp;#39;&lt;/span&gt;
	&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;user_id&lt;/span&gt;
&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='n'&gt;t&lt;/span&gt;
&lt;span class='k'&gt;ORDER&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt; &lt;span class='k'&gt;ASC&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;page_views&lt;/span&gt; &lt;span class='k'&gt;DESC&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And then aggregate by bucket in the next query:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='mysql'&gt;&lt;span class='k'&gt;SELECT&lt;/span&gt;
&lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='n'&gt;bucket&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='nf'&gt;sum&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;uniques&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;uniques&lt;/span&gt;
&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='p'&gt;(&lt;/span&gt;
	&lt;span class='k'&gt;SELECT&lt;/span&gt;
	&lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='n'&gt;page_views&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='k'&gt;CASE&lt;/span&gt;
		&lt;span class='k'&gt;WHEN&lt;/span&gt; &lt;span class='n'&gt;page_views&lt;/span&gt; &lt;span class='o'&gt;&amp;gt;&lt;/span&gt; &lt;span class='mi'&gt;25&lt;/span&gt; &lt;span class='k'&gt;THEN&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;25+&amp;#39;&lt;/span&gt;
		&lt;span class='k'&gt;WHEN&lt;/span&gt; &lt;span class='n'&gt;page_views&lt;/span&gt; &lt;span class='o'&gt;&amp;gt;&lt;/span&gt; &lt;span class='mi'&gt;10&lt;/span&gt; &lt;span class='k'&gt;THEN&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;11-25&amp;#39;&lt;/span&gt;
		&lt;span class='k'&gt;WHEN&lt;/span&gt; &lt;span class='n'&gt;page_views&lt;/span&gt; &lt;span class='o'&gt;&amp;gt;&lt;/span&gt;  &lt;span class='mi'&gt;5&lt;/span&gt; &lt;span class='k'&gt;THEN&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;06-10&amp;#39;&lt;/span&gt;
		&lt;span class='k'&gt;WHEN&lt;/span&gt; &lt;span class='n'&gt;page_views&lt;/span&gt; &lt;span class='o'&gt;&amp;gt;&lt;/span&gt;  &lt;span class='mi'&gt;1&lt;/span&gt; &lt;span class='k'&gt;THEN&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;02-05&amp;#39;&lt;/span&gt;
		&lt;span class='k'&gt;ELSE&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;01&amp;#39;&lt;/span&gt;
	&lt;span class='n'&gt;END&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;bucket&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; 
	&lt;span class='nf'&gt;count&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='k'&gt;distinct&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;))&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;uniques&lt;/span&gt;
	&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='p'&gt;(&lt;/span&gt;
		&lt;span class='k'&gt;SELECT&lt;/span&gt;
		&lt;span class='nf'&gt;MONTH&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;dt&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
		&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
		&lt;span class='nf'&gt;count&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='o'&gt;*&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;page_views&lt;/span&gt;
		&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='n'&gt;events_005&lt;/span&gt;
		&lt;span class='k'&gt;WHERE&lt;/span&gt; &lt;span class='n'&gt;event&lt;/span&gt;&lt;span class='o'&gt;=&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;page_view&amp;#39;&lt;/span&gt;
		&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;user_id&lt;/span&gt;
	&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='n'&gt;t&lt;/span&gt;
	&lt;span class='k'&gt;ORDER&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt; &lt;span class='k'&gt;ASC&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;page_views&lt;/span&gt; &lt;span class='k'&gt;DESC&lt;/span&gt;
&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='n'&gt;u&lt;/span&gt;
&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;bucket&lt;/span&gt;
&lt;span class='k'&gt;ORDER&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;bucket&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Create a new chart in ChartIO using the above query and graph it using the first bar chart icon. Give the graph a suitable name:&lt;/p&gt;

&lt;p&gt;&lt;img alt='chartio-pic-8' src='/static/img/blog/2013/01/chartio-8.png' /&gt;&lt;/p&gt;

&lt;p&gt;This graph tells an interesting story. Overall, the number of unique visitors per month has grown pretty dramatically over time, peaking at about 1700 uniques in November. It is not so easy to tell how the distribution of users by engagement level has changed over time: this is easier if we change the graph to be a &amp;#8220;percent bar&amp;#8221;:&lt;/p&gt;

&lt;p&gt;&lt;img alt='chartio-pic-9' src='/static/img/blog/2013/01/chartio-9.png' /&gt;&lt;/p&gt;

&lt;p&gt;This graph suggests that engagement levels dropped in October, but climbed dramatically from then to December. Curiously, there was no drop in overall engagement level as user numbers increased on the site between August and October: that means that the new users acquired were &amp;#8220;high quality&amp;#8221; or &amp;#8220;highly engaged&amp;#8221;. This is a useful graph: let&amp;#8217;s add it to our dashboard alongside the first graph we created:&lt;/p&gt;

&lt;p&gt;&lt;img alt='chartio-pic-10' src='/static/img/blog/2013/01/chartio-10.png' /&gt;&lt;/p&gt;

&lt;p&gt;Just to put the two baseline graphs in context, let&amp;#8217;s add a third graph the tracks the number of unique users per month to our dashboard. Add a new chart using the following simple query:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='mysql'&gt;&lt;span class='k'&gt;SELECT&lt;/span&gt;
&lt;span class='nf'&gt;MONTH&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;dt&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='nf'&gt;COUNT&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='k'&gt;DISTINCT&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;))&lt;/span&gt; &lt;span class='k'&gt;as&lt;/span&gt; &lt;span class='n'&gt;uniques&lt;/span&gt;
&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='n'&gt;events_004&lt;/span&gt;
&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt;
&lt;span class='k'&gt;ORDER&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And pop it on the dashboard:&lt;/p&gt;

&lt;p&gt;&lt;img alt='chartio-pic-11' src='/static/img/blog/2013/01/chartio-11.png' /&gt;&lt;/p&gt;

&lt;p&gt;Our baseline data tells us an interesting story, which from the dashboard, we&amp;#8217;re in a position to summarise:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Overall, visitor numbers have increased pretty dramatically between June and September&lt;/li&gt;

&lt;li&gt;Over that same period, there was no corresponding drop in engagement, in terms of numbers of page views by visit. If anything, there was a slight increase&lt;/li&gt;

&lt;li&gt;Looking at conversion rates over the same time, the picture is much more hairy. (With a surprising spike in July.) In fact, we know this was to do with a bug on the website, which prevented data being collected from any page apart from the checkout page. Hence user numbers are underreported, but conversion rates are overstated, for July&lt;/li&gt;

&lt;li&gt;Conversion rates and average page views per visit rise in December&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id='unpicking_the_drivers_of_changing_engagement_levels'&gt;Unpicking the drivers of changing engagement levels&lt;/h3&gt;

&lt;p&gt;For our sample data set there appears to be a rather interesting rise in engagement level (as measured by both conversion rates and page views by month) between November and December. What&amp;#8217;s driving that increase? What clues can our Snowplow data give us?&lt;/p&gt;

&lt;p&gt;We can divide drivers into two groups: those that effect all users on our website, and those that only effect some of them. If, for example, we performed a measure rearchitecture of our entire site, that is likely to effect &lt;strong&gt;all&lt;/strong&gt; users&amp;#8217; behaviour. But if we upgraded the site for mobile, then we would &lt;strong&gt;only&lt;/strong&gt; expect that to impact user behaviour for people browsing from mobile sites.&lt;/p&gt;

&lt;p&gt;A good approach, then, to unpick what&amp;#8217;s driving growth in engagement levels is to see if this growth is consistent across all users, or just some of them. One easy way to do this is to compare engagement rates between different types of users, to see if we can spot a difference. It makes sense to start off with factors we have a hunch might be driving those changes (e.g. because we&amp;#8217;re familiar with what has changed at those business over the months in question.) To give a specific examples:&lt;/p&gt;

&lt;h4 id='comparing_engagement_levels_between_users_from_paid_search_campaigns_and_notpaid_search_campaigns'&gt;Comparing engagement levels between users from paid search campaigns and not-paid search campaigns&lt;/h4&gt;

&lt;p&gt;Psychic Bazaar&amp;#8217;s only direct marketing spend is on paid search campaigns on Google and Bing. We might therefore wonder whether a change to those campaigns drove the uplift in engagement we see on the site in September. To do this, first we need to identify all the users acquired via paid search:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='mysql'&gt;&lt;span class='k'&gt;SELECT&lt;/span&gt;
&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='s1'&gt;&amp;#39;1&amp;#39;&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;paid_search&lt;/span&gt;
&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='n'&gt;events_005&lt;/span&gt;
&lt;span class='k'&gt;WHERE&lt;/span&gt; &lt;span class='n'&gt;mkt_medium&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;cpc&amp;#39;&lt;/span&gt;
&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='n'&gt;user_id&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Compare this with our data on which users have converted:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='mysql'&gt;&lt;span class='k'&gt;SELECT&lt;/span&gt;
&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='s1'&gt;&amp;#39;1&amp;#39;&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;buyer&lt;/span&gt;
&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='n'&gt;events_005&lt;/span&gt;
&lt;span class='k'&gt;WHERE&lt;/span&gt; &lt;span class='n'&gt;event&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='n'&gt;transaction&lt;/span&gt;
&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='n'&gt;user_id&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And our list of &lt;strong&gt;all&lt;/strong&gt; users:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='mysql'&gt;&lt;span class='k'&gt;SELECT&lt;/span&gt;
&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='s1'&gt;&amp;#39;1&amp;#39;&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;visitor&lt;/span&gt;
&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='n'&gt;events_005&lt;/span&gt;
&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='n'&gt;user_id&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We join the three data sets:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='mysql'&gt;&lt;span class='k'&gt;SELECT&lt;/span&gt;
&lt;span class='n'&gt;visitors&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='n'&gt;buyer&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='n'&gt;paid_search&lt;/span&gt;
&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='p'&gt;(&lt;/span&gt;
	&lt;span class='k'&gt;SELECT&lt;/span&gt;
	&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='s1'&gt;&amp;#39;1&amp;#39;&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;visitor&lt;/span&gt;
	&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='n'&gt;events_005&lt;/span&gt;
	&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='n'&gt;user_id&lt;/span&gt;
	&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='n'&gt;visitors&lt;/span&gt;
&lt;span class='k'&gt;LEFT&lt;/span&gt; &lt;span class='k'&gt;JOIN&lt;/span&gt; &lt;span class='p'&gt;(&lt;/span&gt;
	&lt;span class='k'&gt;SELECT&lt;/span&gt;
	&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='s1'&gt;&amp;#39;1&amp;#39;&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;buyer&lt;/span&gt;
	&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='n'&gt;events_005&lt;/span&gt;
	&lt;span class='k'&gt;WHERE&lt;/span&gt; &lt;span class='n'&gt;event&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='n'&gt;transaction&lt;/span&gt;
	&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='n'&gt;user_id&lt;/span&gt;
&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='n'&gt;buyers&lt;/span&gt;
&lt;span class='k'&gt;ON&lt;/span&gt; &lt;span class='n'&gt;visitors&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='n'&gt;buyers&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt;
&lt;span class='k'&gt;LEFT&lt;/span&gt; &lt;span class='k'&gt;JOIN&lt;/span&gt; &lt;span class='p'&gt;(&lt;/span&gt;
	&lt;span class='k'&gt;SELECT&lt;/span&gt;
	&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='s1'&gt;&amp;#39;1&amp;#39;&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;paid_search&lt;/span&gt;
	&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='n'&gt;events_005&lt;/span&gt;
	&lt;span class='k'&gt;WHERE&lt;/span&gt; &lt;span class='n'&gt;mkt_medium&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;cpc&amp;#39;&lt;/span&gt;
	&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='n'&gt;user_id&lt;/span&gt;
	&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='n'&gt;paid_search&lt;/span&gt;
&lt;span class='k'&gt;ON&lt;/span&gt; &lt;span class='n'&gt;visitors&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='n'&gt;paid_search&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And then aggregate over the result set to compare conversion rates:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='mysql'&gt;&lt;span class='k'&gt;SELECT&lt;/span&gt;
&lt;span class='n'&gt;paid_search&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='nf'&gt;sum&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;buyer&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt;&lt;span class='o'&gt;/&lt;/span&gt;&lt;span class='nf'&gt;sum&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;visitor&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;conversion_rate&lt;/span&gt;
&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='p'&gt;(&lt;/span&gt;
&lt;span class='k'&gt;SELECT&lt;/span&gt;
&lt;span class='n'&gt;visitors&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='n'&gt;buyer&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='n'&gt;paid_search&lt;/span&gt;
&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='p'&gt;(&lt;/span&gt;
		&lt;span class='k'&gt;SELECT&lt;/span&gt;
		&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
		&lt;span class='s1'&gt;&amp;#39;1&amp;#39;&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;visitor&lt;/span&gt;
		&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='n'&gt;events_005&lt;/span&gt;
		&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='n'&gt;user_id&lt;/span&gt;
		&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='n'&gt;visitors&lt;/span&gt;
	&lt;span class='k'&gt;LEFT&lt;/span&gt; &lt;span class='k'&gt;JOIN&lt;/span&gt; &lt;span class='p'&gt;(&lt;/span&gt;
		&lt;span class='k'&gt;SELECT&lt;/span&gt;
		&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
		&lt;span class='s1'&gt;&amp;#39;1&amp;#39;&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;buyer&lt;/span&gt;
		&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='n'&gt;events_005&lt;/span&gt;
		&lt;span class='k'&gt;WHERE&lt;/span&gt; &lt;span class='n'&gt;event&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='n'&gt;transaction&lt;/span&gt;
		&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='n'&gt;user_id&lt;/span&gt;
	&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='n'&gt;buyers&lt;/span&gt;
	&lt;span class='k'&gt;ON&lt;/span&gt; &lt;span class='n'&gt;visitors&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='n'&gt;buyers&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt;
	&lt;span class='k'&gt;LEFT&lt;/span&gt; &lt;span class='k'&gt;JOIN&lt;/span&gt; &lt;span class='p'&gt;(&lt;/span&gt;
		&lt;span class='k'&gt;SELECT&lt;/span&gt;
		&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
		&lt;span class='s1'&gt;&amp;#39;1&amp;#39;&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;paid_search&lt;/span&gt;
		&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='n'&gt;events_005&lt;/span&gt;
		&lt;span class='k'&gt;WHERE&lt;/span&gt; &lt;span class='n'&gt;mkt_medium&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;cpc&amp;#39;&lt;/span&gt;
		&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='n'&gt;user_id&lt;/span&gt;
		&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='n'&gt;paid_search&lt;/span&gt;
	&lt;span class='k'&gt;ON&lt;/span&gt; &lt;span class='n'&gt;visitors&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='n'&gt;paid_search&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt;
	&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='n'&gt;t&lt;/span&gt;
&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='n'&gt;paid_search&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Plotting the data in ChartIO we can see that users acquired from paid campaigns are much more likely to convert:&lt;/p&gt;

&lt;p&gt;&lt;img alt='chartio-pic-12' src='/static/img/blog/2013/01/chartio-12.png' /&gt;&lt;/p&gt;

&lt;p&gt;This naturally leads to the question: has the number of users acquired from paid search increased over the time period? (Especially between November and December, when our increase in conversion rates is most noticeable?) We can find out by graphing the following query, which looks at the number of uniques by month divided by whether they were acquired by paid search or not:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='mysql'&gt;&lt;span class='k'&gt;SELECT&lt;/span&gt;
&lt;span class='nf'&gt;MONTH&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;dt&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='n'&gt;paid_search&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='nf'&gt;COUNT&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='k'&gt;DISTINCT&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;e&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;))&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;uniques&lt;/span&gt;
&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='n'&gt;events_005&lt;/span&gt; &lt;span class='n'&gt;e&lt;/span&gt;
&lt;span class='k'&gt;LEFT&lt;/span&gt; &lt;span class='k'&gt;JOIN&lt;/span&gt; &lt;span class='p'&gt;(&lt;/span&gt;
	&lt;span class='k'&gt;SELECT&lt;/span&gt;
	&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='s1'&gt;&amp;#39;1&amp;#39;&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;paid_search&lt;/span&gt;
	&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='n'&gt;events_005&lt;/span&gt;
	&lt;span class='k'&gt;WHERE&lt;/span&gt; &lt;span class='n'&gt;mkt_medium&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;cpc&amp;#39;&lt;/span&gt;
	&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='n'&gt;user_id&lt;/span&gt;
&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='n'&gt;paid_search&lt;/span&gt;
&lt;span class='k'&gt;ON&lt;/span&gt; &lt;span class='n'&gt;e&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='n'&gt;paid_search&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt;
&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;paid_search&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Plotting the above graph shows that growth in paid search traffic accounts for some of the growth in traffic volumes between July and September. However, there was &lt;strong&gt;no&lt;/strong&gt; increase in traffic from paid search terms between November and December, so this does &lt;strong&gt;not&lt;/strong&gt; account for the rising conversion rate in December.&lt;/p&gt;

&lt;p&gt;&lt;img alt='chartio-pic-13' src='/static/img/blog/2013/01/chartio-13.png' /&gt;&lt;/p&gt;

&lt;h4 id='other_factors_that_might_account_for_the_rise'&gt;Other factors that might account for the rise&lt;/h4&gt;

&lt;p&gt;There is a wealth of other factors that we can explore using Snowplow data, to see if they account for the rise in engagement levels / conversion rates in December. Doing so is beyond the scope of this blog post. However, we can outline them:&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;&lt;strong&gt;Factor&lt;/strong&gt;&lt;/th&gt;&lt;th&gt;&lt;strong&gt;How we would test it&lt;/strong&gt;&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Improvement to the site structure (e.g. homepage)&lt;/td&gt;&lt;td style='text-align: left;'&gt;Investigate how engagement levels vary by visit by landing page, and see if those changes by landing page on dates when those web pages were updated&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Improvements to the checkout flow&lt;/td&gt;&lt;td style='text-align: left;'&gt;Compare the conversion funnel between November and December - see if there&amp;#8217;s a specific point in the funnel where conversion rates change, and see if that change can be attributed to a development change&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;A change in the makeup of the users e.g. so that in December, a bigger portion of the userbase are repeat visitors&lt;/td&gt;&lt;td style='text-align: left;'&gt;Explore whether there is a change in makeup (e.g. more repeat visitors as a proportion of uniques) and see if there&amp;#8217;s a corresponding difference in conversion rates by different types of users (e.g. new vs returning). Note: this is the same approach as described above for user acquired from &lt;em&gt;paid search&lt;/em&gt;.&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Christmas&lt;/td&gt;&lt;td style='text-align: left;'&gt;Hard to prove definitively - but if no other factor can be identified, and the engagement level drops back in January, then the December bump might be season.&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;Snowplow makes it possible to drill into all of the above, and other factors we can think of, to see which is responsible for driving changing engagement levels.&lt;/p&gt;

&lt;h2 id='summarising_our_thoughts_on_chartio'&gt;Summarising our thoughts on ChartIO&lt;/h2&gt;

&lt;p&gt;From our experience with it in the last couple of weeks, we believe that ChartIO is an excellent tool for visualising Snowplow data. We highly recommend Snowplow users give it a try,: ChartIO&amp;#8217;s simplicitly, speed, and lack of assumptions about the way data is structured make it an ideal analytics tool to run directly on top of Snowplow data stored in Infobright.&lt;/p&gt;

&lt;p&gt;We&amp;#8217;re going to continue to use ChartIO (and blog about the results). We&amp;#8217;d love to hear from other Snowplow users who are using it.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/01/07/the-clojure-collector-in-detail</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/01/07/the-clojure-collector-in-detail"/>
    <title>Understanding the thinking behind the Clojure Collector, and mapping out its development going forwards</title>
    <updated>2013-01-07T00:00:00+00:00</updated>
    <author>
      <name>yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;Last week we released &lt;a href='/blog/2013/01/03/snowplow-0.7.0-released/'&gt;Snowplow 0.7.0&lt;/a&gt;: which included a new Clojure Collector, with some significant new functionality for content networks and ad networks in particular. In this post we explain a lot of the thinking behind the Clojure Collector architecture, before taking a look ahead at the short and long-term development roadmap for the collector.&lt;/p&gt;

&lt;p&gt;This is the first in a series of posts we write where describe in some detail the thinking behind the architecture and design of Snowplow components, and discuss how we plan to develop those components over time. The purpose of doing so is to engage people like yourself: developers and analysts in the Snowplow community, in a discussion about how best to evolve Snowplow. The reasoning is simple: we have had many fantastic ideas and contributions from community members that have proved invaluable in driving Snowplow development, and we want to encourage more of these conversations and contributions, to help make Snowplow great.&lt;/p&gt;

&lt;p&gt;&lt;img alt='engine' src='/static/img/blog/2013/01/engine.jpg' /&gt;&lt;/p&gt;

&lt;h2 id='contents'&gt;Contents&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href='/blog/2013/01/07/the-clojure-collector-in-detail#biz-case'&gt;The business case for a new collector: understanding the limitations of the Cloudfront Collector&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/01/07/the-clojure-collector-in-detail#under-the-hood'&gt;Under the hood: the design decisions behind the Clojure Collector&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/01/07/the-clojure-collector-in-detail#short-term-roadmap'&gt;Moving forwards: short term Clojure Collector roadmap&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/01/07/the-clojure-collector-in-detail#long-term-roadmap'&gt;Looking ahead: long term collector roadmap&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;!--more--&gt;&lt;h2&gt;&lt;a name='biz-case'&gt;1. The business case for a new collector: understanding the limitations of the Cloudfront Collector&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;We launched Snowplow with the &lt;a href='https://github.com/snowplow/snowplow/wiki/setting-up-the-cloudfront-collector'&gt;Cloudfront Collector&lt;/a&gt;. The Cloudfront Collector is simple:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The Snowplow tracking pixel is served from Amazon Cloudfront&lt;/li&gt;

&lt;li&gt;Cloudfront logging is switched on (so that every time the pixel is fetched by a Snowplow tracking tag, the request is logged).&lt;/li&gt;

&lt;li&gt;Events and associated data points we want to capture are stored as name / value pairs and appended to the query string for the tracking pixel, so that they are automatically logged.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The Cloudfront Collector is so simple that many people express surprise that there are so few files in the appropriate section of the &lt;a href='https://github.com/snowplow/snowplow/tree/master/2-collectors/cloudfront-collector'&gt;Snowplow repo&lt;/a&gt;. (It&amp;#8217;s just a &lt;code&gt;readme&lt;/code&gt; and the tracking pixel.) In spite of that simplicity, however, the Cloudfront Collector boasts two key strengths:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Simplicity&lt;/strong&gt;. Simplicity is a strength: because it has no moving parts, the Cloudfront Collector is incredibly robust. It makes no decisions. All it does is faithfully log requests made to the tracking pixel. There is very little that can go wrong with it. (Nothing if Cloudfront stays live.)&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Scalability&lt;/strong&gt;. By using Amazon&amp;#8217;s content distribution network (CDN) to serve the tracking pixel and log requests for the tracking pixel, we can be confident that businesses using the Cloudfront Collector will be able to comfortably track millions of requests per hour. Amazon&amp;#8217;s CDN has been designed to be elastic: it responds automatically to spikes in demand, so you can be confident that even in during peak demand periods, all your data will successfully be captured and stored.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Nonetheless, there are two major limitations to the Cloudfront Collector:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Unable to track users across domains&lt;/strong&gt;. Because Snowplow has been designed to be scalable, we&amp;#8217;ve had a lot of interest in it from media groups, content networks and ad networks. All of these companies want to track individual users across multiple websites. This is not directly supported by the Cloudfront Collector: because it has no moving parts, user identification has to be performed client side, by the &lt;a href='https://github.com/snowplow/snowplow/wiki/javascript-tracker-setup'&gt;Javascript tracker&lt;/a&gt; using first party cookies. As a result, &lt;code&gt;user_id&lt;/code&gt;s that are set on one domain cannot be accessed on another domain, even if both domains are owned and operated by the same company.&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Not real-time&lt;/strong&gt;. Cloudfront log files typically appear in S3 3-4 hours after the requests logged were made. As a result, if you rely on the Cloudfront cCollector for your web analytics data, you will always be looking at data that is at least 3-4 hours old.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The Clojure Collector explicitly addresses the first issue identified above: it has a single moving part, which checks if a &lt;code&gt;user_id&lt;/code&gt; has been set for this user: if so, it logs that &lt;code&gt;user_id&lt;/code&gt;. If not, it sets a &lt;code&gt;user_id&lt;/code&gt; (server side), and stores that &lt;code&gt;user_id&lt;/code&gt; in a cookie on the collectors own domain, accessible from any website running Snowplow that uses the same collector.&lt;/p&gt;

&lt;p&gt;The Clojure Collector does not explicitly address the second issue related to the speed at which data is logged for analysis. Although it logs data faster than the Cloudfront Collector (logs are rotated to S3 hourly), this is still not fast enough for real time analysis. However, it is fast enough that the processing bottleneck shifts from the collector to the ETL step: this is something we plan on addressing in the near future. (More on this &lt;a href='#long-term-roadmap'&gt;later&lt;/a&gt;.)&lt;/p&gt;
&lt;h2&gt;&lt;a name='under-the-hood'&gt;2. Under the hood: the design decisions behind the Clojure Collector&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;We made three important design decisions when building the Clojure Collector:&lt;/p&gt;

&lt;h3 id='1_built_for_elastic_beanstalk'&gt;1. Built for Elastic Beanstalk&lt;/h3&gt;

&lt;p&gt;We built the Clojure Collector specifically for Elastic Beanstalk. This has a number of important advantages:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Comfortably scales to handle spikes in demand&lt;/strong&gt;. Elastic Beanstalk is &lt;strong&gt;elastic&lt;/strong&gt; in the same way as Cloudfront is &lt;strong&gt;elastic&lt;/strong&gt;. It makes it easy to scale services to handle spikes in demand, which is crucial if we&amp;#8217;re going to continue to track events data during spikes in service usage.&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Automatic logging to S3&lt;/strong&gt;. Elastic Beanstalk supports a configuration option that automatically rotates Tomcat access logs to Elastic Beanstalk hourly. By using this feature, we were able to save ourselves having to build build a process to manage that log rotation, and save our users the hassle of installing and maintaining the process.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Amazon Elastic Beanstalk supports open source applications built for the JVM, or PHP, Python and Ruby web apps. Of the four, it was clear that JVM was the most performative platform to build a Collector in.&lt;/p&gt;

&lt;h3 id='2_minimal_moving_parts'&gt;2. Minimal moving parts&lt;/h3&gt;

&lt;p&gt;The Clojure collector &lt;em&gt;only&lt;/em&gt; sets &lt;code&gt;user_id&lt;/code&gt;s and expiry dates on those &lt;code&gt;user_id&lt;/code&gt;s. It does &lt;em&gt;nothing&lt;/em&gt; else: keeping it as simple as possible.&lt;/p&gt;

&lt;h3 id='3_log_files_formats_match_those_produced_by_cloudfront'&gt;3. Log files formats match those produced by Cloudfront&lt;/h3&gt;

&lt;p&gt;The least wieldy part of the Snowplow stack today is the &lt;a href='https://github.com/snowplow/snowplow/wiki/choosing-an-etl-module'&gt;ETL step&lt;/a&gt;. This parses the log files produced by the collector, extracts the relevant data points and loads them into S3 for processing by Hadoop/Hive and/or Infobright for processing in a wide range of tools e.g. &lt;a href='http://chartio.com'&gt;ChartIO&lt;/a&gt; or &lt;a href='http://www.r-project.org/'&gt;R&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We have plans to replace the current &lt;a href='https://github.com/snowplow/snowplow/wiki/hive-etl-setup'&gt;Hive-based ETL process&lt;/a&gt; with an all new process based on &lt;a href='https://github.com/twitter/scalding'&gt;Scalding&lt;/a&gt;. (More on this in the next blog post in this series.) In the meantime, however, we did not want to have to write a new Hive deserializer to parse log files that match a new format: instead, we customised Tomcat in the Clojure Collector to output log files that matched the Cloudfront logging format. (This involved writing a custom &lt;a href='[tomcat-cf-access-valve]'&gt;Tomact Access Valve&lt;/a&gt; and tailoring &lt;a href='https://github.com/snowplow/snowplow/blob/master/2-collectors/clojure-collector/war-resources/.ebextensions/server.xml'&gt;Tomcat&amp;#8217;s server.xml&lt;/a&gt;.) As a result, the new Clojure Collector plays well with the existing ETL process.&lt;/p&gt;
&lt;h2&gt;&lt;a name='short-term-roadmap'&gt;3. Moving forwards: short term Clojure Collector roadmap&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;This is the initial release of the Clojure Collector. If it will be deployed by large media companies, content networks and ad networks, it is important that we learn how to configure it to function well at scale. To this end, we are looking for help, from members of the Snowplow community (particularly those with an interest in tracking users across domains), to help with the following:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Load testing the collector. Test how fast the collector responds to increasing number of requests per second, and how this varies by the size of instance offered by Amazon. (E.g. how does the curve differ for an m1.small instance than an m1.large instance?) It should be possible to use a tool like &lt;a href='http://www.joedog.org/siege-home/'&gt;Siege&lt;/a&gt; or &lt;a href='http://httpd.apache.org/docs/2.2/programs/ab.html'&gt;Apache Bench&lt;/a&gt; to test response levels and response times at increasing levels of request concurrency, and plot one against the other.&lt;/li&gt;

&lt;li&gt;On the basis of the above, working out the optimal way of setting up the Clojure Collector on Elastic Beanstalk. It would be good to answer two questions in particular: what size instance is it most cost effective to use, and what should trigger the starting up of an additional instance to cope with a spike in traffic? Amazon makes it possible to specify custom KPI to use to trigger scaling of services on Elastic Beanstalk, and it may be that doing so results in much improved performance and reliability from the Collector.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Because we haven&amp;#8217;t been able to perform the above tests to date, we&amp;#8217;re still calling the Clojure Collector an experimental release, adn recommend that companies using it in production run it alongside the Cloudfront Collector.&lt;/p&gt;
&lt;h2&gt;&lt;a name='long-term-roadmap'&gt;4. Looking ahead: long term collector roadmap&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Long term we need to move the whole Snowplow so that it&amp;#8217;s processing data faster, closer to real-time. This primarily means moving the &lt;a href='https://github.com/snowplow/snowplow/wiki/choosing-an-etl-module'&gt;ETL&lt;/a&gt; process from a Hadoop, batch-based process that is run at regular intervals to a stream-based, always on process, using a technology like &lt;a href='http://storm-project.net/'&gt;Storm&lt;/a&gt;. In the next post in this blog post series, we will elaborate further on our proposed developments for this part of the Snowplow stack. When the time comes, however, we will need to build a new collector, or modify an existing collector, to work in a stream-based system. (So that rather than rely on the processing of logs, each new event logged generates a message in a queue that kicks of a set of analytic processing tasks that end with the data being stored in S3 / Infobright.)&lt;/p&gt;

&lt;h2 id='want_to_get_involved'&gt;Want to get involved?&lt;/h2&gt;

&lt;p&gt;Want to help us develop the Clojure Collector, or some other part of the Snowplow stack? Have an idea about what we should be doing better, or differently? Then &lt;a href='/about/index.html'&gt;get in touch&lt;/a&gt;. We&amp;#8217;d love to hear from you.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/01/03/snowplow-0.7.0-released</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/01/03/snowplow-0.7.0-released"/>
    <title>Snowplow 0.7.0 released, with new Clojure-based collector</title>
    <updated>2013-01-03T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;Today we are hugely excited to announce the release of Snowplow version &lt;strong&gt;0.7.0&lt;/strong&gt;, which includes an experimental new &lt;a href='https://github.com/snowplow/snowplow/tree/master/2-collectors/clojure-collector'&gt;Clojure-based collector&lt;/a&gt; designed to run on &lt;a href='http://aws.amazon.com/elasticbeanstalk/'&gt;Amazon Elastic Beanstalk&lt;/a&gt;. This release allows you to use Snowplow to uniquely identify and track users across multiple domains - even across a whole content or advertising network.&lt;/p&gt;

&lt;p&gt;Many thanks to community member &lt;a href='https://github.com/shermozle'&gt;Simon Rumble&lt;/a&gt; for developing many of the ideas underpinning the new collector in &lt;a href='https://github.com/shermozle/SnowCannon'&gt;SnowCannon&lt;/a&gt;, his node.js-based collector for Snowplow.&lt;/p&gt;

&lt;p&gt;To date, the primary collector for Snowplow events has been our CloudFront-based collector. The CloudFront-based collector has been easy to setup and very reliable, but has one main drawback: it does not support user tracking across multiple domains.&lt;/p&gt;

&lt;p&gt;The Clojure-based collector changes this: it sets a unique user ID server-side and returns it to the browser as a third-party cookie; this user ID is then stored with your Snowplow events, instead of the first-party cookie set by the JavaScript tracker. This means that user=123 on, say, &lt;a href='http://maven.snplow.com'&gt;maven.snplow.com&lt;/a&gt; will be the same as user=123 on &lt;a href='http://snowplowanalytics.com'&gt;snowplowanalytics.com&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;And the other good news is that our Clojure collector automatically logs the raw Snowplow events to Amazon S3 - and it logs in the exact same format as the CloudFront-based collector, so we can use the same ETL process for both collectors!&lt;/p&gt;

&lt;p&gt;Read on below the fold for installation instructions and some additional information on this release.&lt;/p&gt;
&lt;!--more--&gt;
&lt;h2 id='installation_instructions'&gt;Installation instructions&lt;/h2&gt;

&lt;h3 id='clojurebased_collector'&gt;Clojure-based collector&lt;/h3&gt;

&lt;p&gt;You will find full instructions on setting up the new Clojure-based collector on our Wiki, &lt;a href='https://github.com/snowplow/snowplow/wiki/setting-up-the-clojure-collector'&gt;Setting up the Clojure collector&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id='etl'&gt;ETL&lt;/h3&gt;

&lt;p&gt;If you are using EmrEtlRunner, you need to update to the latest version, which is 0.0.7 - this is available by checking out the master branch of the &lt;a href='https://github.com/snowplow/snowplow'&gt;Snowplow repository&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;You will also need to update your configuration file, &lt;code&gt;config.yml&lt;/code&gt;, to use the latest versions of the HiveQL scripts:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:snowplow:
  # ...
  :hive_hiveql_version: 0.5.4
  :non_hive_hiveql_version: 0.0.5&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id='storage'&gt;Storage&lt;/h3&gt;

&lt;p&gt;If you are using StorageLoader, you need to update to the latest version, which is 0.0.3 - this is available by checking out the master branch of the &lt;a href='https://github.com/snowplow/snowplow'&gt;Snowplow repository&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If you are using Infobright Community Edition, you will need to update your table definition. This is because the &lt;code&gt;user_id&lt;/code&gt; field was not wide enough to store the new user IDs (UUIDs) set by the Clojure collector. To make this easier for you, we have created a script:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;4-storage/infobright-storage/migrate_to_005.sh&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Running this script will create a new table, &lt;code&gt;events_005&lt;/code&gt; (version 0.0.5 of the table definition) in your &lt;code&gt;snowplow&lt;/code&gt; database, copying across all your data from your existing &lt;code&gt;events_004&lt;/code&gt; table, which will not be modified in any way.&lt;/p&gt;

&lt;p&gt;Once you have run this, don&amp;#8217;t forget to update your StorageLoader&amp;#8217;s &lt;code&gt;config.yml&lt;/code&gt; to load into the new &lt;code&gt;events_005&lt;/code&gt; table, not your old &lt;code&gt;events_004&lt;/code&gt; table:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:storage:
  # ...
  :table:    events_005 # NOT &amp;quot;events_004&amp;quot; any more&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That&amp;#8217;s it! Your Clojure collector should be ready to run now. However, please read on for an important note about its experimental nature.&lt;/p&gt;

&lt;h2 id='warning_experimental'&gt;Warning: Experimental!&lt;/h2&gt;

&lt;p&gt;We want to stress that the new Clojure-based collector is a piece of experimental technology - we are looking to the community to try it out and feedback to us on how it&amp;#8217;s working for you, especially at scale.&lt;/p&gt;

&lt;p&gt;In particular, we would recommend running the Clojure-based collector alongside the CloudFront collector to be confident that it is performing under load and that no events are being dropped. We have run both collectors alongside each other for the &lt;a href='http://snowplowanalytics.com'&gt;Snowplow Analytics&lt;/a&gt; website for four complete days, and total event counts are as follows:&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Date&lt;/th&gt;&lt;th&gt;CloudFront&lt;/th&gt;&lt;th&gt;Clojure&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;2013-01-02&lt;/td&gt;&lt;td style='text-align: left;'&gt;275&lt;/td&gt;&lt;td style='text-align: left;'&gt;274&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;2013-01-01&lt;/td&gt;&lt;td style='text-align: left;'&gt;116&lt;/td&gt;&lt;td style='text-align: left;'&gt;108&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;2012-12-31&lt;/td&gt;&lt;td style='text-align: left;'&gt;107&lt;/td&gt;&lt;td style='text-align: left;'&gt;109&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;2012-12-30&lt;/td&gt;&lt;td style='text-align: left;'&gt;142&lt;/td&gt;&lt;td style='text-align: left;'&gt;141&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;Overall for the result set, the absolute percentage difference between results for the Cloudfront and Clojure collectors is less than 2% (1.9%). Possible reasons for this discrepancy include:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Differences in datestamps - possibly an event fell on either side of a date boundary for each collector&lt;/li&gt;

&lt;li&gt;Duplicate rows - the two collectors may be occassionally duplicating different rows (see &lt;a href='https://github.com/snowplow/snowplow/issues/24'&gt;issue 24&lt;/a&gt;)&lt;/li&gt;

&lt;li&gt;Browsing behaviour - it may be that the user navigates away from the page before one or other collector can register the event&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We plan on testing all of this further with larger datasets; we also intend to explore the Clojure collector&amp;#8217;s duplicate rows to check there are no particular issues there.&lt;/p&gt;

&lt;h2 id='other_features_in_this_release'&gt;Other features in this release&lt;/h2&gt;

&lt;p&gt;There are two minor changes in this release not related to the Clojure-based collector:&lt;/p&gt;

&lt;p&gt;Both EmrEtlRunner and StorageLoader now print &amp;#8220;Completed successfully&amp;#8221; to &lt;code&gt;stdout&lt;/code&gt; on completion. This should help to make it clearer (e.g. in logs) that these Ruby programs have completed successfully.&lt;/p&gt;

&lt;p&gt;StorageLoader has been updated so that its &lt;code&gt;--skip&lt;/code&gt; argument works the same way as it does in EmrEtlRunner:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Specific options:
    ...
    -s, --skip download,load,archive   skip work step(s)&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id='getting_help'&gt;Getting help&lt;/h2&gt;

&lt;p&gt;That&amp;#8217;s it! If you have any problems with Snowplow version 0.7.0, please &lt;a href='https://github.com/snowplow/snowplow/issues'&gt;raise an issue&lt;/a&gt; or get in touch with us via &lt;a href='https://github.com/snowplow/snowplow/wiki/Talk-to-us'&gt;the usual channels&lt;/a&gt;.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/01/02/referer-parser-ported-to-3-more-languages</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/01/02/referer-parser-ported-to-3-more-languages"/>
    <title>referer-parser now with Java, Scala and Python support</title>
    <updated>2013-01-02T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;Happy New Year all! It&amp;#8217;s been three months since we &lt;a href='/blog/2012/10/11/attlib-0.0.1-released/'&gt;introduced our Attlib project&lt;/a&gt;, now renamed to &lt;a href='https://github.com/snowplow/referer-parser'&gt;referer-parser&lt;/a&gt;, and we are pleased to announce that referer-parser is now available in three additional languages: Java, Scala and Python.&lt;/p&gt;

&lt;p&gt;To recap: referer-parser is a simple library for extracting seach marketing attribution data from referer &lt;em&gt;(sic)&lt;/em&gt; URLs. You supply referer-parser with a referer URL; it then tells you whether the URL is from a search engine - and if so, which search engine it is, and what keywords the user supplied to arrive at your page.&lt;/p&gt;

&lt;p&gt;Huge thanks to &lt;a href='https://github.com/donspaulding'&gt;Don Spaulding&lt;/a&gt; @ &lt;a href='http://mirusresearch.com/'&gt;Mirus Research&lt;/a&gt; for contributing the &lt;a href='https://github.com/snowplow/referer-parser/tree/master/python'&gt;Python port&lt;/a&gt; of referer-parser; the &lt;a href='https://github.com/snowplow/referer-parser/tree/master/java-scala'&gt;Java/Scala port&lt;/a&gt; was developed by us in-house and it will be a key addition to our &lt;a href='https://github.com/snowplow/snowplow/wiki/etl'&gt;Snowplow ETL&lt;/a&gt; process in the coming months.&lt;/p&gt;

&lt;p&gt;You can checkout the code on GitHub, in the &lt;a href='https://github.com/snowplow/referer-parser'&gt;referer-parser repository&lt;/a&gt;, or read on below the fold for some code examples in the new languages:&lt;/p&gt;
&lt;!--more--&gt;
&lt;h2 id='python'&gt;Python&lt;/h2&gt;

&lt;p&gt;To use referer-parser from a Python script:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='python'&gt;&lt;span class='kn'&gt;from&lt;/span&gt; &lt;span class='nn'&gt;referer_parser&lt;/span&gt; &lt;span class='kn'&gt;import&lt;/span&gt; &lt;span class='n'&gt;Referer&lt;/span&gt;

&lt;span class='n'&gt;referer_url&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='s'&gt;&amp;#39;http://www.google.com/search?q=gateway+oracle+cards+denise+linn&amp;amp;hl=en&amp;amp;client=safari&amp;#39;&lt;/span&gt;

&lt;span class='n'&gt;r&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='n'&gt;Referer&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;referer_url&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt;

&lt;span class='k'&gt;print&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;r&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;known&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt;              &lt;span class='c'&gt;# True&lt;/span&gt;
&lt;span class='k'&gt;print&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;r&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;referer&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt;            &lt;span class='c'&gt;# &amp;#39;Google&amp;#39;&lt;/span&gt;
&lt;span class='k'&gt;print&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;r&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;search_parameter&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt;   &lt;span class='c'&gt;# &amp;#39;q&amp;#39;     &lt;/span&gt;
&lt;span class='k'&gt;print&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;r&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;search_term&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt;        &lt;span class='c'&gt;# &amp;#39;gateway oracle cards denise linn&amp;#39;&lt;/span&gt;
&lt;span class='k'&gt;print&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;r&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;uri&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt;                &lt;span class='c'&gt;# ParseResult(scheme=&amp;#39;http&amp;#39;, netloc=&amp;#39;www.google.com&amp;#39;, path=&amp;#39;/search&amp;#39;, params=&amp;#39;&amp;#39;, query=&amp;#39;q=gateway+oracle+cards+denise+linn&amp;amp;hl=en&amp;amp;client=safari&amp;#39;, fragment=&amp;#39;&amp;#39;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;For more information, please see the Python &lt;a href='https://github.com/snowplow/referer-parser/blob/master/python/README.md'&gt;README&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id='scala'&gt;Scala&lt;/h2&gt;

&lt;p&gt;To use referer-parser from a Scala app:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='scala'&gt;&lt;span class='k'&gt;val&lt;/span&gt; &lt;span class='n'&gt;refererUrl&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='s'&gt;&amp;quot;http://www.google.com/search?q=gateway+oracle+cards+denise+linn&amp;amp;hl=en&amp;amp;client=safari&amp;quot;&lt;/span&gt;

&lt;span class='k'&gt;import&lt;/span&gt; &lt;span class='nn'&gt;com.snowplowanalytics.refererparser.scala.Parser&lt;/span&gt;
&lt;span class='k'&gt;for&lt;/span&gt; &lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;r&lt;/span&gt; &lt;span class='k'&gt;&amp;lt;-&lt;/span&gt; &lt;span class='nc'&gt;Parser&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;parse&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;refererUrl&lt;/span&gt;&lt;span class='o'&gt;))&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
  &lt;span class='n'&gt;println&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;r&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;referer&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;name&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;      &lt;span class='c1'&gt;// =&amp;gt; &amp;quot;Google&amp;quot;&lt;/span&gt;
  &lt;span class='k'&gt;for&lt;/span&gt; &lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;s&lt;/span&gt; &lt;span class='k'&gt;&amp;lt;-&lt;/span&gt; &lt;span class='n'&gt;r&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;search&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
    &lt;span class='n'&gt;println&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;s&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;term&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;            &lt;span class='c1'&gt;// =&amp;gt; &amp;quot;gateway oracle cards denise linn&amp;quot;&lt;/span&gt;
    &lt;span class='n'&gt;println&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;s&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;parameter&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;       &lt;span class='c1'&gt;// =&amp;gt; &amp;quot;q&amp;quot;    &lt;/span&gt;
  &lt;span class='o'&gt;}&lt;/span&gt;
&lt;span class='o'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;For more information, please see the Java/Scala &lt;a href='https://github.com/snowplow/referer-parser/blob/master/java-scala/README.md'&gt;README&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id='usage_java'&gt;Usage: Java&lt;/h2&gt;

&lt;p&gt;To use referer-parser from a Java program:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='java'&gt;&lt;span class='kn'&gt;import&lt;/span&gt; &lt;span class='nn'&gt;com.snowplowanalytics.refererparser.Parser&lt;/span&gt;&lt;span class='o'&gt;;&lt;/span&gt;

&lt;span class='o'&gt;...&lt;/span&gt;

  &lt;span class='n'&gt;String&lt;/span&gt; &lt;span class='n'&gt;refererUrl&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='s'&gt;&amp;quot;http://www.google.com/search?q=gateway+oracle+cards+denise+linn&amp;amp;hl=en&amp;amp;client=safari&amp;quot;&lt;/span&gt;&lt;span class='o'&gt;;&lt;/span&gt;

  &lt;span class='n'&gt;Parser&lt;/span&gt; &lt;span class='n'&gt;refererParser&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='k'&gt;new&lt;/span&gt; &lt;span class='n'&gt;Parser&lt;/span&gt;&lt;span class='o'&gt;();&lt;/span&gt;
  &lt;span class='n'&gt;Referal&lt;/span&gt; &lt;span class='n'&gt;r&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='n'&gt;refererParser&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='na'&gt;parse&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;refererUrl&lt;/span&gt;&lt;span class='o'&gt;);&lt;/span&gt;

  &lt;span class='n'&gt;System&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='na'&gt;out&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='na'&gt;println&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;r&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='na'&gt;referer&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='na'&gt;name&lt;/span&gt;&lt;span class='o'&gt;);&lt;/span&gt;       &lt;span class='c1'&gt;// =&amp;gt; &amp;quot;Google&amp;quot;&lt;/span&gt;
  &lt;span class='n'&gt;System&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='na'&gt;out&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='na'&gt;println&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;r&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='na'&gt;search&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='na'&gt;parameter&lt;/span&gt;&lt;span class='o'&gt;);&lt;/span&gt;   &lt;span class='c1'&gt;// =&amp;gt; &amp;quot;q&amp;quot;    &lt;/span&gt;
  &lt;span class='n'&gt;System&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='na'&gt;out&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='na'&gt;println&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;r&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='na'&gt;search&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='na'&gt;term&lt;/span&gt;&lt;span class='o'&gt;);&lt;/span&gt;        &lt;span class='c1'&gt;// =&amp;gt; &amp;quot;gateway oracle cards denise linn&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;For more information, please see the Java/Scala &lt;a href='https://github.com/snowplow/referer-parser/blob/master/java-scala/README.md'&gt;README&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id='getting_help'&gt;Getting help&lt;/h2&gt;

&lt;p&gt;That&amp;#8217;s it! If you have any problems with the new versions of referer-parser, please &lt;a href='https://github.com/snowplow/referer-parser/issues'&gt;raise an issue&lt;/a&gt; or get in touch with us via &lt;a href='https://github.com/snowplow/snowplow/wiki/Talk-to-us'&gt;the usual channels&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;And do let us know if you find referer-parser useful!&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/12/26/snowplow-0.6.5-released</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/12/26/snowplow-0.6.5-released"/>
    <title>Snowplow 0.6.5 released, with improved event tracking</title>
    <updated>2012-12-26T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We&amp;#8217;re excited to announce our next Snowplow release - version &lt;strong&gt;0.6.5&lt;/strong&gt;, a Boxing Day release for Snowplow!&lt;/p&gt;

&lt;p&gt;This is a big release for us, as it introduces the idea of &lt;strong&gt;event types&lt;/strong&gt; - every event sent by the JavaScript tracker to the collector now has an &lt;code&gt;event&lt;/code&gt; field which specifies what type of event it is. This should be really helpful for a couple of things:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;It should make querying Snowplow events much easier&lt;/li&gt;

&lt;li&gt;It should make Snowplow event data a better fit for JSON-oriented datastores such as MongoDB and Riak&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;As well as event types, in this release we are also introducing &lt;strong&gt;event IDs&lt;/strong&gt;. With this, the ETL phase adds an &lt;code&gt;event_id&lt;/code&gt; UUID (universally unique ID) to each event row, which should help with subsequent querying.&lt;/p&gt;

&lt;p&gt;Here is a taster of how Snowplow event data looks with the new event types and event IDs:&lt;/p&gt;

&lt;p&gt;&lt;img alt='events-screenshot' src='/static/img/blog/2012/event_and_event_id_fields.png' /&gt;&lt;/p&gt;

&lt;p&gt;These are not the only improvements in this version - here are the rest:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;We have cleaned up the code for on-page activity tracking (&amp;#8220;page pings&amp;#8221;)&lt;/li&gt;

&lt;li&gt;We have fixed a bug that affected ad impression tracking - thanks &lt;a href='https://github.com/talkspoon'&gt;Alan Z&lt;/a&gt;!&lt;/li&gt;

&lt;li&gt;The ETL no longer dies if a raw event has a corrupted querystring (e.g. the &lt;code&gt;&amp;amp;refr=&lt;/code&gt; parameter was not escaped)&lt;/li&gt;

&lt;li&gt;The JavaScript tracker&amp;#8217;s build script, &lt;code&gt;snowpak.sh&lt;/code&gt;, now has a combine-only option (no minification), which is helpful for testing purposes&lt;/li&gt;

&lt;li&gt;The JavaScript tracker has a new method, &lt;code&gt;attachUserId(boolean)&lt;/code&gt;, which can be used to stop the tracker sending a &lt;code&gt;&amp;amp;uid=xxx&lt;/code&gt; parameter&lt;/li&gt;

&lt;li&gt;We have added the ability to override the IP address by passing in an &lt;code&gt;&amp;amp;ip=&lt;/code&gt; parameter on the querystring&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Below, we first explain how to upgrade, before taking a brief tour through these updates:&lt;/p&gt;
&lt;!--more--&gt;
&lt;h2 id='0_upgrading'&gt;0. Upgrading&lt;/h2&gt;

&lt;p&gt;Upgrading is a two-step process:&lt;/p&gt;

&lt;h3 id='javascript_tracker'&gt;JavaScript tracker&lt;/h3&gt;

&lt;p&gt;Please update your website(s) to use the latest version of the JavaScript tracker, which is version 0.9.0. As always, the updated minified tracker is available here:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http(s)://d1fc8wv8zag5ca.cloudfront.net/0.9.0/sp.js&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id='etl'&gt;ETL&lt;/h3&gt;

&lt;p&gt;If you are using EmrEtlRunner, you need to update your configuration file, &lt;code&gt;config.yml&lt;/code&gt;, to use the latest versions of the Hive serde and HiveQL scripts:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:snowplow:
  :serde_version: 0.5.3
  :hive_hiveql_version: 0.5.3
  :non_hive_hiveql_version: 0.0.4&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That&amp;#8217;s it! You don&amp;#8217;t need to make any changes to your Infobright setup, assuming you are up-to-date with previous releases.&lt;/p&gt;

&lt;h2 id='1_event_types'&gt;1. Event types&lt;/h2&gt;

&lt;p&gt;To recap, every event sent by the JavaScript tracker now has an &lt;code&gt;event&lt;/code&gt; field which specifies what type of event it is. Currently we have six different types of events, which are set out in the table below:&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Type of event&lt;/th&gt;&lt;th&gt;JavaScript tracker function&lt;/th&gt;&lt;th&gt;Value of Snowplow &lt;code&gt;event&lt;/code&gt; field&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Page view&lt;/td&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;trackPageView()&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;page_view&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Page ping&lt;/td&gt;&lt;td style='text-align: left;'&gt;None (automatic)*&lt;/td&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;page_ping&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Custom event&lt;/td&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;trackEvent()&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;custom&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Ad impression&lt;/td&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;trackImpression()&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;ad_impression&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Transaction&lt;/td&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;addTrans&lt;/code&gt; &amp;amp; &lt;code&gt;trackTrans()&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;transaction&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Transaction item&lt;/td&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;addItem&lt;/code&gt; &amp;amp; &lt;code&gt;trackTrans()&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;transaction_item&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;&lt;em&gt;* for more information on on-page activity tracking, please see the relevant section later in this blog post.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;This new event field should make it much easier to query Snowplow data by the type of event. For example, to retrieve the number of e-commerce transactions per day:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='sql'&gt;&lt;span class='k'&gt;SELECT&lt;/span&gt;
&lt;span class='n'&gt;dt&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='k'&gt;COUNT&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;event_id&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt;
&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='n'&gt;events&lt;/span&gt;
&lt;span class='k'&gt;WHERE&lt;/span&gt; &lt;span class='n'&gt;event&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;transaction&amp;#39;&lt;/span&gt;
&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='n'&gt;dt&lt;/span&gt; &lt;span class='p'&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We will be updating our &lt;a href='/analytics/index.html'&gt;Analytics Cookbook&lt;/a&gt; to use the &lt;code&gt;event&lt;/code&gt; field to simplify queries where possible.&lt;/p&gt;

&lt;h2 id='2_event_ids'&gt;2. Event IDs&lt;/h2&gt;

&lt;p&gt;As stated above, the Snowplow ETL now attaches a unique ID to each event - specifically a &lt;a href='http://en.wikipedia.org/wiki/Universally_unique_identifier#Version_4_.28random.29'&gt;type 4 UUID&lt;/a&gt;. This new &lt;code&gt;event_id&lt;/code&gt; is much more unique than the existing &lt;code&gt;txn_id&lt;/code&gt; field, which is a short random number set in the JavaScript tracker (&lt;code&gt;txn_id&lt;/code&gt; is currently unused, but we may eventually use it to check for duplicate events &lt;code&gt;txn_id&lt;/code&gt; introduced prior to the ETL, see &lt;a href='https://github.com/snowplow/snowplow/issues/24'&gt;issue 24&lt;/a&gt; for more details).&lt;/p&gt;

&lt;p&gt;You can use the new &lt;code&gt;event_id&lt;/code&gt; field to uniquely identify individual events in your event store, and of course to count distinct events. For example, to count the number of page views by day, we simply execute the following query:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='sql'&gt;&lt;span class='k'&gt;SELECT&lt;/span&gt;
&lt;span class='n'&gt;dt&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='k'&gt;COUNT&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;event_id&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt;
&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='n'&gt;events&lt;/span&gt;
&lt;span class='k'&gt;WHERE&lt;/span&gt; &lt;span class='n'&gt;event&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;page_view&amp;#39;&lt;/span&gt;
&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='n'&gt;dt&lt;/span&gt; &lt;span class='p'&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We will be updating our &lt;a href='/analytics/index.html'&gt;Analytics Cookbook&lt;/a&gt; to use &lt;code&gt;event_id&lt;/code&gt; in any examples which currently (erroneously) use &lt;code&gt;txn_id&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id='3_onpage_activity_tracking'&gt;3. On-page activity tracking&lt;/h2&gt;

&lt;p&gt;In this release of the JavaScript tracker we have deprecated the old (undocumented) &lt;code&gt;setHeartBeatTimer()&lt;/code&gt; inherited from &lt;code&gt;piwik.js&lt;/code&gt;, and introduced a new function, &lt;code&gt;enableActivityTracking(minimumVisitLength, heartBeatDelay)&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;With activity tracking enabled, &amp;#8220;page pings&amp;#8221; are sent to Snowplow every &lt;code&gt;heartBeatDelay&lt;/code&gt; seconds, as long as the visitor remains active (moving the mouse, clicking etc) on the page. Page pings are not sent until the &lt;code&gt;minimumVisitLength&lt;/code&gt; seconds have elapsed.&lt;/p&gt;

&lt;p&gt;Here is an example configuration:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='javascript'&gt;&lt;span class='nx'&gt;_snaq&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='nx'&gt;push&lt;/span&gt;&lt;span class='p'&gt;([&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;enableActivityTracking&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='mi'&gt;10&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='mi'&gt;10&lt;/span&gt;&lt;span class='p'&gt;]);&lt;/span&gt; &lt;span class='c1'&gt;// Ping every 10 seconds after 10 seconds&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This is still an experimental feature - but it should provide some interesting data to start to explore page residency, true bounce rates and so on.&lt;/p&gt;

&lt;p&gt;Please note that enabling activity tracking can &lt;strong&gt;significantly&lt;/strong&gt; increase the number of Snowplow events generated, especially with a short &lt;code&gt;heartBeatDelay&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id='4_and_the_rest'&gt;4. And the rest&lt;/h2&gt;

&lt;p&gt;The rest of the changes in this release are much smaller, being either bug fixes or small preparatory features for future releases:&lt;/p&gt;

&lt;h3 id='ad_impression_tracking_bug_fix'&gt;Ad impression tracking bug fix&lt;/h3&gt;

&lt;p&gt;Many thanks to &lt;a href='https://github.com/talkspoon'&gt;Alan Z&lt;/a&gt; @ &lt;a href='http://www.verycd.com'&gt;VeryCD&lt;/a&gt; for spotting a bug in the &lt;code&gt;trackImpression()&lt;/code&gt; method, which was stopping ad impressions from being logged. This is now fixed.&lt;/p&gt;

&lt;h3 id='etl_resilient_against_corrupted_querystrings'&gt;ETL resilient against corrupted querystrings&lt;/h3&gt;

&lt;p&gt;We had a problem with two historic versions of the JavaScript tracker, 0.8.0 and 0.8.1, where querystrings were being transmitted to the Snowplow collector unescaped. These &amp;#8220;corrupted&amp;#8221; querystrings caused the ETL process to error and die.&lt;/p&gt;

&lt;p&gt;We have updated the ETL process so that events with corrupted querystrings can be processed without error: these rows are stored as Snowplow events, but of course with most of the standard fields empty.&lt;/p&gt;

&lt;h3 id='snowpaksh_combineonly_option'&gt;snowpak.sh combine-only option&lt;/h3&gt;

&lt;p&gt;To make it easier to hack on the JavaScript tracker, we have updated the build script, &lt;code&gt;snowpak.sh&lt;/code&gt;, so that it has a &amp;#8220;combine-only&amp;#8221; option. If you set the combine-only flag on the command line, then the minification step will &lt;strong&gt;not&lt;/strong&gt; be run, and debug code will left in. This is helpful for local testing when you want to debug the JavaScript in its pre-minified form.&lt;/p&gt;

&lt;p&gt;Here are the updated usage options for the &lt;code&gt;snowpak.sh&lt;/code&gt; script:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Usage: ./snowpak.sh [options]

Specific options:
  -y PATH             path to YUICompressor 2.4.2 *
  -c                  combine only (no minification or removing debug)

* or set env variable YUI_COMPRESSOR_PATH instead

Common options:
  -h                  Show this message&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id='attachuseridboolean'&gt;attachUserId(boolean)&lt;/h3&gt;

&lt;p&gt;The JavaScript tracker has a new method, &lt;code&gt;attachUserId(boolean)&lt;/code&gt;, which can be used to stop the tracker from sending a &lt;code&gt;&amp;amp;uid=xxx&lt;/code&gt; parameter. By default, the JavaScript tracker sends the user ID to the collector via this &lt;code&gt;uid&lt;/code&gt; parameter; you can now disable this by calling &lt;code&gt;attachUserId()&lt;/code&gt; like so:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='javascript'&gt;&lt;span class='nx'&gt;_snaq&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='nx'&gt;push&lt;/span&gt;&lt;span class='p'&gt;([&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;attachUserId&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='kc'&gt;false&lt;/span&gt;&lt;span class='p'&gt;]);&lt;/span&gt; &lt;span class='c1'&gt;// Don&amp;#39;t attach &amp;amp;uid=xxx to the querystring&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This function is not of immediate use - but it will be an important part of the setup for using the new Clojure Collector, which we are currently working on.&lt;/p&gt;

&lt;h3 id='ip_address_override'&gt;IP address override&lt;/h3&gt;

&lt;p&gt;Finally, we have added the ability to override the IP address by passing in an &lt;code&gt;ip=&lt;/code&gt; parameter on the querystring.&lt;/p&gt;

&lt;p&gt;This one is a little confusing, as there is no capability in the JavaScript tracker to attach an IP address to the querystring (because JavaScript cannot know the user&amp;#8217;s IP address). Rather, we have added this IP address override ability to cater for future server-side trackers and collectors which &lt;em&gt;do&lt;/em&gt; know the user&amp;#8217;s IP address and want to append it to the querystring themselves.&lt;/p&gt;

&lt;h2 id='getting_help'&gt;Getting help&lt;/h2&gt;

&lt;p&gt;That&amp;#8217;s it! If you have any problems with Snowplow version 0.6.5, please &lt;a href='https://github.com/snowplow/snowplow/issues'&gt;raise an issue&lt;/a&gt; or get in touch with us via &lt;a href='https://github.com/snowplow/snowplow/wiki/Talk-to-us'&gt;the usual channels&lt;/a&gt;.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/12/20/snowplow-0.6.4-released</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/12/20/snowplow-0.6.4-released"/>
    <title>Snowplow 0.6.4 released, with Infobright improvements</title>
    <updated>2012-12-20T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We&amp;#8217;re happy to announce our next Snowplow release - version &lt;strong&gt;0.6.4&lt;/strong&gt;. This release includes updates:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;An upgraded Infobright table definition which scales to millions of pageviews easily&lt;/li&gt;

&lt;li&gt;Clarified Hive table definitions&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Before we start - a big thanks to the community members who helped out on this release:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href='https://github.com/moncaubeig'&gt;Gilles Moncaubeig&lt;/a&gt; @ &lt;a href='http://en.overblog.com/'&gt;OverBlog&lt;/a&gt; worked closely with us on the updated Infobright table definition&lt;/li&gt;

&lt;li&gt;&lt;a href='https://github.com/mmoulton'&gt;Mike Moulton&lt;/a&gt; @ &lt;a href='http://meltmedia.com/'&gt;meltmedia&lt;/a&gt; for flagging the missing Hive table definition&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We&amp;#8217;ll take a look at both updates below:&lt;/p&gt;
&lt;!--more--&gt;
&lt;h2 id='upgraded_infobright_table_definition'&gt;Upgraded Infobright table definition&lt;/h2&gt;

&lt;p&gt;With help from &lt;a href='https://github.com/moncaubeig'&gt;Gilles Moncaubeig&lt;/a&gt; we have upgraded the Infobright table definition so that it can easily scale to loading millions of new Snowplow events per day. It also supports much longer &lt;code&gt;br_lang&lt;/code&gt; and &lt;code&gt;page_url&lt;/code&gt; fields, which should prevent you from occasional load errors.&lt;/p&gt;

&lt;p&gt;If you are using Infobright Community Edition for analysis, you will need to update your table definition. This is a little complex, because Infobright does not support in-place table or column renames. To make this easier for you, we have created a script:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;4-storage/infobright-storage/migrate_to_004.sh&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Running this script will create a new table, &lt;code&gt;events_004&lt;/code&gt; (version 0.0.4 of the table definition) in your &lt;code&gt;snowplow&lt;/code&gt; database, copying across all your data from your existing &lt;code&gt;events&lt;/code&gt; table, which will not be modified in any way.&lt;/p&gt;

&lt;p&gt;Once you have run this, don&amp;#8217;t forget to update your StorageLoader&amp;#8217;s &lt;code&gt;config.yml&lt;/code&gt; to load into the new &lt;code&gt;events_004&lt;/code&gt; table, not your old &lt;code&gt;events&lt;/code&gt; table:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:storage:
  :type: infobright
  :database: snowplow
  :table:    events_004 # NOT &amp;quot;events_003&amp;quot; any more&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Done!&lt;/p&gt;

&lt;h2 id='clarified_hive_table_definitions'&gt;Clarified Hive table definitions&lt;/h2&gt;

&lt;p&gt;We have clarified the two different Hive table definitions, available in this folder:&lt;/p&gt;

&lt;p&gt;4-storage/hive-storage&lt;/p&gt;

&lt;p&gt;Which format your Snowplow event files are in will depend on how your EmrEtlRunner is configured. If your &lt;code&gt;config.yml&lt;/code&gt; contains:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:storage_format: non-hive&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;then your Snowplow events will be stored in the format shown in &lt;a href='https://github.com/snowplow/snowplow/blob/master/4-storage/hive-storage/non-hive-format-table-def.q'&gt;&lt;code&gt;non-hive-format-table-def.q&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Whereas if your &lt;code&gt;config.yml&lt;/code&gt; contains:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:storage_format: non-hive&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;then your Snowplow events will be stored in the format shown in &lt;a href='https://github.com/snowplow/snowplow/blob/master/4-storage/hive-storage/hive-format-table-def.q'&gt;&lt;code&gt;hive-format-table-def.q&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id='getting_help'&gt;Getting help&lt;/h2&gt;

&lt;p&gt;That&amp;#8217;s it! If you have any problems with Snowplow version 0.6.4, please &lt;a href='https://github.com/snowplow/snowplow/issues'&gt;raise an issue&lt;/a&gt; or get in touch with us via &lt;a href='https://github.com/snowplow/snowplow/wiki/Talk-to-us'&gt;the usual channels&lt;/a&gt;.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/12/18/snowplow-0.6.3-released</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/12/18/snowplow-0.6.3-released"/>
    <title>Snowplow 0.6.3 released, with JavaScript and HiveQL bug fixes</title>
    <updated>2012-12-18T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;Today we are releasing Snowplow version &lt;strong&gt;0.6.3&lt;/strong&gt; - another clean-up release following on from the 0.6.2 release. This release bumps the JavaScript Tracker to version 0.8.2, and the Hive-data-format HiveQL file to version 0.5.2.&lt;/p&gt;

&lt;p&gt;Many thanks to the community members who contributed bug fixes to this release: &lt;a href='https://github.com/mmoulton'&gt;Mike Moulton&lt;/a&gt; @ &lt;a href='http://meltmedia.com/'&gt;meltmedia&lt;/a&gt;, &lt;a href='https://github.com/ramn'&gt;Simon Andersson&lt;/a&gt; @ &lt;a href='http://www.qwaya.com'&gt;Qwaya&lt;/a&gt; and &lt;a href='https://github.com/mtibben'&gt;Michael Tibben&lt;/a&gt; @ &lt;a href='http://99designs.com'&gt;99designs&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We&amp;#8217;ll take a look at both fixes below:&lt;/p&gt;
&lt;!--more--&gt;
&lt;h2 id='javascript_tracker_fixes'&gt;JavaScript tracker fixes&lt;/h2&gt;

&lt;p&gt;This release fixes the issues in the JavaScript tracker raised in &lt;a href='https://github.com/snowplow/snowplow/issues/103'&gt;issue 103&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;These issues stemmed from the splitting of the JavaScript into multiple files in Snowplow version 0.6.1 (JavaScript tracker version 0.8.0). We do not believe these bugs affected Snowplow data collection, but they are well worth fixing.&lt;/p&gt;

&lt;p&gt;Many thanks to &lt;a href='https://github.com/mtibben'&gt;Michael Tibben&lt;/a&gt; and &lt;a href='https://github.com/ramn'&gt;Simon Andersson&lt;/a&gt; for their help in squashing these bugs!&lt;/p&gt;

&lt;p&gt;With these fixes we have bumped the JavaScript Tracker to version 0.8.2; the updated minified tracker is available as always here:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http(s)://d1fc8wv8zag5ca.cloudfront.net/0.8.2/sp.js&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id='hiveql_script_fix'&gt;HiveQL script fix&lt;/h2&gt;

&lt;p&gt;This release also fixes a bug (&lt;a href='https://github.com/snowplow/snowplow/pull/112'&gt;issue 112&lt;/a&gt;) in the HiveQL file used to generate the &lt;strong&gt;Hive-format&lt;/strong&gt; Snowplow event files.&lt;/p&gt;

&lt;p&gt;This bug prevented the ETL from running if you were using EmrEtlRunner to generate Hive-format Snowplow event files - in other words if you had set:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:storage_format: non-hive&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;in your EmrEtlRunner&amp;#8217;s &lt;code&gt;config.yml&lt;/code&gt; file.&lt;/p&gt;

&lt;p&gt;Many thanks to &lt;a href='https://github.com/mmoulton'&gt;Mike Moulton&lt;/a&gt; for spotting and fixing this one!&lt;/p&gt;

&lt;p&gt;With this fix we have bumped the Hive-format HiveQL file to version 0.5.2. To start using the new file, all you need to do is update your EmrEtlRunner&amp;#8217;s &lt;code&gt;config.yml&lt;/code&gt; file by changing:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:hive_hiveql_version: 0.5.1&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;to:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:hive_hiveql_version: 0.5.2&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id='getting_help'&gt;Getting help&lt;/h2&gt;

&lt;p&gt;If you have any problems with Snowplow version 0.6.3, please &lt;a href='https://github.com/snowplow/snowplow/issues'&gt;raise an issue&lt;/a&gt; or get in touch with us via &lt;a href='https://github.com/snowplow/snowplow/wiki/Talk-to-us'&gt;the usual channels&lt;/a&gt;.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/12/17/transforming-snowplow-data-so-it-can-be-interrogated-by-olap-tools-like-tableau</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/12/17/transforming-snowplow-data-so-it-can-be-interrogated-by-olap-tools-like-tableau"/>
    <title>Transforming Snowplow data so that it can be interrogataed in BI / OLAP tools like Tableau, Qlikview and Pentaho</title>
    <updated>2012-12-17T00:00:00+00:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;Because Snowplow does not ship with any sort of user interface, we get many enquiries from current and prospective users who would like to interrogate Snowplow data with popular BI tools like Tableau or Qlikview.&lt;/p&gt;
&lt;p style='text-align:center;'&gt;&lt;img alt='olap cube' src='/static/img/olap/example-cube-2.png' width='250' /&gt;&lt;/p&gt;
&lt;p&gt;Unfortunately, it is not possible to run a tool like Tableau directly on top of the Snowplow events table. That is because these tools require the data to be in a particular format: one in which each line of data is made up of a combination of dimension and metrics fields, such that it is straightforward for the reporting tool to understand how to aggregate metrics by different combinations of dimensions. To give a very simple example of a data set that would play nicely with a reporting tool like Tableau:&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Country&lt;/th&gt;&lt;th&gt;Date&lt;/th&gt;&lt;th&gt;Product&lt;/th&gt;&lt;th&gt;Number Sold&lt;/th&gt;&lt;th&gt;Revenue&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;UK&lt;/td&gt;&lt;td style='text-align: left;'&gt;Sept 2012&lt;/td&gt;&lt;td style='text-align: left;'&gt;Hats&lt;/td&gt;&lt;td style='text-align: left;'&gt;137&lt;/td&gt;&lt;td style='text-align: right;'&gt;1779.63&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;UK&lt;/td&gt;&lt;td style='text-align: left;'&gt;Oct 2012&lt;/td&gt;&lt;td style='text-align: left;'&gt;Hats&lt;/td&gt;&lt;td style='text-align: left;'&gt;193&lt;/td&gt;&lt;td style='text-align: right;'&gt;2507.07&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;UK&lt;/td&gt;&lt;td style='text-align: left;'&gt;Oct 2012&lt;/td&gt;&lt;td style='text-align: left;'&gt;Shoes&lt;/td&gt;&lt;td style='text-align: left;'&gt;15&lt;/td&gt;&lt;td style='text-align: right;'&gt;1125.00&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;France&lt;/td&gt;&lt;td style='text-align: left;'&gt;Oct 2012&lt;/td&gt;&lt;td style='text-align: left;'&gt;Hats&lt;/td&gt;&lt;td style='text-align: left;'&gt;288&lt;/td&gt;&lt;td style='text-align: right;'&gt;2877.12&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&amp;#8230;&lt;/td&gt;&lt;td style='text-align: left;'&gt;&amp;#8230;&lt;/td&gt;&lt;td style='text-align: left;'&gt;&amp;#8230;&lt;/td&gt;&lt;td style='text-align: left;'&gt;&amp;#8230;&lt;/td&gt;&lt;td style='text-align: right;'&gt;&amp;#8230;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;Reporting tools like Tableau recognise that the two right hand columns (number sold and revenue) are metrics, and that analysts will want to examine how those metrics vary by country, time and product (all of which are dimensions). They will give analysts easy-to-use tools to enable them to slice, dice, drill up and drill down those metrics by different combinations of those dimensions. Enabling those operations is straightforward for the reporting tool, because it knows it knows if the analyst wants to report product sales in the UK over time, to filter all results by country (so only lines of data from the UK are included), and display sales of each type of product by month.&lt;/p&gt;

&lt;p&gt;This type of dimensional analysis is called &lt;a href='http://en.wikipedia.org/wiki/Online_analytical_processing'&gt;OLAP&lt;/a&gt;, and has a long and venerable history in business intelligence. Although the term &amp;#8216;OLAP&amp;#8217; is no longer fashionable, this type of analysis is still the predominant one used by anyone who relies on PivotTables in Excel or any mainstream BI tool including Tableau, Qlikview, Microstrategy, Pentaho etc.&lt;/p&gt;
&lt;!--more--&gt;
&lt;p&gt;Snowplow data is in a log file format. Whilst each line does include metrics (e.g. revenue) and dimensions (e.g. browser features or operating system details), there are a large number of dimensions that we might want to slice and dice the data on that are not included in each line: these data points have to be inferred by reading across several lines of data. To give just one example: the source of traffic i.e. &lt;code&gt;mkt_source&lt;/code&gt; for a particular visit is only given on the &lt;strong&gt;first line&lt;/strong&gt; of data for that visit. Hence, in order to enable users to analyse page views, customer lifetime value and other metrics by different marketing sources (e.g. to do attribution analysis), we need to work out which source of traffic to attribute that visit to, and label every line of data associated with that visit with that source of traffic.&lt;/p&gt;

&lt;p&gt;So in order to use a BI tool like Tableau or Qlikview to interrogate Snowplow data, you need to transform the data first. We&amp;#8217;ve documented how to perform the transformation in the &lt;a href='/analytics/tools-and-techniques/converting-snowplow-data-into-a-format-suitable-for-olap.html'&gt;analytics cookbook&lt;/a&gt;. We hope it provides a useful guide for anyone interested in interrogating or visualising Snowplow data using BI tools.&lt;/p&gt;

&lt;p&gt;We also wonder whether the documentation will be of interest to the wider community of analysts and data scientists interested in using tools like Tableau to query log data. One of the things that surprised us, going in to this exercise, was the lack of material we could find on the internet that covered transforming log data so that it could be analysed using BI tools. We expected this to be well-covered territory: especially given the fact that the vast majority of data processed by Hadoop is log files, and nearly all the BI vendors are working hard to integrate their products with Hadoop. (For example, Tableau now integrates with MapR Hadoop distributions, so you can analyse Hive tables directly in Tableau.) Without the transformation step covered in our &lt;a href='/analytics/tools-and-techniques/converting-snowplow-data-into-a-format-suitable-for-olap.html'&gt;guide&lt;/a&gt; however, this type of integration is useless.&lt;/p&gt;

&lt;p&gt;If we&amp;#8217;ve missed any online literature on the topic, or if there are other people who&amp;#8217;ve looked at this particular problem and come up with different approaches - we&amp;#8217;d love to debate them here. Get in touch! For everyone else, we hope you find our &lt;a href='/analytics/tools-and-techniques/converting-snowplow-data-into-a-format-suitable-for-olap.html'&gt;guide&lt;/a&gt; helpful&amp;#8230;&lt;/p&gt;

&lt;p&gt;References:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Snowplow data &lt;a href='https://github.com/snowplow/snowplow/wiki/canonical-event-model'&gt;canonical data structure&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/11/28/snowplow-0.6.2-released</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/11/28/snowplow-0.6.2-released"/>
    <title>Snowplow 0.6.2 released, with JavaScript tracker bug fixes</title>
    <updated>2012-11-28T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;Today we are releasing Snowplow version &lt;strong&gt;0.6.2&lt;/strong&gt; - a clean-up release after yesterday&amp;#8217;s 0.6.1 release. This release bumps the JavaScript Tracker to version 0.8.1; the updated minified tracker is available as always here:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http(s)://d1fc8wv8zag5ca.cloudfront.net/0.8.1/sp.js&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This release fixes two bugs:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href='https://github.com/snowplow/snowplow/pull/101'&gt;Issue #101&lt;/a&gt; - we had left in a &lt;code&gt;console.log()&lt;/code&gt; in the production version, which should only have been printed in debug mode. Harmless but worth taking out. Many thanks to &lt;a href='https://github.com/mtibben'&gt;Michael Tibben&lt;/a&gt; @ &lt;a href='http://99designs.com'&gt;99designs&lt;/a&gt; for spotting this so quickly and fixing&lt;/li&gt;

&lt;li&gt;&lt;a href='https://github.com/snowplow/snowplow/issues/102'&gt;Issue #102&lt;/a&gt; - there was a trailing space in our initialization code, &lt;a href='https://github.com/snowplow/snowplow/blob/master/1-trackers/javascript-tracker/js/src/init.js'&gt;&lt;code&gt;init.js&lt;/code&gt;&lt;/a&gt;, which could cause some issues in Internet Explorer. Many thanks to community member Alan Z for raising&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The JavaScript Tracker&amp;#8217;s API and the Tracker Protocol are unchanged. We recommend upgrading to using the new JavaScript Tracker version 0.8.1 over the previous 0.8.0.&lt;/p&gt;

&lt;p&gt;Finally, if you haven&amp;#8217;t yet read our tutorial on &lt;a href='https://github.com/snowplow/snowplow/wiki/Integrating-javascript-tags-with-Google-Tag-Manager'&gt;Integrating the JavaScript Tracker with Google Tag Manager&lt;/a&gt;, we would recommend taking a look - it makes these sorts of upgrades much easier.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/11/27/snowplow-0.6.1-released</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/11/27/snowplow-0.6.1-released"/>
    <title>Snowplow 0.6.1 released, with lots of small improvements</title>
    <updated>2012-11-27T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We&amp;#8217;re happy to announce our next Snowplow release - version &lt;strong&gt;0.6.1&lt;/strong&gt;. This release includes updates:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Additional data collection&lt;/strong&gt;. The Javascript tracker has been updated to capture additional data points, including a user fingerprint (which can be used as a &lt;code&gt;user_id&lt;/code&gt; for companies tracking users across domains), the tracker version, browser timezone and color depth&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Javascript tracker updates&lt;/strong&gt;. A number of updates have been made to make the Javascript tracker more robust&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Updates to the ETL flow&lt;/strong&gt; so that the &lt;code&gt;user_agent&lt;/code&gt; string and &lt;code&gt;platform&lt;/code&gt; captured and stored in Hive / Infobright&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Improved EmrEtlRunner command line options&lt;/strong&gt; now provide more flexibility when writing your data to storage&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Bug fixes&lt;/strong&gt; related to loading Snowplow data into Infobright&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Before we start - a big thanks to the community members who helped out on this release in a big way:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href='https://github.com/ramn'&gt;Simon Andersson&lt;/a&gt; @ &lt;a href='http://www.qwaya.com'&gt;Qwaya&lt;/a&gt; substantially re-factored the JavaScript tracker, splitting it into multiple smaller files, which made our work significantly easier :-)&lt;/li&gt;

&lt;li&gt;&lt;a href='https://github.com/moncaubeig'&gt;Gilles Moncaubeig&lt;/a&gt; @ &lt;a href='http://en.overblog.com/'&gt;OverBlog&lt;/a&gt; contributed the user fingerprinting code - thanks Gilles!&lt;/li&gt;

&lt;li&gt;&lt;a href='https://github.com/mtibben'&gt;Michael Tibben&lt;/a&gt; @ &lt;a href='http://99designs.com'&gt;99designs&lt;/a&gt; continued his great work on EmrEtlRunner with improved command line options&lt;/li&gt;
&lt;/ul&gt;
&lt;!--more--&gt;
&lt;h2 id='javascript_tracker_updates'&gt;JavaScript tracker updates&lt;/h2&gt;

&lt;p&gt;We have released a new version of the JavaScript tracker, &lt;strong&gt;0.8.0&lt;/strong&gt;. As always, we are hosting this new version on CloudFront if you don&amp;#8217;t want to host it yourself:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http(s)://d1fc8wv8zag5ca.cloudfront.net/0.8.0/sp.js&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But before you update your tags, we need to share a few important things about this new version, including a &lt;strong&gt;breaking change&lt;/strong&gt;:&lt;/p&gt;

&lt;h3 id='changes_to_the_javascript_api'&gt;Changes to the JavaScript API&lt;/h3&gt;

&lt;p&gt;Three main changes have been made to the JavaScript tracker&amp;#8217;s API - please note that the first is a &lt;strong&gt;breaking change&lt;/strong&gt;, while the other two deprecate some existing functions (and introduce new ones):&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The tracker now GETs &lt;code&gt;i&lt;/code&gt; not &lt;code&gt;ice.png&lt;/code&gt; &lt;strong&gt;(BREAKING CHANGE)&lt;/strong&gt; - version 0.8.0 of the JavaScript tracker now GETs a transparent 1x1 GIF called &lt;code&gt;i&lt;/code&gt;, no longer &lt;code&gt;ice.png&lt;/code&gt;. If you are using the CloudFront Collector, you &lt;strong&gt;must&lt;/strong&gt; upload our &lt;a href='https://github.com/snowplow/snowplow/blob/master/2-collectors/cloudfront-collector/static/i?raw=true'&gt;&lt;code&gt;i&lt;/code&gt;&lt;/a&gt; pixel to sit in your S3 bucket alongside &lt;code&gt;ice.png&lt;/code&gt;. Don&amp;#8217;t forget to make it publically readable. &lt;a href='https://github.com/shermozle/SnowCannon'&gt;SnowCannon&lt;/a&gt;, the node.js collector, already supports &lt;code&gt;i&lt;/code&gt;&lt;/li&gt;

&lt;li&gt;The &lt;code&gt;setAccount()&lt;/code&gt; function was badly named. We have added a new function, &lt;code&gt;setCollectorCf()&lt;/code&gt;, which does the exact same thing, and we will remove &lt;code&gt;setAccount()&lt;/code&gt; in a future version. If you continue to use &lt;code&gt;setAccount()&lt;/code&gt;, then a warning message will be printed to &lt;code&gt;console.log&lt;/code&gt;, but it will still work&lt;/li&gt;

&lt;li&gt;The &lt;code&gt;getTracker()&lt;/code&gt; function (which not many people need to use) was badly named. As wth point 2 above: we have added &lt;code&gt;getTrackerCf()&lt;/code&gt; and &lt;code&gt;getTrackerUrl()&lt;/code&gt;, and deprecated &lt;code&gt;getTracker()&lt;/code&gt; for now&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;As always, our JavaScript tracker&amp;#8217;s current API is fully documented on our Wiki, on the &lt;a href='https://github.com/snowplow/snowplow/wiki/javascript-tracker'&gt;JavaScript Tracker&lt;/a&gt; page.&lt;/p&gt;

&lt;h3 id='new_tracking_data'&gt;New tracking data&lt;/h3&gt;

&lt;p&gt;Version 0.8.0 of the JavaScript tracker now passes additional data along to the Snowplow collector. This additional data is as follows:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Tracker version&lt;/strong&gt; - so you will always know which version of the JavaScript tracker generated a given Snowplow event&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;User fingerprint&lt;/strong&gt; - we are still working on a new collector which supports cross-domain user tracking. In the meantime, we are releasing an experimental feature: a &amp;#8216;user fingerprint&amp;#8217; based on various (hopefully unique) attributes of the user&amp;#8217;s browser. Many thanks to &lt;a href='https://github.com/moncaubeig'&gt;Gilles&lt;/a&gt; for contributing this feature; to read more about this, please take a look at &lt;a href='https://github.com/snowplow/snowplow/issues/70'&gt;issue #70&lt;/a&gt; in GitHub&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Timezone&lt;/strong&gt; - tells you what timezone the user in, recording the &lt;a href='http://en.wikipedia.org/wiki/Tz_database'&gt;Olsen key&lt;/a&gt; for the user&amp;#8217;s timezone&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Color depth&lt;/strong&gt; - the bit depth of the browser&amp;#8217;s color palette for displaying images (in bits per pixel)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We have updated the ETL and storage systems (e.g. Hive and Infobright table definitions) to extract and store these new fields.&lt;/p&gt;

&lt;p&gt;We have updated the &lt;a href='https://github.com/snowplow/snowplow/wiki/snowplow-tracker-protocol'&gt;Snowplow Tracker Protocol&lt;/a&gt; page on our Wiki with these additions.&lt;/p&gt;

&lt;h2 id='etl_and_storage_improvements'&gt;ETL and storage improvements&lt;/h2&gt;

&lt;p&gt;This release includes various improvements to Snowplow ETL and storage which are unrelated to the JavaScript tracker changes above. To break these down:&lt;/p&gt;

&lt;h3 id='additional_data_being_saved'&gt;Additional data being saved&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Useragent&lt;/strong&gt; - the raw browser useragent is now being logged in a new &lt;code&gt;useragent&lt;/code&gt; field. Previously we were throwing this useful data away&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Platform&lt;/strong&gt; - the tracker&amp;#8217;s &lt;code&gt;platform&lt;/code&gt; type is now extracted and stored. The JavaScript tracker always sets this field to &lt;code&gt;web&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id='bug_fixes'&gt;Bug fixes&lt;/h3&gt;

&lt;p&gt;Just one bug fix - in the StorageLoader, we changed the field encloser for Infobright to &lt;code&gt;NULL&lt;/code&gt;, where previously it was &lt;code&gt;&amp;#39;&amp;#39;&lt;/code&gt; (two empty quotes). This was to fix &lt;a href='https://github.com/snowplow/snowplow/issues/88'&gt;issue #88&lt;/a&gt;, where Infobright was throwing an error and dying if a field&amp;#8217;s value started with a double-quote.&lt;/p&gt;

&lt;h3 id='improved_emretlrunner_commandline_options'&gt;Improved EmrEtlRunner command-line options&lt;/h3&gt;

&lt;p&gt;EmrEtlRunner now has some improved command-line options:&lt;/p&gt;

&lt;p&gt;Firstly, the &lt;code&gt;--skip&lt;/code&gt; argument now can take a list of individual steps to skip. So for example you could run &lt;strong&gt;only&lt;/strong&gt; the Hive job with the command-line option:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ bundle exec snowplow-emr-etl-runner --skip staging,archive --config ./config.yml &lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Secondly, there is now a new option, &lt;code&gt;--process-bucket&lt;/code&gt;. This runs the Hive job only on the contents of the specified bucket. This implies &lt;code&gt;--skip staging,archive&lt;/code&gt; as well. An example of usage:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ bundle exec snowplow-emr-etl-runner --process-bucket s3n://my-logs-to-process --config ./config.yml&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Many thanks to &lt;a href='https://github.com/mtibben'&gt;Mike Tibben&lt;/a&gt; for contributing these new options!&lt;/p&gt;

&lt;h3 id='placeholders_for_event_and_event_id'&gt;Placeholders for event and event_id&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Event&lt;/strong&gt; - we have renamed the &lt;code&gt;event_name&lt;/code&gt; field in Infobright to simply &lt;code&gt;event&lt;/code&gt;. This is still a placeholder (it will be populated in a future version of Snowplow)&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Event ID&lt;/strong&gt; - there has been some confusion over the uniqueness of the current &lt;code&gt;txn_id&lt;/code&gt; field - see &lt;a href='https://github.com/snowplow/snowplow/issues/89'&gt;issue #89&lt;/a&gt; for the discussion. We plan on adding a properly unique &lt;code&gt;event_id&lt;/code&gt; for each event in the ETL layer; in the meantime we have added the &lt;code&gt;event_id&lt;/code&gt; field in as a placeholder&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id='upgrading_to_the_new_version'&gt;Upgrading to the new version&lt;/h2&gt;

&lt;h3 id='tracker_and_collector'&gt;Tracker and collector&lt;/h3&gt;

&lt;p&gt;We have discussed above how to update your JavaScript tracker and CloudFront collector to support this new version 0.6.1. If you are using &lt;a href='https://github.com/shermozle/SnowCannon'&gt;SnowCannon&lt;/a&gt;, the node.js collector, you don&amp;#8217;t have to modify it - it already supports the new &lt;code&gt;i&lt;/code&gt; pixel.&lt;/p&gt;

&lt;h3 id='etl'&gt;ETL&lt;/h3&gt;

&lt;p&gt;To upgrade your ETL system, first re-deploy EmrEtlRunner from GitHub as per the &lt;a href='https://github.com/snowplow/snowplow/wiki/deploying-emretlrunner'&gt;EmrEtlRunner Setup Guide&lt;/a&gt;, and then update the ETL dependencies at the bottom of your &lt;code&gt;config.yml&lt;/code&gt; file like so:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:snowplow:
  :serde_version: 0.5.2
  :hive_hiveql_version: 0.5.1
  :non_hive_hiveql_version: 0.0.3&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id='storage'&gt;Storage&lt;/h3&gt;

&lt;h4 id='hive'&gt;Hive&lt;/h4&gt;

&lt;p&gt;If you are only using Hive for storage and analytics, you do not need to do anything to support this new release - because we add all new fields to the end of the file format, and field renames (like &lt;code&gt;event_name&lt;/code&gt; to &lt;code&gt;event&lt;/code&gt;) don&amp;#8217;t affect Hive on Amazon EMR.&lt;/p&gt;

&lt;h4 id='infobright'&gt;Infobright&lt;/h4&gt;

&lt;p&gt;If you are using Infobright Community Edition for analysis, you will need to update your table definition. This is a little complex, because Infobright does not support in-place table or column renames. To make this easier for you, we have created a script:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;4-storage/infobright-storage/migrate_to_003.sh&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Running this script will create a new table, &lt;code&gt;events_003&lt;/code&gt; (version 0.0.3 of the table definition) in your &lt;code&gt;snowplow&lt;/code&gt; database, copying across all your data from your existing &lt;code&gt;events&lt;/code&gt; table, which will not be modified in any way.&lt;/p&gt;

&lt;p&gt;Once you have run this, don&amp;#8217;t forget to update your StorageLoader&amp;#8217;s &lt;code&gt;config.yml&lt;/code&gt; to load into the new &lt;code&gt;events_003&lt;/code&gt; table, not your old &lt;code&gt;events&lt;/code&gt; table:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:storage:
  :type: infobright
  :database: snowplow
  :table:    events_003 # NOT &amp;quot;events&amp;quot; any more&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Done!&lt;/p&gt;

&lt;h2 id='getting_help'&gt;Getting help&lt;/h2&gt;

&lt;p&gt;If you have any problems with version 0.6.1, please &lt;a href='https://github.com/snowplow/snowplow/issues'&gt;raise an issue&lt;/a&gt; or get in touch with us via &lt;a href='https://github.com/snowplow/snowplow/wiki/Talk-to-us'&gt;the usual channels&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;And do let us know if the new features - such as user fingerprinting - are useful!&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/11/16/integrating-snowplow-with-google-tag-manager</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/11/16/integrating-snowplow-with-google-tag-manager"/>
    <title>Integrating Snowplow with Google Tag Manager</title>
    <updated>2012-11-16T00:00:00+00:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;A month and a half ago, Google launched &lt;a href='http://www.google.com/tagmanager/'&gt;Google Tag Manager&lt;/a&gt; (GTM), a free tag management solution. That was a defining moment in tag management history as it will no doubt bring tag management, until now the preserve of big enterprises, into the mainstream.&lt;/p&gt;

&lt;p&gt;&lt;img alt='gtm' src='/static/img/gtm.JPG' /&gt;&lt;/p&gt;

&lt;p&gt;We have spent some time testing how to get Snowplow tags working well with Google Tag Manager, and have documented our recommended approach to setting up Snowplow with GTM on the &lt;a href='https://github.com/snowplow/snowplow/wiki/Integrating%20javascript%20tags%20with%20Google%20Tag%20Manager#wiki-snowplow-setup'&gt;wiki&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In the course of reading the literature on tag management and Google Tag Manager in particular, we were struck by a number of issues and misconceptions:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Setting up a Tag Management System is a big and complicated job. Much of the literature (especially around Google Tag Manager), and discussion (especially amongst members of the web analytics community) suggests it is very easy. Whilst Google Tag Manager is a straightforward product to use, the process of setting it up is difficult, because it involves thinking through, in a very rigorous way, exactly what data should be passed between the website and GTM. If this is not done properly, GTM will not have the relevant data to pass on to the tags it manages, including Snowplow.&lt;/li&gt;

&lt;li&gt;Exacerbating the above, there is a lack of detailed literature on how to go about identifying all the data points that should be passed between your website and tag management solution. Nearly all the guides to setting up GTM that we reviewed covered only the most basic of GTM setups, which is just enough to enable page tracking. Clearly, that is not going to be sufficient for anything but the simplest blogs and brochureware sites.&lt;/li&gt;

&lt;li&gt;One of the tag management features most regularly trumpetted by digital and marketing analysts is that it frees them from having to liaise with their webmasters to add new tags and change existing tag setups. Even with a tag management solution in place, however, this may still be necessary if not all the data that the analyst wants passed through to their analytics package is available in the tag maangement system.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;As a step towards addressing the above issues, we have produced a step-by-step guide to both &lt;a href='https://github.com/snowplow/snowplow/wiki/Integrating%20javascript%20tags%20with%20Google%20Tag%20Manager#wiki-setup-gtm'&gt;setting up Google Tag Manager&lt;/a&gt; and &lt;a href='https://github.com/snowplow/snowplow/wiki/Integrating%20javascript%20tags%20with%20Google%20Tag%20Manager#wiki-snowplow-setup'&gt;setting up Snowplow within GTM&lt;/a&gt;. We hope you find it useful. We plan to produce a similar guide to setting up Snowplow within &lt;a href='http://www.opentag.qubitproducts.com/'&gt;Qubit&amp;#8217;s OpenTag solution&lt;/a&gt; in due course.&lt;/p&gt;

&lt;p&gt;We&amp;#8217;re very excited by Google&amp;#8217;s launch of Tag Manager, and recommend all new Snowplow users who are not currently using a tag management system integrate one as part of their Snowplow setup. Specifically:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The exercise that companies need to go through when setting up a tag management platform i.e. thinking through all the data points that they want to pass to their tag management system so that they can be passed on to whatever tags are managed in the system, is the same process they should go through when they setup Snowplow, with a view to enabling the widest possible set of analyses on their web analytics data. So even though that exercise is not easy, it is valuable&lt;/li&gt;

&lt;li&gt;The processing of mapping that data from the structure defined in the tag management system to one which works with Snowplow&amp;#8217;s data structure is the exact reverse of the analysis process that takes Snowplow data and transforms it back into the structure that&amp;#8217;s most natural for the company performing the analysis.&lt;/li&gt;

&lt;li&gt;Once the tag management system has been installed, it becomes easy to upgrade the tags and / or change the configuration. There are a number of improvements we plan to make to our &lt;a href='https://github.com/snowplow/snowplow/tree/master/1-trackers/javascript-tracker'&gt;Javascript tracker&lt;/a&gt;, and having a tag management program will make it easier for companies to take advantage of those upgrades.&lt;/li&gt;
&lt;/ol&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/11/12/snowplow-0.6.0-released-with-storage-loader</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/11/12/snowplow-0.6.0-released-with-storage-loader"/>
    <title>Snowplow 0.6.0 released, with the new StorageLoader</title>
    <updated>2012-11-12T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We&amp;#8217;re very pleased to start the week by releasing a new version of Snowplow - version &lt;strong&gt;0.6.0&lt;/strong&gt;. This is a big release for us - as it includes the first version of our all-new StorageLoader. The release also includes a small set of tweaks and bug fixes across the existing Snowplow components, but let&amp;#8217;s start by introducing StorageLoader:&lt;/p&gt;

&lt;h2 id='introducing_storageloader'&gt;Introducing StorageLoader&lt;/h2&gt;

&lt;p&gt;Up until now, Snowplow has stored all its data in S3, where it can be queried in Hive. However, our vision with Snowplow has always been to enable to the broadest set of analyses on Snowplow data as possible. That means making it as easy as possible to keep up to date versions of Snowplow data in many different types of database. The StorageLoader is a key component to fulfilling that vision.&lt;/p&gt;

&lt;p&gt;&lt;img alt='snowplow-loader-image' src='/static/img/SnowplowLoader.jpg' /&gt;&lt;/p&gt;

&lt;p&gt;StorageLoader is a Ruby application that downloads Snowplow event files from S3 and loads them into an alternative database. It has been built to make keeping an up to date version of your Snowplow data in other databases as easy as possible. Currently, it only supports loading the data into &lt;a href='http://www.infobright.org/'&gt;Infobright Community Edition (ICE)&lt;/a&gt; - a high-performance columnar database based on MySQL. However, we plan to extend it over the next few months to support a range of other databases including:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href='https://developers.google.com/bigquery'&gt;Google Big Query&lt;/a&gt; for fast analysis of massive data sets. (This could be very powerful for rapid analytics across Snowplow&amp;#8217;s granular data)&lt;/li&gt;

&lt;li&gt;&lt;a href='http://skydb.io'&gt;SkyDB&lt;/a&gt; for event path analysis and other, broader types of event stream analytics&lt;/li&gt;

&lt;li&gt;&lt;a href='http://www.postgresql.org'&gt;PostgreSQL&lt;/a&gt; for web analytics for web properties where the levels of traffic are not Facebook-scale&lt;/li&gt;
&lt;/ul&gt;
&lt;!--more--&gt;
&lt;p&gt;There are significant advantages to storing data in Infobright instead of (or as well as) S3:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;In many cases, query times are much faster&lt;/li&gt;

&lt;li&gt;There are a wide range of analytics tools that plug directly into Infobright. (Any tool that plugs into MySQL.) These can now be run directly on top of Snowplow data. (These tools include R and Tableau.)&lt;/li&gt;

&lt;li&gt;For more details on the pros and cons of storage in S3 vs Infobright, see our &lt;a href='https://github.com/snowplow/snowplow/wiki/choosing-a-storage-module'&gt;guide to choosing between the two&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As you can hopefully get a sense looking at our roadmap for other databases to support, there are obvious advantages to using some of the other databases on our roadmapGoing forwards, we expect that many companies using Snowplow will store that Snowplow data in more than one store, to enable a very broad range of analytics from different types of tools.&lt;/p&gt;

&lt;h2 id='using_the_storageloader'&gt;Using the StorageLoader&lt;/h2&gt;

&lt;p&gt;You can configure StorageLoader with the details of the Infobright table to insert your Snowplow events into, and then you schedule StorageLoader (e.g. in a cronjob) to regularly download your Snowplow events and load them into Infobright. StorageLoader can run as soon as EmrEtlRunner has completed its job (and we include a script to run both in one go).&lt;/p&gt;

&lt;p&gt;With this setup, you will have your Snowplow events easily accessible and queryable in a local Infobright instance - but you can still fall back to querying the data in Hive if you wish.&lt;/p&gt;

&lt;p&gt;The following setup guides should be helpful in terms of setting up StorageLoader:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href='https://github.com/snowplow/snowplow/wiki/infobright-storage-setup'&gt;Infobright storage setup guide&lt;/a&gt; walks you through the process of installing Infobright and setting it up to house Snowplow data&lt;/li&gt;

&lt;li&gt;&lt;a href='https://github.com/snowplow/snowplow/wiki/StorageLoader-setup'&gt;StorageLoader setup guide&lt;/a&gt; walks you through installing and configuring StorageLoader to regularly load Snowplow data into Infobright&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id='the_codebase'&gt;The codebase&lt;/h2&gt;

&lt;p&gt;If you want to take a look at the code, you can find it in the main repository here: &lt;a href='https://github.com/snowplow/snowplow/tree/master/4-storage/storage-loader'&gt;4-storage/storage-loader/&lt;/a&gt;&lt;/p&gt;

&lt;h2 id='getting_help'&gt;Getting help&lt;/h2&gt;

&lt;p&gt;If you have any problems getting StorageLoader working, please &lt;a href='https://github.com/snowplow/snowplow/issues'&gt;raise an issue&lt;/a&gt; or get in touch with us via &lt;a href='https://github.com/snowplow/snowplow/wiki/Talk-to-us'&gt;the usual channels&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id='other_fixes_in_060'&gt;Other fixes in 0.6.0&lt;/h2&gt;

&lt;p&gt;We have made a number of other fixes across Snowplow to prepare the ground for StorageLoader:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;EmrEtlRunner&lt;/strong&gt; has been bumped to 0.0.5, including upgrading it to Sluice 0.0.4 (which has some bug fixes around S3 path handling).&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;Hive deserializer&lt;/strong&gt; has been bumped to 0.5.1, and now outputs booleans such as &lt;code&gt;br_cookies&lt;/code&gt; as 0 or 1 (instead of true or false) for the non-Hive output.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;non-Hive format HiveQL script&lt;/strong&gt; has been bumped to 0.0.2 and now uses the new 0 or 1 approach to booleans. This is necessary so that true/false values can be successfully loaded into Infobright.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;setup_infobright.sql&lt;/strong&gt; script has been bumped to 0.0.2 - we have changed the columns defined as booleans to be tinyint(1)s. This is just a formality, because Infobright creates &amp;#8216;boolean&amp;#8217; columns as tinyint(1)s anyway.&lt;/p&gt;

&lt;p&gt;We will keep you posted as we roll out support for additional database options in StorageLoader! (And welcome suggestinos for other databases we should build support for.)&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/11/06/snowplow-0.5.2-released-and-introducing-sluice</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/11/06/snowplow-0.5.2-released-and-introducing-sluice"/>
    <title>Snowplow 0.5.2 released, and introducing the Sluice Ruby gem</title>
    <updated>2012-11-06T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;Another week, another release: Snowplow &lt;strong&gt;0.5.2&lt;/strong&gt;! This is a small release, consisting just of a small set of bug fixes and improvements to EmrEtlRunner - although we&amp;#8217;ll also use this post to introduce our new Ruby gem, called Sluice.&lt;/p&gt;

&lt;p&gt;Many thanks to community member &lt;a href='https://github.com/testower'&gt;Tom Erik Støwer&lt;/a&gt; for his testing of EmrEtlRunner over the weekend, which helped us to identify and fix these bugs:&lt;/p&gt;

&lt;h2 id='bugs_fixed'&gt;Bugs fixed&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href='https://github.com/snowplow/snowplow/issues/71'&gt;Issue 71&lt;/a&gt;&lt;/strong&gt;: the template &lt;code&gt;config.yml&lt;/code&gt; (in the GitHub repo and in the wiki) was specifying an out-of-date version for the Hive deserializer. We have updated this to specify version &lt;strong&gt;0.5.0&lt;/strong&gt; of the serde, like so:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;...
:snowplow:
  :serde_version: 0.4.9
...&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href='https://github.com/snowplow/snowplow/issues/72'&gt;Issue 72&lt;/a&gt;&lt;/strong&gt;: Tom&amp;#8217;s testing also identified a bug in EmrEtlRunner&amp;#8217;s log archiving, which only occurs if the Processing Bucket contains sub-folders. This has now been fixed too.&lt;/p&gt;
&lt;!--more--&gt;
&lt;h2 id='a_new_feature_skip'&gt;A new feature: &amp;#8211;skip&lt;/h2&gt;

&lt;p&gt;A new release which only contains bug fixes is a boring release, so we have also implemented a new &lt;code&gt;--skip&lt;/code&gt; option for EmrEtlRunner (&lt;a href='https://github.com/snowplow/snowplow/issues/58'&gt;issue #58&lt;/a&gt;). You can use this when you call EmrEtlRunner like so:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ bundle exec snowplow-emr-etl-runner &amp;lt;...&amp;gt; --skip staging OR --skip emr&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This option skips the work steps &lt;strong&gt;up to and including&lt;/strong&gt; the specified step. To give an example: &lt;code&gt;--skip emr&lt;/code&gt; skips both moving the raw logs to the Staging Bucket &lt;strong&gt;and&lt;/strong&gt; running the ETL process on Amazon EMR, i.e. EmrEtlRunner will &lt;strong&gt;only&lt;/strong&gt;* perform the final archiving step.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;--skip&lt;/code&gt; is useful if you encounter a problem midway through your ETL process: you can fix the problem and then skip the steps which ran okay, rather than re-processing from the start. We find it especially helpful when we&amp;#8217;re testing new versions of EmrEtlRunner.&lt;/p&gt;

&lt;h2 id='and_introducing_sluice'&gt;And introducing Sluice&lt;/h2&gt;

&lt;p&gt;At Snowplow Analytics we are committed to making our software as modular and loosely-coupled as possible. Where we have functionality which could be more widely used, we aim to extract it into standalone modules for developers to use even if they are not implementing Snowplow.&lt;/p&gt;

&lt;p&gt;We have followed this approach with the parallel file-copy code for Amazon S3 added to EmrEtlRunner by community member &lt;a href='https://github.com/mtibben'&gt;Michael Tibben&lt;/a&gt; from &lt;a href='http://99designs.com'&gt;99designs&lt;/a&gt;: we have moved this code out of EmrEtlRunner into a new Ruby gem, called Sluice. Sluice now has its own &lt;a href='https://github.com/snowplow/sluice'&gt;GitHub repository&lt;/a&gt;, and has been published on &lt;a href='http://rubygems.org/gems/sluice'&gt;RubyGems.org&lt;/a&gt;. It&amp;#8217;s called Sluice because, like &lt;a href='https://github.com/cwensel'&gt;Chris Wensel&lt;/a&gt; (Cascading), we believe in flowing-water metaphors for ETL tools :-)&lt;/p&gt;

&lt;p&gt;Sluice is used by our EmrEtlRunner, and is also a dependency for the StorageLoader Ruby application which we are currently developing.&lt;/p&gt;

&lt;p&gt;We hope to build out Sluice as a general-purpose Ruby toolkit for cloud-friendly ETL over the coming months - and would love contributors! Our view is that, in a world of cloud services like Amazon S3, Google BigQuery and Elastic MapReduce, it makes most sense to take a programmatic approach to ETL, rather than contort the historic, application-based approach of &lt;a href='http://www.talend.com'&gt;Talend&lt;/a&gt;, &lt;a href='http://www.pentaho.com/explore/pentaho-data-integration/'&gt;Pentaho DI&lt;/a&gt; et al. We see Sluice as part of that toolkit for programmatic ETL, alongside great tools such as &lt;a href='http://www.cascading.org'&gt;Cascading&lt;/a&gt;, Rob Slifka&amp;#8217;s &lt;a href='https://github.com/rslifka/elasticity'&gt;Elasticity&lt;/a&gt; and &lt;a href='http://palletops.com'&gt;Pallet&lt;/a&gt;.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/11/01/snowplow-0.5.1-released</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/11/01/snowplow-0.5.1-released"/>
    <title>Snowplow 0.5.1 released, with lots of small improvements</title>
    <updated>2012-11-01T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We have just released Snowplow &lt;strong&gt;0.5.1&lt;/strong&gt;! Rather than one large new feature, version 0.5.1 is an incremental release which contains lots of small fixes and improvements to the ETL and storage sub-systems. The two big themes of these updates are:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Improving the robustness of the ETL process&lt;/li&gt;

&lt;li&gt;Laying the foundations for loading Snowplow events into &lt;a href='http://www.infobright.org/'&gt;Infobright Community Edition&lt;/a&gt; (ICE)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To take each of these themes in turn:&lt;/p&gt;

&lt;h2 id='1_a_more_robust_etl_process'&gt;1. A more robust ETL process&lt;/h2&gt;

&lt;p&gt;The Hive deserializer now has improved error handling - many thanks to community member &lt;a href='https://github.com/mtibben'&gt;Michael Tibben&lt;/a&gt; from &lt;a href='http://99designs.com'&gt;99designs&lt;/a&gt; for his help here!&lt;/p&gt;

&lt;p&gt;Firstly, the Hive deserializer is now setup to log warnings (rather than die) on non-critical data quality issues.&lt;/p&gt;

&lt;p&gt;Additionally, there is now an option (switched off by default) to continue processing even on unexpected row-level errors (such as an input file not matching the expected CloudFront format). We have added a configuration option to the EmrEtlRunner&amp;#8217;s &lt;a href='https://github.com/snowplow/snowplow/blob/master/3-etl/emr-etl-runner/config/config.yml'&gt;configuration file&lt;/a&gt; to support this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:etl:
  :continue_on_unexpected_error: false&lt;/code&gt;&lt;/pre&gt;
&lt;!--more--&gt;
&lt;p&gt;Switch this to &amp;#8216;true&amp;#8217; to continue processing on unexpected row-level errors.&lt;/p&gt;

&lt;h2 id='2_groundwork_for_infobright_compatibility'&gt;2. Groundwork for Infobright compatibility&lt;/h2&gt;

&lt;p&gt;We have added a table definition (and supporting scripts) for setting up a Snowplow events table in Infobright - you can find these in the main repository under &lt;a href='https://github.com/snowplow/snowplow/tree/master/4-storage/infobright-storage'&gt;&lt;code&gt;snowplow/4-storage/infobright-storage&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Some early ETL design decisions meant that the Snowplow event files being generated before 0.5.1 were not compatible with being loaded into Infobright (or similar relational databases like Postgres or MySQL). We have made some updates to the ETL process in 0.5.1 to fix this:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;In the Hive deserializer, we now convert tabs to 4 spaces to prevent a stray tab from breaking our load into Infobright&lt;/li&gt;

&lt;li&gt;Databases like Infobright don&amp;#8217;t support Hive&amp;#8217;s &lt;code&gt;ARRAY&amp;lt;STRING&amp;gt;&lt;/code&gt; syntax, so we have updated the Hive deserializer to also output individual booleans for the browser features, alongside the browser features array&lt;/li&gt;

&lt;li&gt;We have created a new HiveQL script which outputs Snowplow event files in a format which can be easily loaded into Infobright - this is called &lt;a href='https://github.com/snowplow/snowplow/blob/master/3-etl/hive-etl/hiveql/non-hive-rolling-etl.q'&gt;&lt;code&gt;non-hive-rolling-etl.q&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;We have added a configuration option to the EmrEtlRunner&amp;#8217;s &lt;a href='https://github.com/snowplow/snowplow/blob/master/3-etl/emr-etl-runner/config/config.yml'&gt;configuration file&lt;/a&gt; so that you can choose whether to output Hive-format or non-Hive-format event files&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;On point 4: we believe that most people will want to load their Snowplow event files into other database systems, such as Infobright (or eventually, Postgres, Google BigQuery, SkyDB etc). Therefore, the default setting for the &lt;a href='https://github.com/snowplow/snowplow/blob/master/3-etl/emr-etl-runner/config/config.yml'&gt;configuration option&lt;/a&gt; in the EmrEtlRunner is to output your Snowplow event files in the non-Hive-format:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:etl:
  :storage_format: non-hive&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As the comment says, if you will &lt;strong&gt;only&lt;/strong&gt; be doing analysis in Hive, you could switch this setting to &amp;#8216;hive&amp;#8217; and benefit from the slightly-tweaked, Hive-friendly file format.&lt;/p&gt;

&lt;h2 id='getting_help'&gt;Getting help&lt;/h2&gt;

&lt;p&gt;If you have any problems getting version 0.5.1 working, please &lt;a href='https://github.com/snowplow/snowplow/issues'&gt;raise an issue&lt;/a&gt; or get in touch with us via &lt;a href='https://github.com/snowplow/snowplow/wiki/Talk-to-us'&gt;the usual channels&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id='in_the_pipeline'&gt;In the pipeline&lt;/h2&gt;

&lt;p&gt;At Snowplow we want to support multiple different storage and analytics options for Snowplow events, alongside our current Hive-based approach. This version 0.5.1 provides the building blocks for our Infobright support - for the next release, we are working on a Storage Loader component to download your event files from Amazon S3 and load them into your local Infobright instance. We&amp;#8217;ll keep you posted on our progress here!&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/10/31/snowplow-in-a-universal-analytics-world-what-the-new-version-of-google-analytics-means-for-companies-adopting-snowplow</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/10/31/snowplow-in-a-universal-analytics-world-what-the-new-version-of-google-analytics-means-for-companies-adopting-snowplow"/>
    <title>Snowplow in a Universal Analytics world - what the new version of Google Analytics means for companies adopting Snowplow</title>
    <updated>2012-10-31T00:00:00+00:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;Earlier this week, Google announced a series of significant advances in Google Analytics at the GA Summit, that are collectively referred to as &lt;a href='http://cutroni.com/blog/2012/10/29/universal-analytics-the-next-generation-of-google-analytics/'&gt;Universal Analytics&lt;/a&gt;. In this post, we look at:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href='#what'&gt;The actual features Google has announced&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2012/10/31/snowplow-proposition-in-a-universal-analytics-world-what-the-new-version-of-ga-means-for-snowplow-adoption#whysnowplow'&gt;How those advances change the case for companies considering adopting Snowplow&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img alt='universal-analytics-image' src='/static/img/google-analytics-universal-analytics.png' /&gt;&lt;/p&gt;
&lt;a name='what' /&gt;
&lt;h2 id='1_what_changes_has_google_announced'&gt;1. What changes has Google announced?&lt;/h2&gt;

&lt;p&gt;The most significant change Google has announced is the new &lt;a href='https://developers.google.com/analytics/devguides/collection/protocol/v1/'&gt;Measurement Protocol&lt;/a&gt;, which enables businesses using GA to capture much more data. This will make it possible for Google to deliver a much broader range of reports, of higher business value, than was previously possible. To understand the changes, we start by considering what &lt;a href='#new-data-points'&gt;new data points&lt;/a&gt; businesses can &lt;em&gt;feed&lt;/em&gt; GA, before considering &lt;a href='/blog/2012/10/31/snowplow-proposition-in-a-universal-analytics-world-what-the-new-version-of-ga-means-for-snowplow-adoption#reporting-capabilities'&gt;what that means for GA&amp;#8217;s reporting capabilities&lt;/a&gt;.&lt;/p&gt;
&lt;a name='new-data-points' /&gt;
&lt;h3 id='11_custom_user_identifiers'&gt;1.1 Custom user identifiers&lt;/h3&gt;

&lt;p&gt;The first new data points that businesses can feed into Google Analytics is a user&amp;#8217;s &lt;code&gt;client_id&lt;/code&gt; (basically, a customer ID) as defined on the business&amp;#8217;s own systems.&lt;/p&gt;

&lt;p&gt;Previously, Google Analytics identified unique users using their own &lt;code&gt;cookie_id&lt;/code&gt;s. Google&amp;#8217;s &lt;code&gt;cookie_id&lt;/code&gt;s are an excellent starting point for identifying users, because so many users have Google accounts (thanks to their myriad mass-market services, including Gmail, YouTube, Google Play etc): consumers using these services on multiple devices identify themselves to Google when they login, meaning that Google can marry their &lt;code&gt;cookie_id&lt;/code&gt;s for these users on all the different devices they use. Our assumption is that Google already use this to reliably identify individual users across multiple platforms and devices.&lt;/p&gt;

&lt;p&gt;For businesses using GA, being able to augment Google&amp;#8217;s user identification with their own internal &lt;code&gt;client_id&lt;/code&gt;s is a significant step forwards for two reasons:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Many GA users (especially those with applications or properties where users login, or those with a loyalty card scheme) can identify their own users reliably on specific platforms, devices and physical stores. By adding this additional user identification data into GA, GA will be more accurate at identifying the same user in different places reliably, moving us further from a world in which we rely on persistent cookies dropped on browsers with unique identifiers, to one where users are more robustly identified via logins, payments and loyalty schemes. These approaches will still use cookies, but as part of a broader set of user identification business processes that actively involve the user in the identification process&lt;/li&gt;

&lt;li&gt;It should make it easier for GA users to join GA data with other customer data sets on those &lt;code&gt;client_id&lt;/code&gt;s. This is more of a nuanced point, as it was still possible previously to add customer IDs to GA as custom variables and use that to do joining&lt;/li&gt;
&lt;/ol&gt;
&lt;!--more--&gt;&lt;a name='customer-journey' /&gt;
&lt;h3 id='12_capturing_events_across_a_users_entire_customer_journey_not_just_the_web_not_just_digital_interactions'&gt;1.2 Capturing events across a user&amp;#8217;s entire customer journey (not just the web, not just digital interactions)&lt;/h3&gt;

&lt;p&gt;We have long argued that web analytics is just one customer data source - and that analysts performing customer analytics need to crunch data covering the customer&amp;#8217;s complete journey, including other digital channels and offline interactions. That means joining data sets from different digital products and offline data sets to generate a single customer view. To date, companies that have implemented &amp;#8220;single customer views&amp;#8221; have typically struggled incorporating web behaviour in those views.&lt;/p&gt;

&lt;p&gt;Google has taken a significant step towards enabling businesses to capture much more of their customer&amp;#8217;s journeys in Google Analytics itself. The &lt;a href='https://developers.google.com/analytics/devguides/collection/protocol/v1/'&gt;Measurement Protocol&lt;/a&gt; makes it possible to pass offline events into Google: so for example, when a customer buys an item in store, it would be possible to fire an event to Google Analytics recording that sale. If the customer was on a CRM programme (e.g. loyalty scheme), his / her &lt;code&gt;client_id&lt;/code&gt; could be passed in, and then Google Analytics would know that this is the same user who browsed the website on their mobile phone yesterday and viewed it from their office today, prior to coming in store to make the purchase.&lt;/p&gt;

&lt;p&gt;The Measurement Protocol can also be used to capture events on digital platforms that are not so well suited to traditional web analytics solution e.g. mobile applications, set-top box applications, videogames on consoles etc. It thus opens the door for Google Analytics to capture and report on event data from a range of devices, not just those that are web based.&lt;/p&gt;

&lt;p&gt;Taken together, this means it will be possible for Google Analytics to offer reports detailing customer behaviour across the complete customer journey. Building on this, it should also be possible for GA to enable analysts to calculate &lt;a href='/analytics/customer-analytics/customer-lifetime-value.html'&gt;customer lifetime value&lt;/a&gt; (if the value of different events was passed in with the events): this is one of the most important metrics in customer analytics, and one that has been conspicuous by its absence from web analytics outside of solutions like &lt;a href='/analytics/customer-analytics/customer-lifetime-value.html'&gt;Snowplow&lt;/a&gt; until now. The Measurement Protocol potentially means a huge increase in the scope and value of reports that it should be possible to generate in Google Analytics.&lt;/p&gt;

&lt;h3 id='13_capturing_customeracquisition_cost_data'&gt;1.3 Capturing customer-acquisition cost data&lt;/h3&gt;

&lt;p&gt;One of the most common types of analytics performed on web data is working out the return on marketing investment for different customer-acquisition channels. To perform this analysis, we need to combine:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Data on what was spent on each channel - typically cost data from those channels themselves, e.g. display, PPC, affiliate, social etc&lt;/li&gt;

&lt;li&gt;Web analytics data on how many people visited the website in response to those ads and what fraction of them went on to become customers. By dividing the total spent on each channel (1) by the number of customers acquired from each channel (2), we can calculate the cost of acquiring each customer for that channel.&lt;/li&gt;

&lt;li&gt;Financial data on the revenue/profits generated by those customers, over their lifetimes. By comparing the average value of each customer acquired from each channel against the average cost of acquiring each of those customers, we can calculate the return on that acquisition cost&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Now that Google lets businesses reliably track their users over their entire lifecycles (on and offline), it becomes possible to calculate the user&amp;#8217;s lifetime value, as detailed &lt;a href='#customer-journey'&gt;above&lt;/a&gt;, delivering on point #3. Google has always enabled business to capture #2. Now, Google lets you send the cost data into Google Analytics (point #1) - so that the return of each campaign can be accurately calculated. (Previously, only spend data from AdWords could be imported into GA.) With this information, companies should be better placed to drive marketing spend decisions based on Google Analytics reports. Again though, the reality is more nuanced, because typically those spend decisions have to be made &lt;em&gt;before&lt;/em&gt; a customer&amp;#8217;s lifetime value (#3) can be accurately calculated, so companies really need to develop predictive models of how valuable customers are likely to be. Anything but the most basic models are likely to require tools outside of GA to develop, and then pulling that data out of GA to power those models.&lt;/p&gt;

&lt;h3 id='14_custom_dimensions_and_metrics'&gt;1.4 Custom dimensions and metrics&lt;/h3&gt;

&lt;p&gt;The &lt;a href='https://developers.google.com/analytics/devguides/collection/protocol/v1/'&gt;Measurement Protocol&lt;/a&gt; enables businesses to define and capture their own dimensions and metrics each time an event that is tracked. Those additional metrics and dimensions are then available to report in in GA.&lt;/p&gt;

&lt;p&gt;As well as enabling businesses to add custom dimension and metric values to individual event tracking calls, Google also lets businesses bulk upload multiple dimensions at a time into the GA, if a relationship between those custom dimensions and dimensions already in GA can be defined, and GA knows what values to ascribe events already in it to those new dimensions, based on that defined relationship. To give an example: you could upload the product names/SKUs associated with each web page, enabling reporting on page views by SKU. Or, you could upload a range of product metadata (e.g. book titles and authors) and associate that with an ISBN custom field.&lt;/p&gt;
&lt;a name='reporting-capabilities' /&gt;
&lt;h3 id='15_what_new_reporting_is_enabled_through_the_capture_of_all_these_additional_data_points'&gt;1.5 What new reporting is enabled through the capture of all these additional data points?&lt;/h3&gt;

&lt;p&gt;Taken together, the additional data that businesses can feed into Google Analytics gives Google enough to offer a much broader and more valuable range of reporting than was previously possible:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Customer analytics&lt;/strong&gt;. We have long argued that web analytics packages including GA are too focused on sessions, page views and conversions, and neglect the broader, more valuable customer analytics that underpin the most successful businesses in the world. With these new data points, GA has the raw data to produce useful customer reports including customer lifetime value, and analysis of user behaviours over their entire journeys. No longer will web analysts using GA be confined to viewing actions over an isolated session: now they can slice and dice metrics by users over their user journeys spanning multiple site visits.&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Event analytics&lt;/strong&gt; across platforms, on and offline. GA can now report on user&amp;#8217;s complete journey, not just what they do on websites, but also their behaviours on other digital platforms (esp. mobile) and offline.&lt;/li&gt;
&lt;/ol&gt;
&lt;a name='whysnowplow' /&gt;
&lt;h2 id='2_how_do_the_advances_in_ga_change_the_case_for_adopting_snowplow'&gt;2. How do the advances in GA change the case for adopting Snowplow?&lt;/h2&gt;

&lt;p&gt;Prior to the latest announcement, the case for adopting Snowplow alongside your GA implementation was as follows:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The reporting provided by Google Analytics is very limited, with little/no customer analytics, catalogue analytics and platform analytics supported.&lt;/li&gt;

&lt;li&gt;Snowplow enables you to perform all these three types of analytics, by providing you with access to your raw customer-level and event-level clickstream data, so that you can use whatever analytics tool you like to crunch the data and perform that analysis&lt;/li&gt;

&lt;li&gt;Snowplow makes it easier to join your web analytics data sets with other data sets (e.g. marketing data sets, CRM and offline data sets), by enabling businesses to load customer IDs into Snowplow, and then perform the join on the raw data sets. This means that businesses running Snowplow can analyse user behaviour across their entire customer journey (on and offline, across all digital and non-digital channels)&lt;/li&gt;

&lt;li&gt;Snowplow makes it easy to warehouse your customer data for posterity: an asset which will doubtless grow in value over time.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Following the latest announcement, some of these arguments fall away:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Google has significantly strengthened its customer analytics capability. To what extent is not yet clear - we only know at this stage what extra data points Google Analytics will, hypothetically, let you collect - not what additional reporting UIs GA will provide to process that data&lt;/li&gt;

&lt;li&gt;The additional data points &lt;em&gt;should&lt;/em&gt; improve GA&amp;#8217;s platform and catalogue analytics capabilities; we will only be able to confirm this once we start working with the updated version of GA&lt;/li&gt;

&lt;li&gt;Therefore, the gap between what is possible with GA, and what is possible with Snowplow, has shrunk&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Nevertheless, the case for implementing Snowplow alongside GA is still compelling, for three main reasons. To take each of these in turn:&lt;/p&gt;

&lt;h3 id='21_analytics_capabilities'&gt;2.1 Analytics capabilities&lt;/h3&gt;

&lt;p&gt;There are several different considerations here:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Google Analytics still does not give you access to your customer-level and event-level data. Therefore, &lt;strong&gt;there will always be ways that you can crunch Snowplow data that you cannot accomplish in GA&lt;/strong&gt;: drilling down to segments of one visitor is just the most obvious example&lt;/li&gt;

&lt;li&gt;There are a range of analytics techniques which are hard to imagine Google implementing at all, even with the new data sets that are available. To give just three examples:&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
&lt;li&gt;Using machine learning techniques (e.g. &lt;a href='http://mahout.apache.org/'&gt;Mahout&lt;/a&gt;) to &lt;strong&gt;segment audience by behaviour&lt;/strong&gt;&lt;/li&gt;

&lt;li&gt;Performing &lt;strong&gt;event analytics&lt;/strong&gt; / pathing in a way that takes into account the &lt;strong&gt;structure of the website&lt;/strong&gt;. This is described brilliantly by &lt;a href='http://semphonic.blogs.com/about.html'&gt;Gary Angel&lt;/a&gt; on the &lt;a href='http://semphonic.blogs.com/semangel/2011/01/statistical-analysis-functionalism-and-how-web-analytics-works.html'&gt;Semphonic blog&lt;/a&gt;. This methodology includes identifying those events that are predictive of customer lifetime value&lt;/li&gt;

&lt;li&gt;Building and testing models that &lt;strong&gt;predict customer lifetime value ahead of time&lt;/strong&gt;, so that you can quickly (and robustly) calculate the ROI on marketing campaigns, and adjust your spend accordingly&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
&lt;li&gt;There will always be barriers analysts run up against in trying to fit all of their data into Google&amp;#8217;s schema. For example, it&amp;#8217;s not obvious how Google&amp;#8217;s single &lt;code&gt;client_id&lt;/code&gt; will cope with different packages (CRM, email, CMS et al) each having their own internal set of user IDs&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id='22_creating_live_datadriven_products'&gt;2.2 Creating live, data-driven products&lt;/h3&gt;

&lt;p&gt;There are also important capabilities around using your event data and derived analyses in &lt;strong&gt;live, data-driven products&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Having access to the event stream and your own analyses allows you to make use of that data in data-driven products and systems, including &lt;strong&gt;product / content recommendation&lt;/strong&gt;, &lt;strong&gt;user personalisation engines&lt;/strong&gt; and &lt;strong&gt;internal search algorithms&lt;/strong&gt;&lt;/li&gt;

&lt;li&gt;Because Snowplow is open-source software which can be installed on your own servers, it should be possible to co-locate Snowplow with your own software (CMSes, ecommerce packages, custom apps etc) and thus tightly integrate these data-driven products into your offering&lt;/li&gt;

&lt;li&gt;Because GA doesn&amp;#8217;t provide the granular customer-level and event-level data, GA data cannot be used to prototype or drive these data-driven services&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id='23_data_ownership_and_technical_architecture'&gt;2.3 Data ownership and technical architecture&lt;/h3&gt;

&lt;p&gt;Finally, there are also a number of &lt;strong&gt;data ownership&lt;/strong&gt; and &lt;strong&gt;architectural issues&lt;/strong&gt; which we believe make a Snowplow solution an important compliment, if not yet a full alternative, to a GA implementation. These relate to the fact that, with GA, businesses get more value out by feeding more and more data in: to realise all of the new potential above, they need to be feeding GA with data covering their &lt;em&gt;complete&lt;/em&gt; set of customer interactions. However:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;This is the &lt;strong&gt;most valuable data&lt;/strong&gt; your company owns. Does it make sense to leave the warehousing and storage of that data to a third-party who in many cases is providing the service for free? What guarantees does you have that that data will always be available, 3, 5 or 10 years down the line?&lt;/li&gt;

&lt;li&gt;Does it make sense to feed your detailed event- and customer-level data to Google Analytics, when GA does not share that data back with you at the same atomic level of detail. (GA rolls the data up to &lt;strong&gt;aggregates&lt;/strong&gt; which are less flexible to work with from an analytics perspective)&lt;/li&gt;

&lt;li&gt;What happens when &lt;strong&gt;innacurate data&lt;/strong&gt; is loaded into Google Analytics? Without the ablity to query and diligence the data directly, leave alone clean and reprocess data, there are very limited options available for a business that has innaccurate data in GA. This becomes a bigger issue as implementation become more complex (because data is being ingested across digital and offline platforms), and GA becomes the de facto tool for all customer analytics&lt;/li&gt;

&lt;li&gt;If you need to setup regular ETL processes to load the data from all of your third-party systems into GA, you could &lt;strong&gt;expend the same energy setting up Snowplow instead&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id='closing_thoughts'&gt;Closing thoughts&lt;/h2&gt;

&lt;p&gt;We at Snowplow Analytics are enormously excited by the progress Google are making with their Universal Analytics proposition, and especially the good work Google are doing educating the market into the value of customer-centric analytics. But to unleash the full power of that type of customer, platform and catalogue analytics, the serious analyst will still need access to the customer-level and event-level data: ideally on infrastructure that is totally under your own control. Snowplow is still the best way of getting hold and storing that data.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/10/25/snowplow-0.5.0-released</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/10/25/snowplow-0.5.0-released"/>
    <title>Snowplow 0.5.0 released, now with a Ruby gem to run Snowplow's ETL process on Amazon EMR</title>
    <updated>2012-10-25T00:00:00+01:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We have just released Snowplow &lt;strong&gt;0.5.0&lt;/strong&gt;, with an all-new component, the Snowplow EmrEtlRunner. EmrEtlRunner is a Ruby application to run Snowplow&amp;#8217;s Hive-based ETL (extract, transform, load) process on &lt;a href='http://aws.amazon.com/elasticmapreduce/'&gt;Amazon Elastic MapReduce&lt;/a&gt; with minimum fuss.&lt;/p&gt;

&lt;p&gt;We are hugely grateful to community member &lt;a href='https://github.com/mtibben'&gt;Michael Tibben&lt;/a&gt; from &lt;a href='http://99designs.com'&gt;99designs&lt;/a&gt; for his contributions to EmrEtlRunner: thanks to Michael, EmrEtlRunner is more efficient, more flexible and more robust than it otherwise would have been - and ready sooner. Many thanks Michael!&lt;/p&gt;

&lt;h2 id='using_emretlrunner'&gt;Using EmrEtlRunner&lt;/h2&gt;

&lt;p&gt;EmrEtlRunner is a Ruby application which you can setup on your server to regularly take your raw Snowplow logs (as stored in CloudFront access logs) and apply the Hive-based ETL process to them using &lt;a href='http://aws.amazon.com/elasticmapreduce/'&gt;Amazon Elastic MapReduce&lt;/a&gt;. This ETL process populates a Hive-format events table which you can then use with the HiveQL recipes in our &lt;a href='http://snowplowanalytics.com/analytics/index.html'&gt;Analyst&amp;#8217;s Cookbook&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;For detailed instructions on installing, running and scheduling EmrEtlRunner on your server, please see the &lt;a href='https://github.com/snowplow/snowplow/wiki/Deploying-EmrEtlRunner'&gt;Deploying EmrEtlRunner&lt;/a&gt; guide on the Snowplow Analytics wiki.&lt;/p&gt;
&lt;!--more--&gt;
&lt;h2 id='the_codebase'&gt;The codebase&lt;/h2&gt;

&lt;p&gt;If you want to take a look at the code, you can find it in the main repository here: &lt;a href='https://github.com/snowplow/snowplow/tree/master/3-etl/emr-etl-runner'&gt;&lt;code&gt;3-etl/emr-etl-runner/&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id='getting_help'&gt;Getting help&lt;/h2&gt;

&lt;p&gt;If you have any problems getting EmrEtlRunner working, please &lt;a href='https://github.com/snowplow/snowplow/issues'&gt;raise an issue&lt;/a&gt; or get in touch with us via &lt;a href='https://github.com/snowplow/snowplow/wiki/Talk-to-us'&gt;the usual channels&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id='in_the_pipeline'&gt;In the pipeline&lt;/h2&gt;

&lt;p&gt;At Snowplow we want to support multiple different storage and analytics options for Snowplow events, alongside our current Hive-based approach. Our first priority is supporting &lt;a href='http://www.infobright.org/'&gt;Infobright Community Edition&lt;/a&gt; (ICE) for event storage and querying; extending the current ETL process to load Snowplow events into ICE will be the focus of our next few releases, so please stay tuned!&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/10/24/web-analytics-with-tableau-and-snowplow</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/10/24/web-analytics-with-tableau-and-snowplow"/>
    <title>Performing web analytics on Snowplow data using Tableau - a video demo</title>
    <updated>2012-10-24T00:00:00+01:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;People who see Snowplow for the first time often ask us to &lt;i&gt;&quot;show Snowplow in action&quot;&lt;/i&gt;. It is one thing to tell someone that having access to their customer- and event-level data will open up whole new analysis possibilities, but it is another thing to demonstrate those possibilities.&lt;/p&gt;

&lt;p&gt;Demonstrating Snowplow is tricky because currently, Snowplow only gives you access to data: we have no snazzy front-end UI to show off. The good news is that there are a lot of smart people developing fast, powerful and easy-to-use reporting tools. And because Snowplow gives you access to underlying customer- and event-level data, it is easy to analyse Snowplow data in nearly all of these tools. One such tool is &lt;a href=&quot;http://www.tableausoftware.com/&quot;&gt;Tableau&lt;/a&gt; - we like Tableau as it is fast and intuitive, making it easy for us to perform train-of-thought analyses on Snowplow data. (We will explain more on how to connect Tableau to Snowplow data in a future blog post.)&lt;/p&gt;

&lt;p&gt;In the following series of videos, we start to show how Snowplow lets you use &lt;a href=&quot;http://www.tableausoftware.com/&quot;&gt;Tableau&lt;/a&gt; for exploring your web analytics data. In the first video, we introduce Tableau and talk through the Tableau worksheet created with Snowplow data for an online retailer:&lt;/p&gt;

&lt;video width=&quot;648&quot; height=&quot;563&quot; controls&gt;

	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-1.mp4&quot;  type=&quot;video/mp4&quot; /&gt;
	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-1.webm&quot; type=&quot;video/webm&quot; /&gt;
	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-1.ogv&quot;  type=&quot;video/ogg&quot; /&gt;
	&lt;object width=&quot;648&quot; height=&quot;563&quot; type=&quot;application/x-shockwave-flash&quot; data=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-1.swf&quot;&gt;
		&lt;param name=&quot;movie&quot; value=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-1.swf&quot; /&gt;
		&lt;param name=&quot;flashvars&quot; value=&quot;controlbar=over&amp;amp;image=http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-1-thumb.jpg&amp;amp;file=http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-1.mp4&quot; /&gt;
		&lt;img src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-1-thumb.jpg&quot; width=&quot;648&quot; height=&quot;563&quot; alt=&quot;Web analytics with Tableau and Snowplow introductory video&quot;
		     title=&quot;No video playback capabilities, please download the video below&quot; /&gt;
	&lt;/object&gt; 
&lt;/video&gt;

&lt;p class=&quot;note&quot;&gt;&lt;i&gt;Having trouble viewing the video above? You may download the videos in your format of choice:&lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-1.mp4&quot;&gt;&quot;MP4&quot;&lt;/a&gt;, &lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-1.ogv&quot;&gt;&quot;Ogg&quot;&lt;/a&gt; or &lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-1.webm&quot;&gt;WebM&lt;/a&gt; formats.&lt;/i&gt;&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;In the second video, we show how to perform an analysis of the drivers of growth of traffic on a website. The video serves to highlight how effective Tableau is at performing train-of-thought analysis:&lt;/p&gt;

&lt;video width=&quot;648&quot; height=&quot;563&quot; controls&gt;

	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-2.mp4&quot;  type=&quot;video/mp4&quot; /&gt;
	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-2.webm&quot; type=&quot;video/webm&quot; /&gt;
	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-2.ogv&quot;  type=&quot;video/ogg&quot; /&gt;
	&lt;object width=&quot;648&quot; height=&quot;563&quot; type=&quot;application/x-shockwave-flash&quot; data=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-2.swf&quot;&gt;
		&lt;param name=&quot;movie&quot; value=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-2.swf&quot; /&gt;
		&lt;param name=&quot;flashvars&quot; value=&quot;controlbar=over&amp;amp;image=http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-2-thumb.jpg&amp;amp;file=http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-2.mp4&quot; /&gt;
		&lt;img src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-2-thumb.jpg&quot; width=&quot;648&quot; height=&quot;563&quot; alt=&quot;Web analytics with Tableau and Snowplow introductory video&quot;
		     title=&quot;No video playback capabilities, please download the video below&quot; /&gt;
	&lt;/object&gt; 
&lt;/video&gt;

&lt;p class=&quot;note&quot;&gt;&lt;i&gt;Having trouble viewing the video above? You may download the videos in your format of choice:&lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-2.mp4&quot;&gt;&quot;MP4&quot;&lt;/a&gt;, &lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-2.ogv&quot;&gt;&quot;Ogg&quot;&lt;/a&gt; or &lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-2.webm&quot;&gt;WebM&lt;/a&gt; formats.&lt;/i&gt;&lt;/p&gt;

&lt;p&gt;In the third video, we show how to perform an analysis comparing the relative performance of different products in an online retailer's catalogue. This is an example of &lt;strong&gt;catalogue analytics&lt;/strong&gt;, a very important branch of analytics - where we analyse how different products on a retailer's site perform relative to one another, or how different media items (e.g. articles / videos) on a media site perform. Surprisingly, catalogue analytics is not supported by traditional web analytics packages like Google Analytics:&lt;/p&gt;

&lt;video width=&quot;648&quot; height=&quot;563&quot; controls&gt;

	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-3.mp4&quot;  type=&quot;video/mp4&quot; /&gt;
	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-3.webm&quot; type=&quot;video/webm&quot; /&gt;
	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-3.ogv&quot;  type=&quot;video/ogg&quot; /&gt;
	&lt;object width=&quot;648&quot; height=&quot;563&quot; type=&quot;application/x-shockwave-flash&quot; data=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-3.swf&quot;&gt;
		&lt;param name=&quot;movie&quot; value=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-3.swf&quot; /&gt;
		&lt;param name=&quot;flashvars&quot; value=&quot;controlbar=over&amp;amp;image=http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-3-thumb.jpg&amp;amp;file=http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-3.mp4&quot; /&gt;
		&lt;img src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-3-thumb.jpg&quot; width=&quot;648&quot; height=&quot;563&quot; alt=&quot;Web analytics with Tableau and Snowplow introductory video&quot;
		     title=&quot;No video playback capabilities, please download the video below&quot; /&gt;
	&lt;/object&gt;
&lt;/video&gt;

&lt;p class=&quot;note&quot;&gt;&lt;i&gt;Having trouble viewing the video above? You may download the videos in your format of choice:&lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-3.mp4&quot;&gt;&quot;MP4&quot;&lt;/a&gt;, &lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-3.ogv&quot;&gt;&quot;Ogg&quot;&lt;/a&gt; or &lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-3.webm&quot;&gt;WebM&lt;/a&gt; formats.&lt;/i&gt;&lt;/p&gt;

&lt;p&gt;In the fourth video, we analyse improvements in conversion rates over time for the retailer. This is a core measure to track in order to understand how improvements to the website and marketing strategy drive increased conversion rates. Again, this is something not supported by Google Analytics out of the box. We show how easy it is with Snowplow and Tableau to identify trends in conversion rates over time:&lt;/p&gt;

&lt;video width=&quot;648&quot; height=&quot;563&quot; controls&gt;

	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-4.mp4&quot;  type=&quot;video/mp4&quot; /&gt;
	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-4.webm&quot; type=&quot;video/webm&quot; /&gt;
	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-4.ogv&quot;  type=&quot;video/ogg&quot; /&gt;
	&lt;object width=&quot;648&quot; height=&quot;563&quot; type=&quot;application/x-shockwave-flash&quot; data=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-4.swf&quot;&gt;
		&lt;param name=&quot;movie&quot; value=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-4.swf&quot; /&gt;
		&lt;param name=&quot;flashvars&quot; value=&quot;controlbar=over&amp;amp;image=http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-4-thumb.jpg&amp;amp;file=http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-4.mp4&quot; /&gt;
		&lt;img src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-4-thumb.jpg&quot; width=&quot;648&quot; height=&quot;563&quot; alt=&quot;Web analytics with Tableau and Snowplow introductory video&quot;
		     title=&quot;No video playback capabilities, please download the video below&quot; /&gt;
	&lt;/object&gt;
&lt;/video&gt;

&lt;p class=&quot;note&quot;&gt;&lt;i&gt;Having trouble viewing the video above? You may download the videos in your format of choice:&lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-4.mp4&quot;&gt;&quot;MP4&quot;&lt;/a&gt;, &lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-4.ogv&quot;&gt;&quot;Ogg&quot;&lt;/a&gt; or &lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-4.webm&quot;&gt;WebM&lt;/a&gt; formats.&lt;/i&gt;&lt;/p&gt;

&lt;p&gt;In the fifth video, we show how to visualise patterns of individual user visits over time. This is an interesting starting point to begin to unpick the patterns that make up successful user engagement:&lt;/p&gt;

&lt;video width=&quot;648&quot; height=&quot;563&quot; controls&gt;

	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-5.mp4&quot;  type=&quot;video/mp4&quot; /&gt;
	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-5.webm&quot; type=&quot;video/webm&quot; /&gt;
	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-5.ogv&quot;  type=&quot;video/ogg&quot; /&gt;
	&lt;object width=&quot;648&quot; height=&quot;563&quot; type=&quot;application/x-shockwave-flash&quot; data=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-5.swf&quot;&gt;
		&lt;param name=&quot;movie&quot; value=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-5.swf&quot; /&gt;
		&lt;param name=&quot;flashvars&quot; value=&quot;controlbar=over&amp;amp;image=http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-5-thumb.jpg&amp;amp;file=http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-5.mp4&quot; /&gt;
		&lt;img src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-5-thumb.jpg&quot; width=&quot;648&quot; height=&quot;563&quot; alt=&quot;Web analytics with Tableau and Snowplow introductory video&quot;
		     title=&quot;No video playback capabilities, please download the video below&quot; /&gt;
	&lt;/object&gt;
&lt;/video&gt;

&lt;p class=&quot;note&quot;&gt;&lt;i&gt;Having trouble viewing the video above? You may download the videos in your format of choice:&lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-5.mp4&quot;&gt;&quot;MP4&quot;&lt;/a&gt;, &lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-5.ogv&quot;&gt;&quot;Ogg&quot;&lt;/a&gt; or &lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-5.webm&quot;&gt;WebM&lt;/a&gt; formats.&lt;/i&gt;&lt;/p&gt;

&lt;p&gt;In the sixth video, we show how to visualise the range of product pages visited by each user. This can help us to understand how successful the retailer is at driving users interested in one product to consider buying other products (cross-selling), and onwards to developing recommendation algorithms (users who liked &lt;i&gt;this&lt;/i&gt; product also liked &lt;i&gt;this&lt;/i&gt; product):&lt;/p&gt;

&lt;video width=&quot;648&quot; height=&quot;563&quot; controls&gt;

	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-6.mp4&quot;  type=&quot;video/mp4&quot; /&gt;
	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-6.webm&quot; type=&quot;video/webm&quot; /&gt;
	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-6.ogv&quot;  type=&quot;video/ogg&quot; /&gt;
	&lt;object width=&quot;648&quot; height=&quot;563&quot; type=&quot;application/x-shockwave-flash&quot; data=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-6.swf&quot;&gt;
		&lt;param name=&quot;movie&quot; value=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-6.swf&quot; /&gt;
		&lt;param name=&quot;flashvars&quot; value=&quot;controlbar=over&amp;amp;image=http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-6-thumb.jpg&amp;amp;file=http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-6.mp4&quot; /&gt;
		&lt;img src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-6-thumb.jpg&quot; width=&quot;648&quot; height=&quot;563&quot; alt=&quot;Web analytics with Tableau and Snowplow introductory video&quot;
		     title=&quot;No video playback capabilities, please download the video below&quot; /&gt;
	&lt;/object&gt;
&lt;/video&gt;

&lt;p class=&quot;note&quot;&gt;&lt;i&gt;Having trouble viewing the video above? You may download the videos in your format of choice:&lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-6.mp4&quot;&gt;&quot;MP4&quot;&lt;/a&gt;, &lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-6.ogv&quot;&gt;&quot;Ogg&quot;&lt;/a&gt; or &lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-6.webm&quot;&gt;WebM&lt;/a&gt; formats.&lt;/i&gt;&lt;/p&gt;

&lt;p&gt;In the final video in the series, we perform an example cohort analysis, with a view to understanding how 'sticky' the online retailer site is, and how its stickiness has improved over time. In this example, we use &lt;i&gt;stickiness&lt;/i&gt; to refer to how good the website is at driving repeat visits:&lt;/p&gt;

&lt;video width=&quot;648&quot; height=&quot;563&quot; controls&gt;

	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-7.mp4&quot;  type=&quot;video/mp4&quot; /&gt;
	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-7.webm&quot; type=&quot;video/webm&quot; /&gt;
	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-7.ogv&quot;  type=&quot;video/ogg&quot; /&gt;
	&lt;object width=&quot;648&quot; height=&quot;563&quot; type=&quot;application/x-shockwave-flash&quot; data=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-7.swf&quot;&gt;
		&lt;param name=&quot;movie&quot; value=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-7.swf&quot; /&gt;
		&lt;param name=&quot;flashvars&quot; value=&quot;controlbar=over&amp;amp;image=http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-7-thumb.jpg&amp;amp;file=http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-7.mp4&quot; /&gt;
		&lt;img src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-7-thumb.jpg&quot; width=&quot;648&quot; height=&quot;563&quot; alt=&quot;Web analytics with Tableau and Snowplow introductory video&quot;
		     title=&quot;No video playback capabilities, please download the video below&quot; /&gt;
	&lt;/object&gt;
&lt;/video&gt;

&lt;p class=&quot;note&quot;&gt;&lt;i&gt;Having trouble viewing the video above? You may download the videos in your format of choice:&lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-7.mp4&quot;&gt;&quot;MP4&quot;&lt;/a&gt;, &lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-7.ogv&quot;&gt;&quot;Ogg&quot;&lt;/a&gt; or &lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-7.webm&quot;&gt;WebM&lt;/a&gt; formats.&lt;/i&gt;&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/10/21/infobright-ruby-loader-released</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/10/21/infobright-ruby-loader-released"/>
    <title>Infobright Ruby Loader Released</title>
    <updated>2012-10-21T00:00:00+01:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We&amp;#8217;re pleased to start the week with the release of a new Ruby gem, our &lt;a href='https://github.com/snowplow/infobright-ruby-loader'&gt;Infobright Ruby Loader&lt;/a&gt; (IRL).&lt;/p&gt;

&lt;p&gt;At Snowplow we&amp;#8217;re committed to supporting multiple different storage and analytics options for Snowplow events, alongside our current Hive-based approach. One of the alternative data stores we are working with is &lt;a href='http://www.infobright.org/'&gt;Infobright&lt;/a&gt;, a columnar database which is available in open source and commercial versions.&lt;/p&gt;

&lt;p&gt;For all but the largest Snowplow users, columnar databases such as Infobright should be an attractive alternative to doing all of your analysis in Hive. The main advantages of columnar databases are as follows:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Scale to terabytes (although not petabytes, unlike Hive)&lt;/li&gt;

&lt;li&gt;Fixed cost (dedicated RAM-heavy analytics server), versus pay-as-you-go querying on Amazon EMR&lt;/li&gt;

&lt;li&gt;Significantly faster query times – typically seconds, not minutes&lt;/li&gt;

&lt;li&gt;Plug in to many analytics front ends e.g. Tableau, Qlikview, R&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;So, open source columnar databases like Infobright Community Edition (ICE) are a good fit for Snowplow analytics. Unfortunately, when we started to load Snowplow event logs into ICE, we realised that there wasn&amp;#8217;t a good data-loading solution for Infobright in Ruby, our ETL language of choice. So, we built one :-)&lt;/p&gt;

&lt;p&gt;Our freshly minted &lt;a href='https://github.com/snowplow/infobright-ruby-loader'&gt;Infobright Ruby Loader&lt;/a&gt; (IRL) can be used in two different ways:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;As a command-line tool&lt;/strong&gt; - for manual loading of data into Infobright at the command-line. No Ruby expertise required&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;As part of another application&lt;/strong&gt; - because it&amp;#8217;s a Ruby gem with a Ruby API, IRL can be integrated into larger Ruby ETL processes&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We will be using IRL at Snowplow as part of our larger ETL process to load Snowplow events into ICE for analysis - we hope to roll this out within the next few weeks.&lt;/p&gt;

&lt;p&gt;In the meantime, we hope that IRL is useful to people in the Infobright community who need to run data loads at the command-line; IRL was inspired by &lt;a href='http://www.infobright.org/Blog/Entry/unscripted/'&gt;ParaFlex&lt;/a&gt;, an excellent Bash script from the Infobright team to perform parallel loading of Infobright, and can be used as a direct alternative to ParaFlex.&lt;/p&gt;

&lt;p&gt;To find out more about our Infobright Ruby Loader, please check out the detailed &lt;a href='https://github.com/snowplow/infobright-ruby-loader/blob/master/README.md'&gt;README&lt;/a&gt; in the GitHub repository. And please direct any questions through the &lt;a href='https://github.com/snowplow/snowplow/wiki/Talk-to-us'&gt;usual channels&lt;/a&gt;!&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/10/12/how-the-role-of-hive-is-changing-at-snowplow</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/10/12/how-the-role-of-hive-is-changing-at-snowplow"/>
    <title>How we use Hive at Snowplow, and how the role of Hive is changing. (Slides from our presentation to Hive London.)</title>
    <updated>2012-10-12T00:00:00+01:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;Last night I gave a presentation to the clever folks at Hive London covering three things:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;How big data technologies like Apache Hive are transforming web analytics&lt;/li&gt;

&lt;li&gt;Howe we&amp;#8217;ve used Hive in Snowplow development&lt;/li&gt;

&lt;li&gt;How the role of Hive has changed at Snowplow over time, including a comparison of Hive against other technologies.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The slides from the presentation are below. As always, any questions / comments, please post them below.&lt;/p&gt;
&lt;iframe frameborder='0' height='356' marginheight='0' marginwidth='0' scrolling='no' src='http://www.slideshare.net/slideshow/embed_code/14696456' style='border:1px solid #CCC;border-width:1px 1px 0;margin-bottom:5px' width='427'&gt;  &lt;/iframe&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/10/11/snowplow-0.4.10-released</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/10/11/snowplow-0.4.10-released"/>
    <title>Snowplow 0.4.10 released</title>
    <updated>2012-10-11T00:00:00+01:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We have just released version &lt;strong&gt;0.4.10&lt;/strong&gt; of Snowplow - people using 0.4.8 can jump straight to this version. This version updates:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;snowplow.js to version 0.7.0&lt;/li&gt;

&lt;li&gt;the Hive deserializer to version 0.4.9&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Big thanks to community members &lt;a href='https://github.com/mtibben'&gt;Michael Tibben&lt;/a&gt; from &lt;a href='http://99designs.com'&gt;99designs&lt;/a&gt; and &lt;a href='https://github.com/ramn'&gt;Simon Andersson&lt;/a&gt; from &lt;a href='http://www.qwaya.com'&gt;Qwaya&lt;/a&gt; for their most-helpful contributions to this release!&lt;/p&gt;

&lt;h2 id='main_changes'&gt;Main changes&lt;/h2&gt;

&lt;p&gt;The main changes are as follows:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The querystring parameter for site ID which the JavaScript tracker sends to your collector is renamed from &lt;code&gt;said&lt;/code&gt; to &lt;code&gt;aid&lt;/code&gt;&lt;/li&gt;

&lt;li&gt;The Hive-based ETL process now extracts the ecommerce tracking fields and the site ID field and adds them into your processed events table&lt;/li&gt;

&lt;li&gt;We fixed a bug in the Hive deserializer where a partially-processed row was returned even if a fatal error was found in the row (now, a null row is returned instead)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The rest of the changes were all enhancements to the Hive deserializer&amp;#8217;s Specs2 test suite - these improvements should help to accelerate work on the deserializer (we have lots of cool new stuff we want to add to the deserializer!). &lt;!--more--&gt;&lt;/p&gt;

&lt;h2 id='new_event_table_fields'&gt;New event table fields&lt;/h2&gt;

&lt;p&gt;The new fields in the event table all relate directly to additional tracking functionality which was added to the JavaScript tracker in &lt;a href='/blog/2012/09/06/snowplow-0.4.7-released/'&gt;Snowplow 0.4.7&lt;/a&gt;. Specifically:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The &lt;code&gt;setSiteId()&lt;/code&gt; functionality is now extracted to the &lt;code&gt;app_id&lt;/code&gt; field (short for application ID)&lt;/li&gt;

&lt;li&gt;The ecommerce tracking functionality is now extracted to a set of &lt;code&gt;tr_&lt;/code&gt; and &lt;code&gt;ti_&lt;/code&gt; fields&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For details on the new fields, please review our latest &lt;a href='/analytics/snowplow-table-structure.html'&gt;Hive events table definition&lt;/a&gt; - there is now a column indicating in which version a given field was added.&lt;/p&gt;

&lt;h2 id='how_to_get_the_new_version'&gt;How to get the new version&lt;/h2&gt;

&lt;p&gt;As usual, the new version of the Hive deserializer is available from the GitHub repository&amp;#8217;s &lt;a href='https://github.com/snowplow/snowplow/downloads'&gt;Downloads&lt;/a&gt; section as &lt;strong&gt;snowplow-log-deserializers-0.4.9.jar&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The updated snowplow.js is &lt;a href='https://raw.github.com/snowplow/snowplow/master/1-trackers/javascript-tracker/js/snowplow.js'&gt;available in our GitHub repository&lt;/a&gt; for you to minify and upload, or alternatively you can use the one on our CDN:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;https://d1fc8wv8zag5ca.cloudfront.net/0.7.0/sp.js&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you have any problems with either of these components, please &lt;a href='https://github.com/snowplow/snowplow/issues'&gt;raise an issue&lt;/a&gt;!&lt;/p&gt;

&lt;h2 id='a_note_on_backwards_compatibility_for_the_events_table'&gt;A note on backwards compatibility for the events table&lt;/h2&gt;

&lt;p&gt;We will continue to add extra fields to the Snowplow events table as we add extra capabilities to the ETL process - for example, we are working on functionality to extract geo-location information from IP addresses via MaxMind.&lt;/p&gt;

&lt;p&gt;Starting with our new &lt;code&gt;app_id&lt;/code&gt; field, we will be adding all such new fields to the &lt;strong&gt;end&lt;/strong&gt; of our Hive events table definition. This will mean that you will &lt;strong&gt;not&lt;/strong&gt; have to re-run the ETL process across all your historic raw logs, provided you do &lt;strong&gt;not&lt;/strong&gt; need the data found in the new fields. This is because a Hive query across both the old event table format and the new table format works as long as you don&amp;#8217;t explicitly query a new field.&lt;/p&gt;

&lt;p&gt;In other words, Hive is futureproofed against new fields being added to the end of your underlying data files, and we&amp;#8217;ll take advantage of this to improve backwards compatibility for our events table!&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/10/11/attlib-0.0.1-released</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/10/11/attlib-0.0.1-released"/>
    <title>Attlib - an open source library for extracting search marketing attribution data from referrer URLs</title>
    <updated>2012-10-11T00:00:00+01:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;&lt;strong&gt;Update 17-Dec-12&lt;/strong&gt;: We have renamed Attlib to &lt;a href='https://github.com/snowplow/referer-parser'&gt;referer-parser&lt;/a&gt;, to make it clearer what Attlib does: parse referer URLs. The repository has been updated accordingly. Some of the example code below is out-of-date now: we recommend checking out the &lt;a href='https://github.com/snowplow/referer-parser'&gt;repository&lt;/a&gt; for more information.&lt;/p&gt;

&lt;p&gt;Last night we published &lt;a href='https://github.com/snowplow/referer-parser'&gt;Attlib&lt;/a&gt;, an open source Ruby library for extracting search marketing attribution data from referer (sic) URLs. In this post we talk through:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href='#what_attlib_does'&gt;What Attlib does, and how to use it&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='#install'&gt;Installing Attlib&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='#search_engine_yaml'&gt;The search_engine.yml file&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='#snowplow_stack'&gt;Attlib as part of the Snowplow stack&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='#other_languages'&gt;Attlib in other languages&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='#snowplow_components_as_standalone_projects'&gt;Making components of Snowplow available as standalone open source projects&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;a name='what_attlib_does' /&gt;
&lt;h3 id='what_attlib_does_and_how_to_use_it'&gt;What Attlib does, and how to use it&lt;/h3&gt;

&lt;p&gt;Attlib is straightforward Ruby library for extracting seach marketing attribution data from referrer URLs. You give it a referer URL to parse: it then lets you now whether the URL is from a search engine. If it is, it will tell you which search engine it is, and what keywords were typed. (If those keywords are included in the query string - this is no longer the case for users logged in to Google, as documented &lt;a href='http://googlewebmastercentral.blogspot.co.uk/2011/10/accessing-search-query-data-for-your.html'&gt;here&lt;/a&gt;.)&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='ruby'&gt;&lt;span class='nb'&gt;require&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;attlib&amp;#39;&lt;/span&gt;

&lt;span class='n'&gt;r&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='no'&gt;Referrer&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;new&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;http://images.google.ca/imgres?q=hermetic+tarot&amp;amp;hl=en&amp;amp;biw=1189&amp;amp;bih=521&amp;amp;tbm=isch&amp;amp;tbnid=BuQ_IyUbc25usM:&amp;amp;imgrefurl=http://www.psychicbazaar.com/tarot-cards/15-the-hermetic-tarot.html&amp;amp;imgurl=http://mdm.pbzstatic.com/tarot/the-hermetic-tarot/card-4.png&amp;amp;w=1064&amp;amp;h=1551&amp;amp;ei=ue9AUMe7Osn9iwLZ-4H4Dw&amp;amp;zoom=1&amp;amp;iact=hc&amp;amp;vpx=107&amp;amp;vpy=48&amp;amp;dur=2477&amp;amp;hovh=271&amp;amp;hovw=186&amp;amp;tx=133&amp;amp;ty=157&amp;amp;sig=115588264602219115047&amp;amp;page=4&amp;amp;tbnh=162&amp;amp;tbnw=120&amp;amp;start=57&amp;amp;ndsp=19&amp;amp;ved=1t:429,r:12,s:57,i:291&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt;

&lt;span class='n'&gt;r&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;is_search_engine?&lt;/span&gt; &lt;span class='c1'&gt;# True&lt;/span&gt;
&lt;span class='n'&gt;r&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;search_engine&lt;/span&gt; &lt;span class='c1'&gt;# &amp;#39;Google Images&amp;#39;&lt;/span&gt;
&lt;span class='n'&gt;r&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;keywords&lt;/span&gt; 	&lt;span class='c1'&gt;# &amp;#39;hermetic tarot&amp;#39;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;a name='install' /&gt;
&lt;h3 id='installing_attlib'&gt;Installing Attlib&lt;/h3&gt;

&lt;p&gt;Attlib is available via a Ruby Gem. To install, simply run the following at the command line:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo gem install attlib&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The sourcecode is available on &lt;a href='https://github.com/snowplow/referer-parser'&gt;Github&lt;/a&gt;&lt;/p&gt;
&lt;a name='search_engine_yaml' /&gt;
&lt;h3 id='the_search_enginesyml_file'&gt;The search_engines.yml file&lt;/h3&gt;

&lt;p&gt;Extracting search engine names and keywords from a referer URL is pretty straightforward. What is more complicated is keeping track of the myriad search engines that are out there, operating in different countries, the myriad domains they operate on, and the different query parameters that each of them uses to store the keywords.&lt;/p&gt;

&lt;p&gt;Because the space is constantly evolving, none of this information (about search engines, parameters and domains) has been hard coded into Attlib. All of it is available in the &lt;a href='https://github.com/snowplow/referer-parser/blob/master/search.yml'&gt;search_engines.yml&lt;/a&gt; file, in the &lt;a href='https://github.com/snowplow/attlib/tree/master'&gt;data&lt;/a&gt; in the repo. &lt;!--more--&gt;&lt;/p&gt;

&lt;p&gt;The structure of the YAML file should be straightforward to understand. Each search engine is a top level item. For each search engine, two lists are given: one is a list of parameters used in that search engine&amp;#8217;s query string to identify the keywords entered. The other is the list of domains on which that search engine operates. An extract is shown below:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='yaml'&gt;&lt;span class='l-Scalar-Plain'&gt;Babylon&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt;
  &lt;span class='l-Scalar-Plain'&gt;parameters&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; 
    &lt;span class='p-Indicator'&gt;-&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;q&lt;/span&gt;
  &lt;span class='l-Scalar-Plain'&gt;domains&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; 
   &lt;span class='p-Indicator'&gt;-&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;search.babylon.com&lt;/span&gt;
   &lt;span class='p-Indicator'&gt;-&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;searchassist.babylon.com&lt;/span&gt;

&lt;span class='l-Scalar-Plain'&gt;Baidu&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt;
  &lt;span class='l-Scalar-Plain'&gt;parameters&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; 
    &lt;span class='p-Indicator'&gt;-&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;wd&lt;/span&gt;
    &lt;span class='p-Indicator'&gt;-&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;word&lt;/span&gt;
    &lt;span class='p-Indicator'&gt;-&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;kw&lt;/span&gt;
    &lt;span class='p-Indicator'&gt;-&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;k&lt;/span&gt;
  &lt;span class='l-Scalar-Plain'&gt;domains&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt;
    &lt;span class='p-Indicator'&gt;-&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;www.baidu.com&lt;/span&gt;
    &lt;span class='p-Indicator'&gt;-&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;www1.baidu.com&lt;/span&gt;
    &lt;span class='p-Indicator'&gt;-&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;zhidao.baidu.com&lt;/span&gt;
    &lt;span class='p-Indicator'&gt;-&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;tieba.baidu.com&lt;/span&gt;
    &lt;span class='p-Indicator'&gt;-&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;news.baidu.com&lt;/span&gt;
    &lt;span class='p-Indicator'&gt;-&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;web.gougou.com&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Keeping this file up to date is a big job: one of our hopes releasing Attlib as an open source, standalone library, is that the community contributes to the file. We are enormously grateful to our friends at &lt;a href='http://piwik.org/'&gt;Piwik&lt;/a&gt; as our initial version of the file is based on the Piwik equivalent &lt;a href='https://github.com/piwik/piwik/blob/master/core/DataFiles/SearchEngines.php'&gt;SearchEngines.php&lt;/a&gt;, for the hard work they put into this version.&lt;/p&gt;
&lt;a name='snowplow_stack' /&gt;
&lt;h3 id='attlib_as_part_of_the_snowplow_stack'&gt;Attlib as part of the Snowplow stack&lt;/h3&gt;

&lt;p&gt;Our intention is to port &lt;a href='https://github.com/snowplow/referer-parser'&gt;Attlib&lt;/a&gt; into Scala and integrate it into the Snowplow stack: specifically the ETL phase. Both Ruby and Scala versions of Attlib will run based on the same &lt;a href='https://github.com/snowplow/referer-parser/blob/master/search.yml'&gt;search_engines.yml&lt;/a&gt; file.&lt;/p&gt;
&lt;a name='other_languages' /&gt;
&lt;h3 id='attlib_in_other_languages'&gt;Attlib in other languages&lt;/h3&gt;

&lt;p&gt;As well as contributing to the search &lt;a href='https://github.com/snowplow/referer-parser/blob/master/search.yml'&gt;search_engines.yml&lt;/a&gt; file, we also hope that community members will develop versions of Attlib in other languages e.g. Python.&lt;/p&gt;
&lt;a name='snowplow_components_as_standalone_projects' /&gt;
&lt;h3 id='making_components_of_snowplow_available_as_standalone_open_source_projects'&gt;Making components of Snowplow available as standalone open source projects&lt;/h3&gt;

&lt;p&gt;Attlib is the first component in the Snowplow stack that we have released as a standalone library. There are many more in the pipeline. (More on this in future blog posts :-) ). For us, this is a key part of the Snowplow strategy:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Keeping the Snowplow architecture as loosely coupled as possible. We believe this makes Snowplow robust, scalable and extendable&lt;/li&gt;

&lt;li&gt;Grow the userbase of people using and contributing to each component. Processing web analytics data is a big job: there are many individual components involved, and each of them needs to evolve with the changing marketplace. Attlib is concerned today with extracting useful data from search engine referrers: but it is likely that as time goes on, we&amp;#8217;ll want to extend it to capture data from other types of referrers e.g. social networks or affiliate sites. The bigger the community of people on top of those developments, the better for everyone in the web analytics community. Releasing each component as a standalone open source library should help grow that community.&lt;/li&gt;
&lt;/ol&gt;
&lt;hr /&gt;
&lt;p&gt;Any questions about Attlib, or anything else in this post? Then &lt;a href='/about/index.html'&gt;get in touch&lt;/a&gt; with the Snowplow team.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/09/24/what-does-snowplow-let-you-do</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/09/24/what-does-snowplow-let-you-do"/>
    <title>Why set your data free?</title>
    <updated>2012-09-24T00:00:00+01:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;At Saturday&amp;#8217;s &lt;a href='http://ukdaa.co.uk/'&gt;Measure Camp&lt;/a&gt;, I had the chance to introduce Snowplow to a large number of some incredibly thoughtful and insightful people in the web analytics industry.&lt;/p&gt;

&lt;p&gt;With each person, I started by explaining that Snowplow gave them direct access to their customer-level and event-level data. The response I got in nearly all cases was: &lt;strong&gt;what does having direct access to my web analytics data enable me to do, that I can&amp;#8217;t do with Google Analytics / Omniture?&lt;/strong&gt; It&amp;#8217;s such a good question I thought I should publish an answer below:&lt;/p&gt;

&lt;h3 id='1_integrate_web_analytics_data_with_other_data_sources'&gt;1. Integrate web analytics data with other data sources&lt;/h3&gt;

&lt;p&gt;Integrating your web analytics data with other data sets enables you to answer a wide range of valuable business questions:&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;&lt;strong&gt;Data source&lt;/strong&gt;&lt;/th&gt;&lt;th&gt;&lt;strong&gt;Example business questions&lt;/strong&gt;&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Marketing spend data e.g. AdWords, ad server data&lt;/td&gt;&lt;td style='text-align: left;'&gt;What is the return on my ad spend? How should I optimize my return on ad spend&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Customer data e.g. CRM, loyalty&lt;/td&gt;&lt;td style='text-align: left;'&gt;How does the online behaviour of my differnet customer segments vary by segment? Do online promotions drive offline sales? (Or vice versa?)&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Product / media catalogue data&lt;/td&gt;&lt;td style='text-align: left;'&gt;What are my most profitable product lines? Do different types of products attract different customer segments? What are the products that drive the most visits?&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;Snowplow makes integrating web analytics data with other data sources easier in a two ways:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;All your Snowplow data is directly accessible in Apache Hive or Infobright. (So no expensive export process is required, prior to linking the data sets.)&lt;/li&gt;

&lt;li&gt;Custom variables and event tracking give you plenty of opportunity to join e.g. customer IDs or campaigns names to enable &lt;code&gt;JOIN&lt;/code&gt;s across data set&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For more details on how to perform &lt;code&gt;JOIN&lt;/code&gt;s between Snowplow data and other sources, see refer to the guide to &lt;a href='/analytics/customer-analytics/joining-customer-data.html'&gt;joining Snowplow engagement data with other sources of customer data&lt;/a&gt;&lt;/p&gt;

&lt;h3 id='2_slice_and_dice_your_data_by_any_combination_of_dimensions__metrics_you_want'&gt;2. Slice and dice your data by any combination of dimensions / metrics you want&lt;/h3&gt;

&lt;p&gt;Google Analytics in particular only lets users create reports about of set combinations of dimensions and metrics. Examples of combinations that are &lt;strong&gt;not supported&lt;/strong&gt; include:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Number of unique visitors by product page&lt;/li&gt;

&lt;li&gt;Different sources of traffic by product page (and how this changes over time)&lt;/li&gt;

&lt;li&gt;Engagement levels (e.g. number of visits, number of page views, conversion rates) by traffic source&lt;/li&gt;

&lt;li&gt;Improvements to conversion rates over time&lt;/li&gt;
&lt;/ol&gt;
&lt;!--more--&gt;
&lt;p&gt;In contrast, because Snowplow gives you access to the underlying data, it is possible to use BI tools like &lt;a href='http://www.tableausoftware.com/'&gt;Tableau&lt;/a&gt; and &lt;a href='http://www.microsoft.com/en-us/bi/powerpivot.aspx'&gt;PowerPivot&lt;/a&gt; to quickly slice and dice web analytics data by any dimensions / metrics you want. We&amp;#8217;ll be posting examples of how to do this in the next few days.&lt;/p&gt;

&lt;h3 id='3_use_machine_learning_tools_on_your_web_analytics_data'&gt;3. Use machine learning tools on your web analytics data&lt;/h3&gt;

&lt;p&gt;Machine learning tools, and &lt;a href='http://mahout.apache.org/'&gt;Mahout&lt;/a&gt; in particular, have created some new and exciting opportunities to:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Develop product and content recommendation engines, based on user web behaviour. (E.g. users who viewed these content items, also viewed&amp;#8230;)&lt;/li&gt;

&lt;li&gt;Segment your audience by online behaviour&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Snowplow makes it easy to extract the core input data you would need to feed a machine learning algorithm in a single query. (E.g. a matrix mapping users to products by page views / add to baskets / purchases etc.) We will be exploring ways to integrate Snowplow with &lt;a href='http://mahout.apache.org/'&gt;Mahout&lt;/a&gt; in a future blog post.&lt;/p&gt;

&lt;h3 id='4_view_data_for_individual_users_over_their_entire_lives'&gt;4. View data for individual users over their entire lives&lt;/h3&gt;

&lt;p&gt;Whereas reports on Google Analytics tend to be about visits, page views or transactions, Snowplow lets you slice data by users over multiple visits, opening up a wide range of possibilities:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Develop accurate models of customer lifetime value&lt;/li&gt;

&lt;li&gt;Develop more rigorous approaches to attribution modelling, by capturing in granular detail which channels touched a user at different points in their lifecycle&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id='5_interested_in_any__all_of_the_above'&gt;5. Interested in any / all of the above?&lt;/h3&gt;

&lt;p&gt;Then &lt;a href='/product/get-started.html'&gt;get started&lt;/a&gt; with Snowplow, or &lt;a href='/about/index.html'&gt;get in touch&lt;/a&gt; to find out more!&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/09/14/snowplow-0.4.8-released</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/09/14/snowplow-0.4.8-released"/>
    <title>Snowplow 0.4.8 released</title>
    <updated>2012-09-14T00:00:00+01:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We have just released Snowplow version &lt;strong&gt;0.4.8&lt;/strong&gt;, with a set of enhancements to the existing Hive deserializer:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The Hive deserializer now supports Amazon&amp;#8217;s new CloudFront log file format (launched 12 September 2012) as well as the older format&lt;/li&gt;

&lt;li&gt;The Hive deserializer now supports a tracking pixel called simply &lt;code&gt;i&lt;/code&gt; (saving some characters versus &lt;code&gt;ice.png&lt;/code&gt;) (&lt;a href='https://github.com/snowplow/snowplow/issues/35'&gt;issue #35&lt;/a&gt;)&lt;/li&gt;

&lt;li&gt;The Hive deserializer now works if the CloudFront distribution has Forward Query String = yes (&lt;a href='https://github.com/snowplow/snowplow/pull/39'&gt;issue #39&lt;/a&gt;)&lt;/li&gt;

&lt;li&gt;The Hive deserializer no longer dies if the calling page&amp;#8217;s querystring is malformed&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Many thanks to community member &lt;a href='https://github.com/mtibben'&gt;Michael Tibben&lt;/a&gt; from &lt;a href='http://99designs.com'&gt;99designs&lt;/a&gt; in Melbourne for contributing the Forward Query String = yes fix!&lt;/p&gt;

&lt;h2 id='new_cloudfront_log_file_format'&gt;New CloudFront log file format&lt;/h2&gt;

&lt;p&gt;On 12th September 2012, Amazon &lt;a href='http://aws.amazon.com/about-aws/whats-new/2012/09/04/cloudfront-support-for-cookies-and-price-classes/'&gt;rolled out a new CloudFront log file format&lt;/a&gt;, adding three additional fields onto the end of each line:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;cs(Cookie)&lt;/strong&gt;, the cookie header in the request (if any). Logging of this field is optional.&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;x-edge-result-type&lt;/strong&gt;, the result type of each HTTP(s) request (for example, cache hit/miss/error).&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;x-edge-request-id&lt;/strong&gt;, an encrypted string that uniquely identifies a request to help AWS troubleshoot/debug any issues.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As always, please consult the Amazon CloudFront &lt;a href='http://docs.amazonwebservices.com/AmazonCloudFront/latest/DeveloperGuide/AccessLogs.html#LogFileFormat'&gt;Developer Guide&lt;/a&gt; for more information on these fields. &lt;!--more--&gt;&lt;/p&gt;

&lt;p&gt;As part of this new &lt;strong&gt;0.4.8&lt;/strong&gt; Snowplow release, the Hive deserializer now supports the new CloudFront format as well as the old format: if you deploy the latest version of the deserializer, you should be able to process both old-format and new-format CloudFront logs without issue.&lt;/p&gt;

&lt;h2 id='support_for__as_the_tracking_pixel'&gt;Support for &lt;code&gt;i&lt;/code&gt; as the tracking pixel&lt;/h2&gt;

&lt;p&gt;Currently the Snowplow JavaScript tracker fires a GET request to a tracking pixel called &lt;code&gt;ice.png&lt;/code&gt;. This works fine, but it makes more sense to call the pixel &lt;code&gt;i&lt;/code&gt;, for two reasons:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;We free up 5 extra characters to use for sending data&lt;/li&gt;

&lt;li&gt;A transparent GIF is smaller to send than a transparent PNG&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Thanks to &lt;a href='https://github.com/shermozle/'&gt;Simon Rumble&lt;/a&gt; (author of &lt;a href='https://github.com/shermozle/SnowCannon'&gt;SnowCannon&lt;/a&gt;) for pointing this out! In due course we will update the JavaScript tracker and CloudFront collector to implement this change (see issues &lt;a href='https://github.com/snowplow/snowplow/issues/29'&gt;#29&lt;/a&gt; and &lt;a href='https://github.com/snowplow/snowplow/issues/25'&gt;#25&lt;/a&gt;), but to start off we have added support for &lt;code&gt;i&lt;/code&gt; to the new version of the Hive deserializer.&lt;/p&gt;

&lt;p&gt;This is a small change, but highlights a wider point for Snowplow development: in general, whenever we have a &amp;#8220;breaking change&amp;#8221; coming upstream, we will try to prepare for this change downstream first, to prevent any disruption to your use of Snowplow.&lt;/p&gt;

&lt;h2 id='support_for_forward_query_string__yes'&gt;Support for Forward Query String = yes&lt;/h2&gt;

&lt;p&gt;Thanks to &lt;a href='https://github.com/mtibben'&gt;Michael Tibben&lt;/a&gt; from &lt;a href='http://99designs.com'&gt;99designs&lt;/a&gt; for spotting that the Hive deserializer does not work if your CloudFront distribution has Forward Query String set to Yes; Michael not only raised the issue but also provided a fix, many thanks Michael!&lt;/p&gt;

&lt;p&gt;Most Snowplow users will have Forward Query String in their CloudFront distribution set to No, so this issue will not arise for them; however this fix will be invaluable for anyone who does have it set to Yes. If you want to read more about this, please check out &lt;a href='https://github.com/snowplow/snowplow/pull/39'&gt;issue #39&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We&amp;#8217;re aware that our guide for setting up the CloudFront distribution is a bit out of date (which is how this issue can arise) - we will be refreshing the tracking pixel guide soon (&lt;a href='https://github.com/snowplow/snowplow/issues/25'&gt;issue #25&lt;/a&gt;)! Many thanks for your patience.&lt;/p&gt;

&lt;h2 id='more_robust_querystring_handling'&gt;More robust querystring handling&lt;/h2&gt;

&lt;p&gt;A small change - we have made the code for extracting marketing attribution more robust. Specifically, the Hive deserializer no longer dies (i.e. throws a non-recoverable &lt;code&gt;SerDeException&lt;/code&gt;) if the calling page&amp;#8217;s URL has a malformed querystring.&lt;/p&gt;

&lt;p&gt;An example of a malformed querystring would be something like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http://www.psychicbazaar.com/2-tarot-cards?n=48?utmsource=GoogleSearch&amp;amp;...&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note the two &lt;code&gt;?&lt;/code&gt; questionmarks (the second one should be an &lt;code&gt;&amp;amp;&lt;/code&gt; ampersand). In the case of a malformed querystring like this, the five marketing attribution fields in the Hive output format for this row will all be set to null.&lt;/p&gt;

&lt;h2 id='deploying_the_new_version'&gt;Deploying the new version&lt;/h2&gt;

&lt;p&gt;The new version of the Hive deserializer is available from the GitHub repository&amp;#8217;s &lt;a href='https://github.com/snowplow/snowplow/downloads'&gt;Downloads&lt;/a&gt; section as &lt;strong&gt;snowplow-log-deserializers-0.4.8.jar&lt;/strong&gt;. If you have any problems running it, please &lt;a href='https://github.com/snowplow/snowplow/issues'&gt;raise an issue&lt;/a&gt;!&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/09/06/snowplow-0.4.7-released</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/09/06/snowplow-0.4.7-released"/>
    <title>Snowplow 0.4.7 released</title>
    <updated>2012-09-06T00:00:00+01:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We have just released Snowplow version &lt;strong&gt;0.4.7&lt;/strong&gt;. This release bumps the Snowplow JavaScript tracker to version &lt;strong&gt;0.6&lt;/strong&gt;, with two significant new features:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The ability to set a site ID for your tracking - useful for multi-site publishers&lt;/li&gt;

&lt;li&gt;The ability to log ecommerce transactions - useful for merchants wanting to track orders&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;A huge thanks to community member &lt;a href='https://github.com/ramn'&gt;Simon Andersson&lt;/a&gt; from &lt;a href='http://www.qwaya.com'&gt;Qwaya&lt;/a&gt; for contributing the ecommerce tracking functionality - thank you Simon!&lt;/p&gt;

&lt;p&gt;We&amp;#8217;ll take a look at both of these new features in turn:&lt;/p&gt;

&lt;h2 id='site_id'&gt;Site ID&lt;/h2&gt;

&lt;p&gt;The Snowplow JavaScript tracker now lets you set a site identifier before you start logging events. The new method for this is called &lt;code&gt;setSiteId()&lt;/code&gt; - it takes one argument, the identifier you have assigned to this site. For example:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='javascript'&gt;&lt;span class='nx'&gt;_snaq&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='nx'&gt;push&lt;/span&gt;&lt;span class='p'&gt;([&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;setAccount&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;d3rkrsqld9gmqf&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;]);&lt;/span&gt;
&lt;span class='nx'&gt;_snaq&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='nx'&gt;push&lt;/span&gt;&lt;span class='p'&gt;([&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;setSiteId&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;CFe23a&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;]);&lt;/span&gt;
&lt;span class='nx'&gt;_snaq&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='nx'&gt;push&lt;/span&gt;&lt;span class='p'&gt;([&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;trackPageView&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;]);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The querystring passed to your Snowplow collector will now include the following parameter:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;...&amp;amp;said=CFe23a&amp;amp;...&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;where &lt;code&gt;said&lt;/code&gt; stands for &lt;em&gt;Site or App ID&lt;/em&gt; - because we plan on using the same parameter for mobile and desktop app tracking as well. &lt;!--more--&gt;&lt;/p&gt;

&lt;p&gt;This new feature should be helpful for anyone running multiple sites (or perhaps clients) against the same Snowplow collector - it means that you can easily partition your Snowplow events by site, whilst still being able to run cross-site analyses should you so wish.&lt;/p&gt;

&lt;p&gt;Note that we haven&amp;#8217;t yet added extracting &lt;code&gt;said&lt;/code&gt; to our ETL process, but we have an &lt;a href='https://github.com/snowplow/snowplow/issues/33'&gt;open ticket for this&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id='ecommerce_transactions'&gt;Ecommerce transactions&lt;/h2&gt;

&lt;p&gt;To date, we have been analysing e-commerce transactions using Snowplow by:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Logging every &lt;em&gt;product add to basket&lt;/em&gt; event&lt;/li&gt;

&lt;li&gt;Logging every &lt;em&gt;product remove from basket&lt;/em&gt; event&lt;/li&gt;

&lt;li&gt;Netting these events off to determine the final contents of the order&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This approach works, but it adds complexity in the analysis step. Happily community member Simon Andersson has contributed an alternative solution: dedicated Snowplow e-commerce transaction tracking, similar to the functionality found in the Google Analytics JavaScript API.&lt;/p&gt;

&lt;p&gt;The idea is that you add the new tracking code to your shop&amp;#8217;s checkout confirmation page, so that the completed order can be sent to Snowplow. A complete example of the new tracking code looks like this:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='javascript'&gt;&lt;span class='kd'&gt;var&lt;/span&gt; &lt;span class='nx'&gt;orderId&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;order-123&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;;&lt;/span&gt;

&lt;span class='c1'&gt;// addTrans sets up the transaction, should be called first.&lt;/span&gt;
&lt;span class='nx'&gt;_snaq&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='nx'&gt;push&lt;/span&gt;&lt;span class='p'&gt;([&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;addTrans&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
  &lt;span class='nx'&gt;orderId&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;                &lt;span class='c1'&gt;// order ID - required&lt;/span&gt;
  &lt;span class='s1'&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;                     &lt;span class='c1'&gt;// affiliation or store name&lt;/span&gt;
  &lt;span class='s1'&gt;&amp;#39;8000&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;                 &lt;span class='c1'&gt;// total - required&lt;/span&gt;
  &lt;span class='s1'&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;                     &lt;span class='c1'&gt;// tax&lt;/span&gt;
  &lt;span class='s1'&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;                     &lt;span class='c1'&gt;// shipping&lt;/span&gt;
  &lt;span class='s1'&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;                     &lt;span class='c1'&gt;// city&lt;/span&gt;
  &lt;span class='s1'&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;                     &lt;span class='c1'&gt;// state or province&lt;/span&gt;
  &lt;span class='s1'&gt;&amp;#39;&amp;#39;&lt;/span&gt;                      &lt;span class='c1'&gt;// country&lt;/span&gt;
  &lt;span class='p'&gt;]);&lt;/span&gt;

&lt;span class='c1'&gt;// addItem is called for each item in the shopping cart.&lt;/span&gt;
&lt;span class='nx'&gt;_snaq&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='nx'&gt;push&lt;/span&gt;&lt;span class='p'&gt;([&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;addItem&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
  &lt;span class='nx'&gt;orderId&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;                &lt;span class='c1'&gt;// order ID - required&lt;/span&gt;
  &lt;span class='s1'&gt;&amp;#39;1001&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;                 &lt;span class='c1'&gt;// SKU - required&lt;/span&gt;
  &lt;span class='s1'&gt;&amp;#39;Blue t-shirt&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;         &lt;span class='c1'&gt;// product name&lt;/span&gt;
  &lt;span class='s1'&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;                     &lt;span class='c1'&gt;// category&lt;/span&gt;
  &lt;span class='s1'&gt;&amp;#39;2000&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;                 &lt;span class='c1'&gt;// unit price - required&lt;/span&gt;
  &lt;span class='s1'&gt;&amp;#39;2&amp;#39;&lt;/span&gt;                     &lt;span class='c1'&gt;// quantity - required&lt;/span&gt;
  &lt;span class='p'&gt;]);&lt;/span&gt;
&lt;span class='nx'&gt;_snaq&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='nx'&gt;push&lt;/span&gt;&lt;span class='p'&gt;([&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;addItem&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
  &lt;span class='nx'&gt;orderId&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;                &lt;span class='c1'&gt;// order ID - required&lt;/span&gt;
  &lt;span class='s1'&gt;&amp;#39;1002&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;                 &lt;span class='c1'&gt;// SKU - required&lt;/span&gt;
  &lt;span class='s1'&gt;&amp;#39;Red shoes&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;            &lt;span class='c1'&gt;// product name&lt;/span&gt;
  &lt;span class='s1'&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;                     &lt;span class='c1'&gt;// category&lt;/span&gt;
  &lt;span class='s1'&gt;&amp;#39;4000&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;                 &lt;span class='c1'&gt;// unit price - required&lt;/span&gt;
  &lt;span class='s1'&gt;&amp;#39;1&amp;#39;&lt;/span&gt;                     &lt;span class='c1'&gt;// quantity - required&lt;/span&gt;
  &lt;span class='p'&gt;]);&lt;/span&gt;

&lt;span class='c1'&gt;// trackTrans sends the transaction to Snowplow tracking servers.&lt;/span&gt;
&lt;span class='c1'&gt;// Must be called last to commit the transaction.&lt;/span&gt;
&lt;span class='nx'&gt;_snaq&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='nx'&gt;push&lt;/span&gt;&lt;span class='p'&gt;([&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;trackTrans&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;]);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The above example creates an order (aka &amp;#8220;transaction&amp;#8221;) with ID &lt;code&gt;order-123&lt;/code&gt; and then adds two line items (two blue t-shirts and one pair of red shoes) as line items to the order. The final &lt;code&gt;trackTrans&lt;/code&gt; call sends this complete order to Snowplow as three separate events - one each for the order and its line items.&lt;/p&gt;

&lt;p&gt;This new functionality should be useful for anybody who wants to track orders transacted in a online shopping cart such as Magento, PrestaShop or Spree.&lt;/p&gt;

&lt;p&gt;Note that we haven&amp;#8217;t yet added extracting these e-commerce orders to our ETL process, but we have an &lt;a href='https://github.com/snowplow/snowplow/issues/34'&gt;open ticket for this&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id='upgrading'&gt;Upgrading&lt;/h2&gt;

&lt;p&gt;We have made the minified JavaScript tracker version 0.6 available on this URL:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http://d1fc8wv8zag5ca.cloudfront.net/0.6/sp.js&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There are no breaking changes with the previous version 0.5, so you can upgrade your existing Snowplow JavaScript tracker without issue.&lt;/p&gt;

&lt;p&gt;Note that we have now added versioning to the JavaScript tracker&amp;#8217;s URL. This is because we have &amp;#8220;breaking changes&amp;#8221; to the JavaScript tracker in the pipeline (see e.g. issues &lt;a href='https://github.com/snowplow/snowplow/issues/29'&gt;#29&lt;/a&gt; and &lt;a href='https://github.com/snowplow/snowplow/issues/32'&gt;#32&lt;/a&gt;).&lt;/p&gt;

&lt;h2 id='thanks'&gt;Thanks&lt;/h2&gt;

&lt;p&gt;A final note to say thanks again to &lt;a href='https://github.com/ramn'&gt;Simon Andersson&lt;/a&gt; for contributing the ecommerce tracking functionality! Community contributors like Simon A and Simon R(umble) are helping us to quickly make the Snowplow vision a reality.&lt;/p&gt;

&lt;p&gt;And of course, we welcome contributions across the five Snowplow sub-systems. If you would like help implementing a new tracker, trying a different ETL approach or loading Snowplow events into an alternative database, please &lt;a href='mailto:contribute@snowplowanalytics.com'&gt;get in touch&lt;/a&gt;!&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/08/21/amazon-glacier-launch</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/08/21/amazon-glacier-launch"/>
    <title>Amazon announces Glacier - lowers the cost of running Snowplow</title>
    <updated>2012-08-21T00:00:00+01:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;Today Amazon announced the launch of &lt;a href='http://aws.amazon.com/glacier/'&gt;Amazon Glacier&lt;/a&gt;, which is a low-cost data archiving service designed for rarely accessed data.&lt;/p&gt;

&lt;p&gt;As Werner Vogels described it in his &lt;a href='http://www.allthingsdistributed.com/2012/08/amazon-glacier.html'&gt;blog post&lt;/a&gt; this morning:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Amazon Glacier provides the same high durability guarantee as Amazon S3 but relaxes the access times to a few hours. This is the right service for customers who have archival data that requires highly reliable storage but for which immediate access is not needed.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;At first sight, Amazon Glacier looks to be a fantastic fit for archiving the raw event logs generated by the Snowplow collector (whether the CloudFront collector or alternatives such as &lt;a href='https://github.com/shermozle/SnowCannon'&gt;SnowCannon&lt;/a&gt;). Once the nightly Snowplow ETL has been run on your raw event logs, you shouldn&amp;#8217;t need to access those raw logs frequently. However, we would always recommend retaining them, as there may well be a reason to revisit them in the future. We never recommend throwing away atomic source data!&lt;/p&gt;

&lt;p&gt;This is where Amazon Glacier comes in - at the proposed pricing levels for Glacier, you could archive 2 terabytes of raw Snowplow data for around $20 a month; this would be significantly cheaper than storing your raw logs in Amazon S3, which is the current Snowplow approach.&lt;/p&gt;

&lt;p&gt;Moreover, Werner has indicated that:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;In the coming months, Amazon S3 will introduce an option that will allow customers to seamlessly move data between Amazon S3 and Amazon Glacier based on data lifecycle policies.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Once Amazon has launched this feature, we&amp;#8217;ll get this automatic S3-&amp;gt;Glacier archiving process working internally, and then release a howto for Snowplow users so you can do the same, and start running your Snowplow over Amazon Glacier!&lt;/p&gt;

&lt;p&gt;Exciting times for everybody who likes storing atomic event data cheaply and safely - stay tuned!&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/08/20/snowplow-0.4.6-released</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/08/20/snowplow-0.4.6-released"/>
    <title>Snowplow 0.4.6 released</title>
    <updated>2012-08-20T00:00:00+01:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;Over the weekend we released Snowplow version &lt;strong&gt;0.4.6&lt;/strong&gt;. This was a minor release that added a new capability into the Snowplow JavaScript tracker.&lt;/p&gt;

&lt;p&gt;Specifically, with the JavaScript you can now specify your own collector URL, rather than simply pass in an account ID which resolves to a CloudFront bucket.&lt;/p&gt;

&lt;p&gt;You can use this feature in your JavaScript invocation code like so:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='javascript'&gt;&lt;span class='c'&gt;&amp;lt;!--&lt;/span&gt; &lt;span class='nx'&gt;Snowplow&lt;/span&gt; &lt;span class='nx'&gt;starts&lt;/span&gt; &lt;span class='nx'&gt;plowing&lt;/span&gt; &lt;span class='o'&gt;--&amp;gt;&lt;/span&gt;
&lt;span class='o'&gt;&amp;lt;&lt;/span&gt;&lt;span class='nx'&gt;script&lt;/span&gt; &lt;span class='nx'&gt;type&lt;/span&gt;&lt;span class='o'&gt;=&lt;/span&gt;&lt;span class='s2'&gt;&amp;quot;text/javascript&amp;quot;&lt;/span&gt;&lt;span class='o'&gt;&amp;gt;&lt;/span&gt;
&lt;span class='kd'&gt;var&lt;/span&gt; &lt;span class='nx'&gt;_snaq&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='nx'&gt;_snaq&lt;/span&gt; &lt;span class='o'&gt;||&lt;/span&gt; &lt;span class='p'&gt;[];&lt;/span&gt;

&lt;span class='nx'&gt;_snaq&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='nx'&gt;push&lt;/span&gt;&lt;span class='p'&gt;([&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;setCollectorUrl&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;collector.mydomain.com&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;]);&lt;/span&gt;
&lt;span class='nx'&gt;_snaq&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='nx'&gt;push&lt;/span&gt;&lt;span class='p'&gt;([&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;trackPageView&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;]);&lt;/span&gt;

&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='kd'&gt;function&lt;/span&gt;&lt;span class='p'&gt;()&lt;/span&gt; &lt;span class='p'&gt;{&lt;/span&gt;
&lt;span class='p'&gt;...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Where &lt;code&gt;collector.mydomain.com&lt;/code&gt; is the URL to your own collector.&lt;/p&gt;

&lt;p&gt;We added this capability to Snowplow in support of Simon Rumble&amp;#8217;s excellent &lt;a href='https://github.com/shermozle/SnowCannon'&gt;SnowCannon&lt;/a&gt; prototype node.js collector for Snowplow. Going forwards you can of course use this custom URL to send your Snowplow events to any kind of collector on a domain you control.&lt;/p&gt;

&lt;p&gt;Anyway I hope you like the feature and let us know how you get on with it!&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/08/14/updated-hive-serde-released</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/08/14/updated-hive-serde-released"/>
    <title>Updated Hive SerDe released</title>
    <updated>2012-08-14T00:00:00+01:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;One of the key elements in the Snowplow technology stack is the Hive SerDe. This is what makes it possible for Elastic MapReduce to read the Cloudfront log files generated by the Snowplow javascript trackings tags, extarct the relevant fields and make these available in Hive as a nice, clean query table. (The structure of the Hive table is documented &lt;a href='https://github.com/snowplow/snowplow/wiki/Hive-data-structure'&gt;here&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;A number of improvements have been made in the new versions. However, the most significant is that the 5 utm_marketing fields have been added, so that campaign attributes are now available for analytics.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/08/13/introducing-snow-cannon-a-node-js-collector-for-snowplow</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/08/13/introducing-snow-cannon-a-node-js-collector-for-snowplow"/>
    <title>SnowCannon - a node.js collector for Snowplow</title>
    <updated>2012-08-13T00:00:00+01:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We are hugely excited to introduce &lt;a href='https://github.com/shermozle/SnowCannon'&gt;SnowCannon&lt;/a&gt;, a Node.js collector for Snowplow, authored by &lt;a href='http://twitter.com/shermozle'&gt;@shermozle&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;SnowCannon is an alternative collector to the default cloudfront collector included with Snowplow. It offers a number of significant advantages over the Cloudfront connector:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;It allows the use of 3rd party cookies. In particular, this makes it possible to track usage across multiple domains&lt;/li&gt;

&lt;li&gt;It enables real-time analytics. (This is not possible with the Cloudfront-enabled collector, where there&amp;#8217;s a 20-30 minute delay between the javascript tracking event and the associated log being written to S3.)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To learn more about SnowCannon, visit the &lt;a href='https://github.com/shermozle/SnowCannon'&gt;Github repo&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;SnowCannon is the first user-contributed module for Snowplow, and we are delighted to see community members working to build out the Snowplow platform. There are other contributions in the works, including a Snowplow IOS client, that we hope to be announcing shortly.&lt;/p&gt;

&lt;p&gt;To encourage users to extend Snowplow, we&amp;#8217;ve architected Snowplow in a module way, to enable developers to swap out elements in the Snowplow stack with their own elements or complimenet those already in the stack with parallel implementations. Learn more about the Snowplow architecture &lt;a href='/product/technical-architecture.html'&gt;here&lt;/a&gt;.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/08/02/snowplow-setup-documentation-overhauled</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/08/02/snowplow-setup-documentation-overhauled"/>
    <title>The setup guide has been overhauled</title>
    <updated>2012-08-02T00:00:00+01:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;Following a lot of invaluable feedback from users setting up Snowplow for the first time, we&amp;#8217;ve updated the Snowplow setup documentation.&lt;/p&gt;

&lt;p&gt;The documentation can be found &lt;a href='https://github.com/snowplow/snowplow/wiki/Snowplow-setup-guide'&gt;here&lt;/a&gt;. Any further feedback would be much appreciated - we want to make it as painless as possible for Snowplow newbies to get up and running&amp;#8230;&lt;/p&gt;</content>
  </entry>
  
 
</feed>