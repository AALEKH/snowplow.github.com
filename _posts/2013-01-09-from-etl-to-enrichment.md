---
layout: blog-post
shortenedlink: From ETL to enrichment - development roadmap
title: The SnowPlow development roadmap for the ETL step - from ETL to enrichment
tags: etl enrichment
category: Inside the Plow
author: yali
---

In this blog post, we outline our plans to develop the [etl] [etl] part of the SnowPlow stack. Although in many respects the least sexy element of the stack, it is critical to SnowPlow. We intend to rearchitect the ETL step in quite significant ways. In this post, we discuss our plans and the rationale behind them, in the hope to get

1. Feedback from the community on them
2. Ideas for alternative approaches or new features

We will cover:

1. [Recap: the point of the ETL step] [purpose]
2. [Limitations with the current, Hive-based ETL process] [limitations]
3. [From ETL to enrichment] [enrichment]: what we want the ETL step to achieve
4. [Towards a real-time ETL] [speed]: speeding things up
5. [Moving to Cascading / Scalding] [scalding]: what we plan to do
6. [Benefits of this approach] [benefits]: both in the short and long term

To get the conversation started, a conceptual map of the new ETL process is shown below. (You will probably want to click on it to see a blown up PDF version, as it is rather large...)

<p><a href="/static/pdf/snowplow-scalding-etl-specification.pdf"><img src="/static/img/blog/2013/01/scalding-etl-spec.gif"></a></p>

<!--more-->

<a name="purpose"><h2>Recap: the point of the ETL step</h2></a>

The primary purpose of the ETL step is to parse the logs generated by the SnowPlow collector(s) and pushes it into one or more storage options (e.g. S3, Infobright) where it can be analysed. However, there are two complexities that have to be dealt with:

1. **Checking data quality and resolving any issues**. Sometimes, SnowPlow has not been setup right. Sometimes, there may even be a bug in a tracker or collector, that means log files contain errors. In an ideal world, the ETL step should validate the lines of data in the logs, push data through to storage when the quality of the data is good, and initiate a process for handling malformed data in the unfortunate cases when it is not. (Note: most web analytics programmes do not support this, so if you haven't set your tracking up properly and haven't been logging data correctly for a couple of months - tough - there's no way of fixing it.)
2. **Supporting multiple storage options**. We want SnowPlow to support the widest range of analytics: encompassing [OLAP style aggregations] [olap], slicing and dicing of data, [Mahout-like machine learning] [mahout] and [Sky-like] [sky-db] event stream analytics.  The ETL step has to be powerful enough to push data into multiple locations in an efficient manner, and support pushing different cuts and structures of the data into each of those different storage options.


<a name="limitations"><h2>Limitations with the current, Hive-based ETL process</h2></a>

The current ETL process is based on Hive, which processes Cloudfront-formatted log files using a [custom deserializer] [custom-serde]. This was a good option to build an initial prototype of the ETL step: it enabled us to query data in the raw logs directly, and made it relatively straightforward to transfer the data from the SnowPlow log format into a more standard format suitable for faster querying in Hive or importing into Infobright. However, there are a number of important limitations to the Hive-based ETL process:

1. **It makes error handling very difficult**. Either a row is processed, or it is not. There's no option to build more sophisticated data processing pipelines including flows to handle malformed data.
2. **It is a highly coupled process**: all the parsing on the entire row is performed by the custom deserializer. If something goes wrong, it is hard to debug what went wrong. If we want to build out part of the ETL process, we have to go in and upgrade the deserializer or the Hive QL. As the conceptual map of our proposed ETL shown at the top of this post demonstrates, our ideal ETL process consists of multiple decoupled steps.
3. **It is hard to extend the ETL process to build enrichments of the data**. (See the [next section] (#enrichments).)


<a name="enrichment"><h2>From ETL to enrichment: what we want to achieve</h2> </a>

Although the initial purpose of the ETL step was quite narrow: to mvoe data generated by the collectors into the different storage options for analytics, we have since realised that there are a number of important enrichments that can be performed on the data, that are best done as part of the ETL step, so that they are available when the data comes to be analysed. Examples include:

1. Inferring location from `user_ipaddress` e.g. using [Maxmind] [maxmind] or [Digital Element] [digital-element].
2. Inferring marketing parameters (source, medium, keywords) by processing referrer url and query strings using [referer-parser] [referer-parser]. This would include identifying search engine originated traffic and social network originated traffic, for example.

In addition, decomposing some of the fields into constituent elements can make analysis easier: for example, breaking up `page_url` and `referrer_url` into host, domain, path and query string.

<a name="speed"><h2>Towards a real-time ETL process: speeding things up</h2></a>

The majority of SnowPlow users run their ETL process daily, so that yesterday's data is available today. 

We need to move the whole SnowPlow stack so that data is available for analytics faster. Doing so will be a welcome by analysts crunching SnowPlow data, but it will crucially open up the possibility of building real-time response engines based on SnowPlow data: these might include things like retargeting users who've performed specific actions with display ads or emails, or personalising the content shown to a user based on their recent browsing history.

There is limited scope to speed up the current Hive-based ETL process. However, there are lots of interesting opportunities that arise if we consider an alternative archtiecture.

<a name="scalding"><h2>Moving to Cascading / Scalding: how we plan to rearchitect the ETL process</h2></a>

We intend to replace the current Hive-based ETL process with one based on the Scala implementation of [Cascading] [cascading], known as [Scalding] [scalding].

Cascading is an application framework specifically designed to build robust data pipelines using Hadoop. We intend to use it to build the pipeline [sketched above] [pipeline].

<a name="benefits"><h2>Benefits of this approach: both in the short and long term</h2></a>

By rearchitecting the ETL using Scalding / Cascading, we hope to realise the following benefits in the short-term:

1. Deliver enrichments on the data: in particular, classify visits based on referrer and locate users via geo-ip
2. Improved handling of malformed data: making it easier to spot bugs in SnowPlow, mistakes in tracker or collector setup, and the ability to fix and reprocess malformed data
3. Make it easier to run the ETL process more frequently, so that SnowPlow data is more up-to-date

In the long term there are a number of important benefits we hope moving to Scalding will help us realise:

1. **Expand the ETL to output data in a format suitable for OLAP reporting**. Currently, users who want to use OLAP tools e.g. Tableau, Pentaho or Microstrategy, to report on SnowPlow data, need to transform that data prior to running those tools on top of it. We want to build out the ETL process to output two versions of the data: the raw event field (as it currently does) and a cube-formatted version that can be used directly with these tools. Delivering this with the current Hive-based process would be incredibly difficult.
2. **Move towards a real-time engine**. In order to deliver data in real-time, SnowPlow ETL would need to move from a Hadoop, batch-based process into a stream-based process, likely using [Storm] [storm]. Moving the data pipeline from Cascading to Storm should be significantly easier than from Hive to storm: as such, Cascading provides a useful stepping stone on our journey to deliver real time event-level analytics.
3. **Make it easier to support a wider range of collector log formats**. Because the ETL process is decoupled, handling a different log file format means only updating the first processing step in the data pipeline. That means building out the ETL to support other collectors (e.g. [SnowCannon] [snowcannon]) should be much simpler.
5. **Make it easier to support a growing range of event types**. As should be clear from the [data pipeline flowchart] [pipeline], seven event types are currently supported, each with their own set of fields. (Page views, page pings, link clicks, custom events, ad impressions, transaction events and transaction items.) That list is only likely to grow over time. By clearly differentiating each of them in the data pipeline, a Scalding-based ETL process should be easier to extend to support a greater range of events.

## We want your feedback

We've been very lucky to have community members contribute an enormous number of fantastic ideas and code that we've been able to incorporate into SnowPlow. We've shared our roadmap for the ETL step and our rationale for that roadmap to see what you think. Does our approach sound sensible? What should we do differently? What can we add to it to make it more robust and valuable?



[etl]: https://github.com/snowplow/snowplow/wiki/etl
[collector-dev-roadmap]: /blog/2013/01/07/the-clojure-collector-in-detail/
[clojure-collector]: https://github.com/snowplow/snowplow/wiki/setting-up-the-clojure-collector
[scalding-spec-top-half]: /static/img/blog/2013/01/scalding-etl-spec-1-2.gif
[scalding-spec-bottom-half]: /static/img/blog/2013/01/scalding-etl-spec-2-2.gif
[scalding-spec-total]: /static/img/blog/2013/01/scalding-etl-spec.gif
[purpose]: /blog/2013/01/09/from-etl-to-enrichment/#purpose
[limitations]: /blog/2013/01/09/from-etl-to-enrichment/#limitations
[enrichment]: /blog/2013/01/09/from-etl-to-enrichment/#enrichment
[scalding]: /blog/2013/01/09/from-etl-to-enrichment/#scalding
[benefits]: /blog/2013/01/09/from-etl-to-enrichment/#benefits
[speed]: /blog/2013/01/09/from-etl-to-enrichment/#speed
[olap]: /analytics/tools-and-techniques/converting-snowplow-data-into-a-format-suitable-for-olap.html
[mahout]: http://mahout.apache.org/
[sky-db]: https://github.com/skydb
[custom-serde]: https://github.com/snowplow/snowplow/tree/master/3-etl/hive-etl/snowplow-log-deserializers
[maxmind]: http://www.maxmind.com/en/geolocation_landing
[digital-element]: http://www.digitalelement.com/our_technology/our_technology.html
[referer-parser]: https://github.com/snowplow/referer-parser 
[cascading]: http://www.cascading.org/
[scalding]: https://github.com/twitter/scalding
[pipeline]: /static/pdf/snowplow-scalding-etl-specification.pdf
[storm]: http://storm-project.net/
[snowcannon]: /blog/2012/08/13/introducing-snow-cannon-a-node-js-collector-for-snowplow/