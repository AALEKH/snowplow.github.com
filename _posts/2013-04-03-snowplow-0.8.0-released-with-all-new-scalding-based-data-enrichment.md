---
layout: blog-post
shortenedlink: Snowplow 0.8.0 released
title: Snowplow 0.8.0 released with all-new Scalding-based data enrichment
tags: snowplow scalding hadoop etl
author: Alex
category: Releases
---

A new month, a new release! We're excited to announce the immediate availability of Snowplow version **0.8.0**. This has been our most complex release to date: we have done a full rewrite our ETL (aka enrichment) process, adding a few nice performance and data quality improvements in the process.

This release has been heavily informed by our January blog post, [The Snowplow development roadmap for the ETL step - from ETL to enrichment](/blog/2013/01/09/from-etl-to-enrichment/#scalding). In technical terms, we have ported our existing ETL process (which was a combination of HiveQL scripts plus a custom Java deserializer) to a new Hadoop-only ETL process which does not require Hive. The new ETL process is written in Scala, using [Scalding] [scalding], a Scala API built on top of [Cascading] [cascading], the Hadoop ETL framework.

In the rest of this post we will cover:

1. [The benefits of the new ETL](#benefits)
2. [Limitations of the new ETL](#limitations)
3. [A note for Infobright/Hive users](#infobright-hive-note)
4. [Upgrading and usage](#upgrading-usage)
5. [Getting help](#help)

Read on below the fold to find out more.

<!--more-->

<h2><a name="benefits">1. Benefits of the new ETL</a></h2>

The new ETL process is essentially a direct re-write of the existing Hive-based ETL process, however we have made some functionality improvements along the way. The benefits of the new Scalding-based ETL process as we see them are as follows:

1. **Fewer moving parts** - the new ETL process no longer requires Hive running on top of Hadoop. This should make it simpler to setup and more robust
2. **New data validation** - the new ETL process runs a set of validation checks on each raw line of Snowplow log data. If a line does not pass validation, then the line along with its validation errors is written to a new bucket for "bad rows"
3. **Improved performance** - in our performance tests, the new ETL process is significantly faster than the old process
4. **Better handling of unexpected errors** - if you set your ETL process to continue on unexpected errors, any raw lines which trigger unexpected errors will appear in a new "errors" bucket
5. **Fewer Redshift import errors** - we now truncate six "high-risk" fields (`useragent`, `page_title` et al) and validate that `ev_value` is a float, to prevent Redshift load errors
6. **Stronger technical foundation for our roadmap** - the foundations are now in-place for us adding more enrichment of our Snowplow events (e.g. referer parsing and geo-location - both coming soon), and the "gang of three" cross-row ETL processes we are planning (one, two, three)

<h2><a name="limitations">2. Limitations of the new ETL</a></h2>

We want to be very clear about the limitations of the new ETL process as it stands today:

1. **Redshift only** - the new ETL process only supports writing out in Redshift format. We discuss this further in [A note for Infobright/Hive users](#infobright-hive-note) below.
2. **Small files problem** - being Hadoop-based, our ETL inherits Hadoop's ["small files problem"] [small-files-problem]. Above around 3,000 raw Snowplow log files, the job slows down considerably.
3. **Prototype** - please treat the new ETL process as a prototype. We **strong recommend** trying it out away from your existing Snowplow installation rather than upgrading your existing process in-place

<h2><a name="limitations">3. A note for Infobright/Hive users</a></h2>

If you are using Infobright or plain-Hive to store your Snowplow data, we understand that you'll be feeling a little left out of this release. Unfortunately, releasing support for Redshift, Infobright and Hive all in the v1 of the new ETL process just wasn't feasible from a development-effort perspective.

The good news is that it's totally safe to upgrade to 0.8.0 for Infobright/Hive users: the Hive-based ETL process continues to work as before, and we will be continuing to support the Hive ETL with bug fixes etc for the foreseeable future.

For **Hive users** - the good news is that we are working on a new Avro-based storage format for Snowplow events. Being based on Avro, it should be less fragile than our existing flatfile approach, and easily queryable from Hive using the AvroSerde. This will be the 0.9.x release series and should come later in Q2 or early Q3.

For **Infobright users** - we will be adding Infobright support into the new Hadoop-based ETL later this year. If you would rather not wait, we recommend switching to Redshift now or switching to PostgreSQL support when that is release in late Q2.

As always, you can checkout what features are coming approximately when on our [Product roadmap] [product-roadmap] wiki page.

<h2><a name="limitations">4. Upgrading and usage</a></h2>

Upgrading is simply a matter of upgrading your EmrEtlRunner installation and then updating your `config.yml` configuration file; nothing else is changed in this release.

The new `config.yml` file format introduces a few new configuration options:

REST TO WRITE.

Using Snowplow (command-line options, the StorageLoader etc) is unchanged.

<h2><a name="help">5. Getting help</a></h2>

As always, if you do run into any issues or don't understand any of the above changes, please [raise an issue] [issues] or get in touch with us via [the usual channels] [talk-to-us].

[scalding]: https://github.com/twitter/scalding
[cascading]: http://www.cascading.org

[small-files-problem]: http://amilaparanawithana.blogspot.co.uk/2012/06/small-file-problem-in-hadoop.html

[product-roadmap]: https://github.com/snowplow/snowplow/wiki/Product-roadmap

[issues]: https://github.com/snowplow/snowplow/issues
[talk-to-us]: https://github.com/snowplow/snowplow/wiki/Talk-to-us