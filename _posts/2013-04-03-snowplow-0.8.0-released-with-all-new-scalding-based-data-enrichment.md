---
layout: blog-post
shortenedlink: Snowplow 0.8.0 released
title: Snowplow 0.8.0 released with all-new Scalding-based data enrichment
tags: snowplow scalding hadoop etl
author: Alex
category: Releases
---

A new month, a new release! We're excited to announce the immediate availability of Snowplow version **0.8.0**. This has been our most complex release to date: we have done a full rewrite our ETL (aka enrichment) process, adding a few nice performance and data quality improvements in the process.

In technical terms, we have ported our existing ETL process (which was a combination of HiveQL scripts plus a custom Java deserializer) to a new Hadoop-only ETL process which does not require Hive. The new ETL process is written in Scala, using [Scalding] [scalding], a Scala API built on top of [Cascading] [cascading], the Hadoop ETL framework.

In the rest of this post we will cover:

1. [The benefits of the new ETL](/blog/2013/04/03/snowplow-0.8.0-released-with-all-new-scalding-based-data-enrichment#benefits)
2. [Limitations of the new ETL](/blog/2013/04/03/snowplow-0.8.0-released-with-all-new-scalding-based-data-enrichment#limitations)
3. [A note for Infobright Snowplow users](/blog/2013/04/03/snowplow-0.8.0-released-with-all-new-scalding-based-data-enrichment#infobright-note)
4. [Upgrading and usage](/blog/2013/04/03/snowplow-0.8.0-released-with-all-new-scalding-based-data-enrichment#upgrading-usage)
5. [Getting help](/blog/2013/04/03/snowplow-0.8.0-released-with-all-new-scalding-based-data-enrichment#help)

Read on below the fold to find out more.

<!--more-->

<h2><a name="benefits">1. Benefits of the new ETL</a></h2>

The new ETL process is essentially a direct re-write of the existing Hive-based ETL process, however we have made some functionality improvements along the way. The benefits of the new Scalding-based ETL process as we see them are as follows:

1. **Fewer moving parts** - the new ETL process no longer requires Hive running on top of Hadoop. This should make it simpler to setup and more robust
2. **New data validation** - the new ETL process runs a set of validation checks on each raw line of Snowplow log data. If a line does not pass validation, then the line along with its validation errors is written to a new bucket for "bad rows"
3. **Improved performance** - in our performance tests, the new ETL process is significantly faster than the old process
4. **Better handling of unexpected errors** - if you set your ETL process to continue on unexpected errors, any raw lines which trigger unexpected errors will appear in a new "errors" bucket
5. **Stronger technical foundation for our roadmap** - the foundations are now in-place for us adding more enrichment of our SnowPlow events (e.g. referer parsing and geo-location - both coming soon), and the "gang of three" cross-row ETL processes we are planning (one, two, three)




 from HiveQL + a custom Hive deserializer to a pure-Hadoop (no-Hive) in Scalding

 The primary purpose of this release is to clean up and rationalise our event data model, in particular around **user IDs** and **event timestamps**. This release should lay the foundations for more sophisticated eventstream analytics (such as funnel analysis), by:

 <h2><a name="help">5. Getting help</a></h2>

As always, if you do run into any issues or don't understand any of the above changes, please [raise an issue] [issues] or get in touch with us via [the usual channels] [talk-to-us].

[issues]: https://github.com/snowplow/snowplow/issues
[talk-to-us]: https://github.com/snowplow/snowplow/wiki/Talk-to-us