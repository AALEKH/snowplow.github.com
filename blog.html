<!DOCTYPE html>
<html>
<head>
	
	<title></title>
	

	<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
	<link href="/static/css/styles.css" type="text/css" rel="stylesheet" />
	<link href="/static/css/pygments.css" type="text/css" rel="stylesheet" />
	

	<!-- SnowPlow starts plowing -->
	<script type="text/javascript">
	var _snaq = _snaq || [];

	_snaq.push(['setAccount', 'd3v6ndkyapxc2w']);
	_snaq.push(['trackPageView']);
	_snaq.push(['enableLinkTracking']);

	(function() {
	var sp = document.createElement('script'); sp.type = 'text/javascript'; sp.async = true; sp.defer = true;
	sp.src = ('https:' == document.location.protocol ? 'https' : 'http') + '://d1fc8wv8zag5ca.cloudfront.net/sp.js';
	var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(sp, s);
	})();
	 </script>
	<!-- SnowPlow stops plowing -->	

	<!--Google Analytics tracking-->
	<script type="text/javascript">

	  var _gaq = _gaq || [];
	  _gaq.push(['_setAccount', 'UA-34290195-1']);
	  _gaq.push(['_trackPageview']);

	  (function() {
	    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
	    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
	    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
	  })();

	</script>
	<!--Finish Google Analytics tracking-->
</head>
<body>
	<div id="container">
		<div id="header" class="span-24">
  <div id="logo">
    <h1><a href="/">SnowPlow</a></h1>
    <p>Your web analytics data in your hands</p>
  </div>
  <div id="menu" class="span-15">
    <ul>
      <li ><a href="/product/index.html">Product</a></li>
      <li ><a href="/services/index.html">Services</a></li>
      <li ><a href="/analytics/index.html">Analytics</a></li>
      <li ><a href="/technology/index.html">Technology</a></li>
      <li  class="active" ><a href="/blog.html">Blog</a></li>
      <li ><a id="mail" href="/contact/index.html">Contact</a></li>
    </ul>
  </div>
</div>
	
		<div id="contents">
	
		<a name="infobright-ruby-loader-released" />
		<div class="post">
			21 Oct 2012
			<h1><a href="/blog/2012/10/21/infobright-ruby-loader-released">Infobright Ruby Loader Released</a></h1>
			<p>We&#8217;re pleased to start the week with the release of a new Ruby gem, our <a href='https://github.com/snowplow/infobright-ruby-loader'>Infobright Ruby Loader</a> (IRL).</p>

<p>At SnowPlow we&#8217;re committed to supporting multiple different storage and analytics options for SnowPlow events, alongside our current Hive-based approach. One of the alternative data stores we are working with is <a href='http://www.infobright.org/'>Infobright</a>, a columnar database which is available in open source and commercial versions.</p>

<p>For all but the largest SnowPlow users, columnar databases such as Infobright should be an attractive alternative to doing all of your analysis in Hive. The main advantages of columnar databases are as follows:</p>

<ol>
<li>Scale to terabytes (although not petabytes, unlike Hive)</li>

<li>Fixed cost (dedicated RAM-heavy analytics server), versus pay-as-you-go querying on Amazon EMR</li>

<li>Significantly faster query times â€“ typically seconds, not minutes</li>

<li>Plug in to many analytics front ends e.g. Tableau, Qlikview, R</li>
</ol>

<p>So, open source columnar databases like Infobright Community Edition (ICE) are a good fit for SnowPlow analytics. Unfortunately, when we started to load SnowPlow event logs into ICE, we realised that there wasn&#8217;t a good data-loading solution for Infobright in Ruby, our ETL language of choice. So, we built one :-)</p>

<p>Our freshly minted <a href='https://github.com/snowplow/infobright-ruby-loader'>Infobright Ruby Loader</a> (IRL) can be used in two different ways:</p>

<ol>
<li><strong>As a command-line tool</strong> - for manual loading of data into Infobright at the command-line. No Ruby expertise required</li>

<li><strong>As part of another application</strong> - because it&#8217;s a Ruby gem with a Ruby API, IRL can be integrated into larger Ruby ETL processes</li>
</ol>

<p>We will be using IRL at SnowPlow as part of our larger ETL process to load SnowPlow events into ICE for analysis - we hope to roll this out within the next few weeks.</p>

<p>In the meantime, we hope that IRL is useful to people in the Infobright community who need to run data loads at the command-line; IRL was inspired by <a href='http://www.infobright.org/Blog/Entry/unscripted/'>ParaFlex</a>, an excellent Bash script from the Infobright team to perform parallel loading of Infobright, and can be used as a direct alternative to ParaFlex.</p>

<p>To find out more about our Infobright Ruby Loader, please check out the detailed <a href='https://github.com/snowplow/infobright-ruby-loader/blob/master/README.md'>README</a> in the GitHub repository. And please direct any questions through the <a href='https://github.com/snowplow/snowplow/wiki/Talk-to-us'>usual channels</a>!</p>
			<span class="comments-link"><a href="/blog/2012/10/21/infobright-ruby-loader-released#disqus_thread" rel="nofollow">View Comments</a></span>
		</div>
	
		<a name="How the role of Hive is changing in SnowPlow" />
		<div class="post">
			12 Oct 2012
			<h1><a href="/blog/2012/10/12/how-the-role-of-hive-is-changing-at-snowplow">How we use Hive at SnowPlow, and how the role of Hive is changing. (Slides from our presentation to Hive London.)</a></h1>
			<p>Last night I gave a presentation to the clever folks at Hive London covering three things:</p>

<ol>
<li>How big data technologies like Apache Hive are transforming web analytics</li>

<li>Howe we&#8217;ve used Hive in SnowPlow development</li>

<li>How the role of Hive has changed at SnowPlow over time, including a comparison of Hive against other technologies.</li>
</ol>

<p>The slides from the presentation are below. As always, any questions / comments, please post them below.</p>
<iframe marginwidth='0' scrolling='no' src='http://www.slideshare.net/slideshow/embed_code/14696456' width='427' height='356' frameborder='0' style='border:1px solid #CCC;border-width:1px 1px 0;margin-bottom:5px' marginheight='0'>  </iframe>
			<span class="comments-link"><a href="/blog/2012/10/12/how-the-role-of-hive-is-changing-at-snowplow#disqus_thread" rel="nofollow">View Comments</a></span>
		</div>
	
		<a name="SnowPlow 0.4.10 released with Hive deserializer improvements" />
		<div class="post">
			11 Oct 2012
			<h1><a href="/blog/2012/10/11/snowplow-0.4.10-released">SnowPlow 0.4.10 released</a></h1>
			<p>We have just released version <strong>0.4.10</strong> of SnowPlow - people using 0.4.8 can jump straight to this version. This version updates:</p>

<ol>
<li>snowplow.js to version 0.7.0</li>

<li>the Hive deserializer to version 0.4.9</li>
</ol>

<p>Big thanks to community members <a href='https://github.com/mtibben'>Michael Tibben</a> from <a href='http://99designs.com'>99designs</a> and <a href='https://github.com/ramn'>Simon Andersson</a> from <a href='http://www.qwaya.com'>Qwaya</a> for their most-helpful contributions to this release!</p>

<h2 id='main_changes'>Main changes</h2>

<p>The main changes are as follows:</p>

<ul>
<li>The querystring parameter for site ID which the JavaScript tracker sends to your collector is renamed from <code>said</code> to <code>aid</code></li>

<li>The Hive-based ETL process now extracts the ecommerce tracking fields and the site ID field and adds them into your processed events table</li>

<li>We fixed a bug in the Hive deserializer where a partially-processed row was returned even if a fatal error was found in the row (now, a null row is returned instead)</li>
</ul>

<p>The rest of the changes were all enhancements to the Hive deserializer&#8217;s Specs2 test suite - these improvements should help to accelerate work on the deserializer (we have lots of cool new stuff we want to add to the deserializer!).</p>

<h2 id='new_event_table_fields'>New event table fields</h2>

<p>The new fields in the event table all relate directly to additional tracking functionality which was added to the JavaScript tracker in <a href='/blog/2012/09/06/snowplow-0.4.7-released/'>SnowPlow 0.4.7</a>. Specifically:</p>

<ol>
<li>The <code>setSiteId()</code> functionality is now extracted to the <code>app_id</code> field (short for application ID)</li>

<li>The ecommerce tracking functionality is now extracted to a set of <code>tr_</code> and <code>ti_</code> fields</li>
</ol>

<p>For details on the new fields, please review our latest <a href='/analytics/snowplow-table-structure.html'>Hive events table definition</a> - there is now a column indicating in which version a given field was added.</p>

<h2 id='how_to_get_the_new_version'>How to get the new version</h2>

<p>As usual, the new version of the Hive deserializer is available from the GitHub repository&#8217;s <a href='https://github.com/snowplow/snowplow/downloads'>Downloads</a> section as <strong>snowplow-log-deserializers-0.4.9.jar</strong>.</p>

<p>The updated snowplow.js is <a href='https://raw.github.com/snowplow/snowplow/master/1-trackers/javascript-tracker/js/snowplow.js'>available in our GitHub repository</a> for you to minify and upload, or alternatively you can use the one on our CDN:</p>

<pre><code>https://d1fc8wv8zag5ca.cloudfront.net/0.7.0/sp.js</code></pre>

<p>If you have any problems with either of these components, please <a href='https://github.com/snowplow/snowplow/issues'>raise an issue</a>!</p>

<h2 id='a_note_on_backwards_compatibility_for_the_events_table'>A note on backwards compatibility for the events table</h2>

<p>We will continue to add extra fields to the SnowPlow events table as we add extra capabilities to the ETL process - for example, we are working on functionality to extract geo-location information from IP addresses via MaxMind.</p>

<p>Starting with our new <code>app_id</code> field, we will be adding all such new fields to the <strong>end</strong> of our Hive events table definition. This will mean that you will <strong>not</strong> have to re-run the ETL process across all your historic raw logs, provided you do <strong>not</strong> need the data found in the new fields. This is because a Hive query across both the old event table format and the new table format works as long as you don&#8217;t explicitly query a new field.</p>

<p>In other words, Hive is futureproofed against new fields being added to the end of your underlying data files, and we&#8217;ll take advantage of this to improve backwards compatibility for our events table!</p>
			<span class="comments-link"><a href="/blog/2012/10/11/snowplow-0.4.10-released#disqus_thread" rel="nofollow">View Comments</a></span>
		</div>
	
		<a name="Attlib launched" />
		<div class="post">
			11 Oct 2012
			<h1><a href="/blog/2012/10/11/attlib-0.0.1-released">Attlib - an open source library for extracting search marketing attribution data from referrer URLs</a></h1>
			<p>Last night we published <a href='https://github.com/snowplow/attlib'>Attlib</a>, an open source Ruby library for extracting search marketing attribution data from referrer URLs. In this post we talk through:</p>

<ol>
<li><a href='#what_attlib_does'>What Attlib does, and how to use it</a></li>

<li><a href='#install'>Installing Attlib</a></li>

<li><a href='#search_engine_yaml'>The search_engine.yml file</a></li>

<li><a href='#snowplow_stack'>Attlib as part of the SnowPlow stack</a></li>

<li><a href='#other_languages'>Attlib in other languages</a></li>

<li><a href='#snowplow_components_as_standalone_projects'>Making components of SnowPlow available as standalone open source projects</a></li>
</ol>
<a name='what_attlib_does' />
<h3 id='what_attlib_does_and_how_to_use_it'>What Attlib does, and how to use it</h3>

<p>Attlib is straightforward Ruby library for extracting seach marketing attribution data from referrer URLs. You give it a referrer URL: it then lets you now whether the URL is from a search engine. If it is, it will tell you which search engine it is, and what keywords were typed. (If those keywords are included in the query string - this is no longer the case for users logged in to Google, as documented <a href='http://googlewebmastercentral.blogspot.co.uk/2011/10/accessing-search-query-data-for-your.html'>here</a>.)</p>
<div class='highlight'><pre><code class='ruby'><span class='nb'>require</span> <span class='s1'>&#39;attlib&#39;</span>

<span class='n'>r</span> <span class='o'>=</span> <span class='no'>Referrer</span><span class='o'>.</span><span class='n'>new</span><span class='p'>(</span><span class='s1'>&#39;http://images.google.ca/imgres?q=hermetic+tarot&amp;hl=en&amp;biw=1189&amp;bih=521&amp;tbm=isch&amp;tbnid=BuQ_IyUbc25usM:&amp;imgrefurl=http://www.psychicbazaar.com/tarot-cards/15-the-hermetic-tarot.html&amp;imgurl=http://mdm.pbzstatic.com/tarot/the-hermetic-tarot/card-4.png&amp;w=1064&amp;h=1551&amp;ei=ue9AUMe7Osn9iwLZ-4H4Dw&amp;zoom=1&amp;iact=hc&amp;vpx=107&amp;vpy=48&amp;dur=2477&amp;hovh=271&amp;hovw=186&amp;tx=133&amp;ty=157&amp;sig=115588264602219115047&amp;page=4&amp;tbnh=162&amp;tbnw=120&amp;start=57&amp;ndsp=19&amp;ved=1t:429,r:12,s:57,i:291&#39;</span><span class='p'>)</span>

<span class='n'>r</span><span class='o'>.</span><span class='n'>is_search_engine?</span> <span class='c1'># True</span>
<span class='n'>r</span><span class='o'>.</span><span class='n'>search_engine</span> <span class='c1'># &#39;Google Images&#39;</span>
<span class='n'>r</span><span class='o'>.</span><span class='n'>keywords</span> 	<span class='c1'># &#39;hermetic tarot&#39;</span>
</code></pre>
</div><a name='install' />
<h3 id='installing_attlib'>Installing Attlib</h3>

<p>Attlib is available via a Ruby Gem. To install, simply run the following at the command line:</p>

<pre><code>sudo gem install attlib</code></pre>

<p>The sourcecode is available on <a href='https://github.com/snowplow/attlib'>Github</a></p>
<a name='search_engine_yaml' />
<h3 id='the_search_enginesyml_file'>The search_engines.yml file</h3>

<p>Extracting search engine names and keywords from a referrer URL is pretty straightforward. What is more complicated is keeping track of the myriad search engines that are out there, operating in different countries, the myriad domains they operate on, and the different query parameters that each of them uses to store the keywords.</p>

<p>Because the space is constantly evolving, none of this information (about search engines, parameters and domains) has been hard coded into Attlib. All of it is available in the <a href='https://github.com/snowplow/attlib/blob/master/data/search_engines.yml'>search_engines.yml</a> file, in the <a href='https://github.com/snowplow/attlib/tree/master/data'>data</a> in the repo.</p>

<p>The structure of the YAML file should be straightforward to understand. Each search engine is a top level item. For each search engine, two lists are given: one is a list of parameters used in that search engine&#8217;s query string to identify the keywords entered. The other is the list of domains on which that search engine operates. An extract is shown below:</p>
<div class='highlight'><pre><code class='yaml'><span class='l-Scalar-Plain'>Babylon</span><span class='p-Indicator'>:</span>
  <span class='l-Scalar-Plain'>parameters</span><span class='p-Indicator'>:</span> 
    <span class='p-Indicator'>-</span> <span class='l-Scalar-Plain'>q</span>
  <span class='l-Scalar-Plain'>domains</span><span class='p-Indicator'>:</span> 
   <span class='p-Indicator'>-</span> <span class='l-Scalar-Plain'>search.babylon.com</span>
   <span class='p-Indicator'>-</span> <span class='l-Scalar-Plain'>searchassist.babylon.com</span>

<span class='l-Scalar-Plain'>Baidu</span><span class='p-Indicator'>:</span>
  <span class='l-Scalar-Plain'>parameters</span><span class='p-Indicator'>:</span> 
    <span class='p-Indicator'>-</span> <span class='l-Scalar-Plain'>wd</span>
    <span class='p-Indicator'>-</span> <span class='l-Scalar-Plain'>word</span>
    <span class='p-Indicator'>-</span> <span class='l-Scalar-Plain'>kw</span>
    <span class='p-Indicator'>-</span> <span class='l-Scalar-Plain'>k</span>
  <span class='l-Scalar-Plain'>domains</span><span class='p-Indicator'>:</span>
    <span class='p-Indicator'>-</span> <span class='l-Scalar-Plain'>www.baidu.com</span>
    <span class='p-Indicator'>-</span> <span class='l-Scalar-Plain'>www1.baidu.com</span>
    <span class='p-Indicator'>-</span> <span class='l-Scalar-Plain'>zhidao.baidu.com</span>
    <span class='p-Indicator'>-</span> <span class='l-Scalar-Plain'>tieba.baidu.com</span>
    <span class='p-Indicator'>-</span> <span class='l-Scalar-Plain'>news.baidu.com</span>
    <span class='p-Indicator'>-</span> <span class='l-Scalar-Plain'>web.gougou.com</span>
</code></pre>
</div>
<p>Keeping this file up to date is a big job: one of our hopes releasing Attlib as an open source, standalone library, is that the community contributes to the file. We are enormously grateful to our friends at <a href='http://piwik.org/'>Piwik</a> as our initial version of the file is based on the Piwik equivalent <a href='https://github.com/piwik/piwik/blob/master/core/DataFiles/SearchEngines.php'>SearchEngines.php</a>, for the hard work they put into this version.</p>
<a name='snowplow_stack' />
<h3 id='attlib_as_part_of_the_snowplow_stack'>Attlib as part of the SnowPlow stack</h3>

<p>Our intention is to port <a href='https://github.com/snowplow/attlib'>Attlib</a> into Scala and integrate it into the SnowPlow stack: specifically the ETL phase. Both Ruby and Scala versions of Attlib will run based on the same <a href='https://github.com/snowplow/attlib/blob/master/data/search_engines.yml'>search_engines.yml</a> file.</p>
<a name='other_languages' />
<h3 id='attlib_in_other_languages'>Attlib in other languages</h3>

<p>As well as contributing to the search <a href='https://github.com/snowplow/attlib/blob/master/data/search_engines.yml'>search_engines.yml</a> file, we also hope that community members will develop versions of Attlib in other languages e.g. Python.</p>
<a name='snowplow_components_as_standalone_projects' />
<h3 id='making_components_of_snowplow_available_as_standalone_open_source_projects'>Making components of SnowPlow available as standalone open source projects</h3>

<p>Attlib is the first component in the SnowPlow stack that we have released as a standalone library. There are many more in the pipeline. (More on this in future blog posts :-) ). For us, this is a key part of the SnowPlow strategy:</p>

<ol>
<li>Keeping the SnowPlow architecture as loosely coupled as possible. We believe this makes SnowPlow robust, scalable and extendable</li>

<li>Grow the userbase of people using and contributing to each component. Processing web analytics data is a big job: there are many individual components involved, and each of them needs to evolve with the changing marketplace. Attlib is concerned today with extracting useful data from search engine referrers: but it is likely that as time goes on, we&#8217;ll want to extend it to capture data from other types of referrers e.g. social networks or affiliate sites. The bigger the community of people on top of those developments, the better for everyone in the web analytics community. Releasing each component as a standalone open source library should help grow that community.</li>
</ol>
<hr />
<p>Any questions about Attlib, or anything else in this post? Then <a href='/contact/index.html'>get in touch</a> with the SnowPlow team.</p>
			<span class="comments-link"><a href="/blog/2012/10/11/attlib-0.0.1-released#disqus_thread" rel="nofollow">View Comments</a></span>
		</div>
	
		<a name="Why set your data free?" />
		<div class="post">
			24 Sep 2012
			<h1><a href="/blog/2012/09/24/what-does-snowplow-let-you-do">Why set your data free?</a></h1>
			<p>At Saturday&#8217;s <a href='http://ukdaa.co.uk/'>Measure Camp</a>, I had the chance to introduce SnowPlow to a large number of some incredibly thoughtful and insightful people in the web analytics industry.</p>

<p>With each person, I started by explaining that SnowPlow gave them direct access to their customer-level and event-level data. The response I got in nearly all cases was: <strong>what does having direct access to my web analytics data enable me to do, that I can&#8217;t do with Google Analytics / Omniture?</strong> It&#8217;s such a good question I thought I should publish an answer below:</p>

<h3 id='1_integrate_web_analytics_data_with_other_data_sources'>1. Integrate web analytics data with other data sources</h3>

<p>Integrating your web analytics data with other data sets enables you to answer a wide range of valuable business questions:</p>
<table><thead><tr><th><strong>Data source</strong></th><th><strong>Example business questions</strong></th></tr></thead><tbody><tr><td style='text-align: left;'>Marketing spend data e.g. AdWords, ad server data</td><td style='text-align: left;'>What is the return on my ad spend? How should I optimize my return on ad spend</td>
</tr><tr><td style='text-align: left;'>Customer data e.g. CRM, loyalty</td><td style='text-align: left;'>How does the online behaviour of my differnet customer segments vary by segment? Do online promotions drive offline sales? (Or vice versa?)</td>
</tr><tr><td style='text-align: left;'>Product / media catalogue data</td><td style='text-align: left;'>What are my most profitable product lines? Do different types of products attract different customer segments? What are the products that drive the most visits?</td>
</tr></tbody></table>
<p>SnowPlow makes integrating web analytics data with other data sources easier in a two ways:</p>

<ol>
<li>All your SnowPlow data is directly accessible in Apache Hive or Infobright. (So no expensive export process is required, prior to linking the data sets.)</li>

<li>Custom variables and event tracking give you plenty of opportunity to join e.g. customer IDs or campaigns names to enable <code>JOIN</code>s across data set</li>
</ol>

<p>For more details on how to perform <code>JOIN</code>s between SnowPlow data and other sources, see refer to the guide to <a href='/analytics/customer-analytics/joining-customer-data.html'>joining SnowPlow engagement data with other sources of customer data</a></p>

<h3 id='2_slice_and_dice_your_data_by_any_combination_of_dimensions__metrics_you_want'>2. Slice and dice your data by any combination of dimensions / metrics you want</h3>

<p>Google Analytics in particular only lets users create reports about of set combinations of dimensions and metrics. Examples of combinations that are <strong>not supported</strong> include:</p>

<ol>
<li>Number of unique visitors by product page</li>

<li>Different sources of traffic by product page (and how this changes over time)</li>

<li>Engagement levels (e.g. number of visits, number of page views, conversion rates) by traffic source</li>

<li>Improvements to conversion rates over time</li>
</ol>

<p>In contrast, because SnowPlow gives you access to the underlying data, it is possible to use BI tools like <a href='http://www.tableausoftware.com/'>Tableau</a> and <a href='http://www.microsoft.com/en-us/bi/powerpivot.aspx'>PowerPivot</a> to quickly slice and dice web analytics data by any dimensions / metrics you want. We&#8217;ll be posting examples of how to do this in the next few days.</p>

<h3 id='3_use_machine_learning_tools_on_your_web_analytics_data'>3. Use machine learning tools on your web analytics data</h3>

<p>Machine learning tools, and <a href='http://mahout.apache.org/'>Mahout</a> in particular, have created some new and exciting opportunities to:</p>

<ol>
<li>Develop product and content recommendation engines, based on user web behaviour. (E.g. users who viewed these content items, also viewed&#8230;)</li>

<li>Segment your audience by online behaviour</li>
</ol>

<p>SnowPlow makes it easy to extract the core input data you would need to feed a machine learning algorithm in a single query. (E.g. a matrix mapping users to products by page views / add to baskets / purchases etc.) We will be exploring ways to integrate SnowPlow with <a href='http://mahout.apache.org/'>Mahout</a> in a future blog post.</p>

<h3 id='4_view_data_for_individual_users_over_their_entire_lives'>4. View data for individual users over their entire lives</h3>

<p>Whereas reports on Google Analytics tend to be about visits, page views or transactions, SnowPlow lets you slice data by users over multiple visits, opening up a wide range of possibilities:</p>

<ol>
<li>Develop accurate models of customer lifetime value</li>

<li>Develop more rigorous approaches to attribution modelling, by capturing in granular detail which channels touched a user at different points in their lifecycle</li>
</ol>

<h3 id='5_interested_in_any__all_of_the_above'>5. Interested in any / all of the above?</h3>

<p>Then <a href='/product/get-started.html'>get started</a> with SnowPlow, or <a href='/contact/index.html'>get in touch</a> to find out more!</p>
			<span class="comments-link"><a href="/blog/2012/09/24/what-does-snowplow-let-you-do#disqus_thread" rel="nofollow">View Comments</a></span>
		</div>
	

	<!-- Pagination links -->
	<div class="pagination">
		
			<span class="previous">Previous</span>
		
		<span class="page_number">Page: 1 of 3</span>
		
			<a href="/page2" class="next">Next</a>
		
	</div>
</div>

<div id="sidebar">
	<h1>Recent posts</h1>
	<ul>
		
			<li><a href="/blog/2012/10/21/infobright-ruby-loader-released">Infobright Ruby Loader Released</a></li>
		
			<li><a href="/blog/2012/10/12/how-the-role-of-hive-is-changing-at-snowplow">How we use Hive at SnowPlow, and how the role of Hive is changing. (Slides from our presentation to Hive London.)</a></li>
		
			<li><a href="/blog/2012/10/11/snowplow-0.4.10-released">SnowPlow 0.4.10 released</a></li>
		
			<li><a href="/blog/2012/10/11/attlib-0.0.1-released">Attlib - an open source library for extracting search marketing attribution data from referrer URLs</a></li>
		
			<li><a href="/blog/2012/09/24/what-does-snowplow-let-you-do">Why set your data free?</a></li>
		
			<li><a href="/blog/2012/09/14/snowplow-0.4.8-released">SnowPlow 0.4.8 released</a></li>
		
			<li><a href="/blog/2012/09/06/snowplow-0.4.7-released">SnowPlow 0.4.7 released</a></li>
		
			<li><a href="/blog/2012/08/21/amazon-glacier-launch">Amazon announces Glacier - lowers the cost of running SnowPlow</a></li>
		
			<li><a href="/blog/2012/08/20/snowplow-0.4.6-released">SnowPlow 0.4.6 released</a></li>
		
			<li><a href="/blog/2012/08/14/updated-hive-serde-released">Updated Hive SerDe released</a></li>
		
			<li><a href="/blog/2012/08/13/introducing-snow-cannon-a-node-js-collector-for-snowplow">SnowCannon - a node.js collector for SnowPlow</a></li>
		
			<li><a href="/blog/2012/08/02/snowplow-setup-documentation-overhauled">The setup guide has been overhauled</a></li>
		
	</ul>
	<h1>Useful links</h1>
	<ul>
		<li><a href="/blog/atom.xml">Atom feed</a></li>
	</ul>
	<!--<strong>Tags</strong> -->
</div>
		<div id="footer">
	<p>Copyright Â© SnowPlow Analytics Limited 2012.  All rights reserved</p>
</div>
	</div>
	<!-- Following Javascript function used by Disqus to count the number of comments for each blog post and display in the main index -->
	  <script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'snowplow'; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function () {
            var s = document.createElement('script'); s.async = true;
            s.type = 'text/javascript';
            s.src = 'http://' + disqus_shortname + '.disqus.com/count.js';
            (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
        }());
        </script>
</body>
</html>